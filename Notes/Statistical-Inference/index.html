<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Statistical Inference Inference is about predicting an answer given an observation.
Bayes Rule  Maximum Likelihood Parameter Estimate The goal of MLE is to find the optimal way to fit a distribution to the data, i."><title>Statistical Inference</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://www.brendanang.dev//icon.png><link href=https://www.brendanang.dev/styles.c6adaa3b473914a38283f492dfdbcb76.min.css rel=stylesheet><link href=https://www.brendanang.dev/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://www.brendanang.dev/js/darkmode.f4d43c22d05773345705b5a308888af6.min.js></script>
<script src=https://www.brendanang.dev/js/util.fa8e74b4065b97e6980a72cc472e436f.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://www.brendanang.dev/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://www.brendanang.dev/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://www.brendanang.dev/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://www.brendanang.dev/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,PRODUCTION=!0,BASE_URL="https://www.brendanang.dev/",fetchData=Promise.all([fetch("https://www.brendanang.dev/indices/linkIndex.ecfa414e1282826cc43a2473116a81b9.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://www.brendanang.dev/indices/contentIndex.d49fde9522c188996a6772cce3033c34.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://www.brendanang.dev",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://www.brendanang.dev",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/www.brendanang.dev\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://www.brendanang.dev/js/full-text-search.51f0b1753e9b30839d053f8a98cc20d1.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://www.brendanang.dev/>Brendan Ang</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Statistical Inference</h1><p class=meta>Last updated
Feb 22, 2023
<a href=https://github.com/bbawj/site/Notes/Statistical%20Inference.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#bayes-rule>Bayes Rule</a></li><li><a href=#maximum-likelihood-parameter-estimate>Maximum Likelihood Parameter Estimate</a><ol><li><a href=#naïve-bayes-classifier>Naïve Bayes Classifier</a></li><li><a href=#mle-for-linear-regression>MLE for Linear Regression</a></li><li><a href=#mle-for-gaussian>MLE for Gaussian</a></li><li><a href=#mle-for-bernoulli>MLE for Bernoulli</a></li></ol></li><li><a href=#maximum-a-posteriori-estimation>Maximum a posteriori Estimation</a></li><li><a href=#classification>Classification</a><ol><li><a href=#maximum-likelihood-classification>Maximum Likelihood Classification</a></li><li><a href=#maximum-a-posteriori-classification>Maximum a Posteriori Classification</a></li><li><a href=#naive-bayes-classifier>Naive Bayes Classifier</a></li></ol></li><li><a href=#bayesian-estimation>Bayesian Estimation</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><a href=#statistical-inference><h1 id=statistical-inference><span class=hanchor arialabel=Anchor># </span>Statistical Inference</h1></a><p>Inference is about predicting an answer given an observation.</p><a href=#bayes-rule><h2 id=bayes-rule><span class=hanchor arialabel=Anchor># </span>Bayes Rule</h2></a><p><img src=https://i.imgur.com/0Wm1wow.png width=auto alt></p><a href=#maximum-likelihood-parameter-estimate><h2 id=maximum-likelihood-parameter-estimate><span class=hanchor arialabel=Anchor># </span>Maximum Likelihood Parameter Estimate</h2></a><p>The goal of MLE is to find the optimal way to fit a distribution to the data, i.e. the best distribution (parameters) to maximise the probability of observing our data.
<img src=https://i.imgur.com/GbzkGDj.png width=auto alt></p><a href=#naïve-bayes-classifier><h3 id=naïve-bayes-classifier><span class=hanchor arialabel=Anchor># </span>Naïve Bayes Classifier</h3></a><p>Take the dimensions of the data as <strong>independent of each other</strong> such that the joint probability of the feature set is broken up into the product of the probabilities of each feature. Naive Bayes is thus naive due to its indiscretion towards these combined probabilities, even though they may in fact be correlated to each other.
<img src=https://i.imgur.com/Ty4MF2I.png width=auto alt=500></p><a href=#mle-for-linear-regression><h3 id=mle-for-linear-regression><span class=hanchor arialabel=Anchor># </span>MLE for Linear Regression</h3></a><p><a href=/Notes/Regression/ rel=noopener class=internal-link data-src=/Notes/Regression/>Linear Regression</a> used the intuition of minimizing the least squared error from the data to the model to find the best fit line. Here we can see that finding the maximum likelihood also leads us to the same conclusion:
<img src=https://i.imgur.com/rvR9jVs.png width=auto alt></p><a href=#mle-for-gaussian><h3 id=mle-for-gaussian><span class=hanchor arialabel=Anchor># </span>MLE for Gaussian</h3></a><p><img src=https://i.imgur.com/qgRt2hd.png width=auto alt></p><a href=#mle-for-bernoulli><h3 id=mle-for-bernoulli><span class=hanchor arialabel=Anchor># </span>MLE for Bernoulli</h3></a><p><img src=https://i.imgur.com/CJX9UoG.png width=auto alt></p><a href=#maximum-a-posteriori-estimation><h2 id=maximum-a-posteriori-estimation><span class=hanchor arialabel=Anchor># </span>Maximum a posteriori Estimation</h2></a><p>Finding the most likely distribution parameter, given the data.
<img src=https://i.imgur.com/JguWcO5.png width=auto alt=500></p><ul><li>From the equation, we can see that MAP means maximising the product of the likelihood and the <strong>prior</strong> probability (some known information of the distribution).</li></ul><a href=#classification><h2 id=classification><span class=hanchor arialabel=Anchor># </span>Classification</h2></a><p>With our parameters known, we can make classifications on new data.
Will I play orienteering given the forecast? i.e. yes/no given that the new forecast is rainy.
<img src=https://i.imgur.com/biubIZ4.png width=auto alt=300>
<img src=https://i.imgur.com/jbpZyKV.png width=auto alt=400x300></p><a href=#maximum-likelihood-classification><h3 id=maximum-likelihood-classification><span class=hanchor arialabel=Anchor># </span>Maximum Likelihood Classification</h3></a><p>Classify based on the highest likelihood $P(x|y)\forall y$
$$
\begin{align}
P(rainy|y=yes)=\frac{3}{9}\\ P(rainy|y=no)=\frac{2}{5}\\ y_{ML}=NO
\end{align}
$$</p><a href=#maximum-a-posteriori-classification><h3 id=maximum-a-posteriori-classification><span class=hanchor arialabel=Anchor># </span>Maximum a Posteriori Classification</h3></a><p>Classify based on the highest posterior probability $P(y|x)\forall y$
$$
\begin{align}
P(yes|rainy)=\frac{P(rainy|yes)P(yes)}{P(rainy)}\\ =\frac{(3/9)(9/14)}{5/14}=0.6\\ P(no|rainy)=\frac{P(rainy|no)P(no)}{P(rainy)}\\ =\frac{(2/5)(5/14)}{5/14}=0.4\\ y_{MAP}=YES
\end{align}
$$</p><a href=#naive-bayes-classifier><h3 id=naive-bayes-classifier><span class=hanchor arialabel=Anchor># </span>Naive Bayes Classifier</h3></a><p>Take each feature as iid, will I go play orienteering given x?:
$$
\begin{align}
x&=(sunny, cool, high, true)\\ P(y=yes|forecast=x)&= \frac{Pr(x|y=yes)P(y=yes)}{P(forecast=x)}\\ argmax_{y\in Y}(P(yes|x))&=argmax_{y\in Y}(P(x|yes)P(yes))\\ &=0.005\\ argmax_{y\in Y}(P(no|x))&=argmax_{y\in Y}(P(x|no)P(yes))\\ &=0.021\\ y_{MAP}&=0.021=NO
\end{align}
$$</p><a href=#bayesian-estimation><h2 id=bayesian-estimation><span class=hanchor arialabel=Anchor># </span>Bayesian Estimation</h2></a><p>ML and MAP produce point estimates for $\theta$, which assumes that the true distribution follows these parameters. Bayes estimation uses the data to estimate a probability distribution for $\theta$ rather than just 1 point estimate. This gives a posterior estimate given the data rather than given the $\theta$.
<img src=https://i.imgur.com/pRxhWqJ.png width=auto alt></p><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li><a href=https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d rel=noopener>https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/2421-Machine-Learning/ data-ctx="Statistical Inference" data-src=/2421-Machine-Learning class=internal-link>2421 Machine Learning</a></li><li><a href=/Notes/Statistics/ data-ctx="Statistical Inference" data-src=/Notes/Statistics class=internal-link>Statistics</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://www.brendanang.dev/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Brendan Ang using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://www.brendanang.dev/>Home</a></li><li><a href=https://github.com/bbawj>Github</a></li></ul></footer></div></div></body></html>