DP Template.md,Problem Formulation,
DP Template.md,Strategy,
DP Template.md,Pseudocode,
Design Pattern Template.md,Problems we want to solve,1.
Design Pattern Template.md,General Idea,
Design Pattern Template.md,Pros,1.
Design Pattern Template.md,Cons,1.
Algorithm Template.md,General Idea,
Algorithm Template.md,Pseudocode,
Algorithm Template.md,Complexity,
Algorithm Template.md,Examples,
README.md,Viewing the Notes,"The files in this repository are not written using [GitHub flavoured markdown](https://github.github.com/gfm/), and will not render properly on the browser. They are written using [Obsidian](https://obsidian.md/), a personal knowledge base application and hence includes important internal linking syntax _""\[\[ \]\]""_ which helps organize the flow and linkages to various topics."
README.md,Navigating,__Module specific content__ are structured under 4 digit codes _e.g. 2101 Algorithm Design and Analysis_ corresponding to the course code in NTU. They contain the links to notes relevant to the course but may contain more information than in the examinable syllabus.__Important topics__ are structured under 3 digit codes _e.g. 001 Search Strategies_ which are meant to help provide a more cohesive method to study big branches of knowledge.
Wireless Networks.md,Network characteristics and its effects,"- Decreasing signal strength as signal passes through matter- Interference from other sources- Multipath propagation as portions of the signal reflect off surfaces, taking different paths between the sender and receiver causing the signal to be blurred$$C=BW\times log_2(1+\frac{S}{N})$$- C is channel capacity, which is the maximum information rate- BW is bandwith in *Hz*- S is signal and N is noise in *watts*"
Wireless Networks.md,Bandwidth,"Wireless communications run based on electromagnetic waves, and the bandwidth is the frequency range over which this communication can occur. *e.g. the 802.11b and 802.11g standards use the 2.4-2.5GHz band across all WiFi devices*.Higher frequencies can transfer more information (see equation above), but come at the cost of lower range as signals cannot travel as far."
Wireless Networks.md,Signal Power to Noise Power Ratio,"The larger the amount of background noise, the stronger the signal has to be to carry the information. Increasing SNR can be done in 2 ways, increasing the transmission power, or reducing the distance between receiver and transmitter"
Wireless Networks.md,Problems,- Near-far problem: louder signals crowd out weaker signals- Cell-breathing: more signals result in greater interference and shrinks the effective range of a signal
Wireless Networks.md,Bit Error Rate,"Bit error rate varies according to the bit transmission rate and SNR. This means that as SNR changes, it would be good to support dynamic selection of the modulation technique (bit transmission rate) to adapt to the channel conditions."
Wireless Networks.md,Hidden terminal problem and fading,"Signals are not strong enough to be detected at each individual source, but end up interfering at each other at the destination (B)."
Wireless Networks.md,Wireless LANs: WiFi 802.11,"Infrastructure (use of access points) wireless LAN architecture, connecting basic service sets:Each AP is given a unique MAC address for its interface, similar to [Ethernet](Notes/Link%20Layer.md#Ethernet)."
Wireless Networks.md,Channels and Association,"- Each access point is assigned by the network admin a Service Set Identifier (SSID) (which also shows up as the names of the nearby APs on your device).- 802.11 operates in a 85 Mhz band with 11 partially overlapping channels. Each AP is assigned by the network admin a channel number.To obtain information about an AP, the AP periodically sends beacon frames, each with the AP's SSID and MAC address"
Wireless Networks.md,CSMA/CA Multiple Access Control Protocol,"Similar to Ethernet's [Carrier Sense Multiple Access Collision Detection (CSMA/CD)](Notes/Link%20Layer.md#Carrier%20Sense%20Multiple%20Access%20Collision%20Detection%20(CSMA/CD)) but using collision avoidance instead.- Not collision detection: [Hidden terminal problem](#Hidden%20terminal%20problem) makes it impossible to detect all collisions- Collision avoidance: transmit the frame in entirety, but in a manner as to avoid collisions"
Wireless Networks.md,Link-layer acknowledgements:,"1. If idle, transmits frame after a short period of time known as the Distributed Inter-frame Space2. Else, choose a random value using [Binary Exponential Backoff](Notes/Binary%20Exponential%20Backoff.md) and count down this value after DIFS while the channel is sensed idle. While it is busy, the counter is frozen. In 2 competing senders, they will hopefully choose a different backoff value, causing the ""winning"" sender to transmit first. The ""loser"" will hear the winner's signal, freeze its counter and refrain from transmitting until the winner has completed.3. When counter = 0, transmit entirely and wait for ACK4. If ACK not received, retransmit from step 2 with increased value"
Wireless Networks.md,Clear to Send (CTS) / Request To Send (RTS):,"*If 2 senders are out of range of each other, they would not be able to freeze their counter value.*1. Sender sends a RTS frame to the AP with the total time required to transmit the data and acknowledgement frame2. AP responds by broadcasting a CTS, allowing sender permission to transmit while instructing others not to."
Wireless Networks.md,802.11 Frame,"Note 4 address fields compared to just source/receiver:- Address 1: MAC address of receiver- Address 2: MAC address of transmitting station- Address 3: MAC address of router interface for the router which connects the subnet in the BSS to the internet- Router encapsulates data frame into an Ethernet frame with source/destination and sends it to an AP- AP converts the ethernet frame to 802.11 frame with source/destination, but adds the router MAC address to the address 3- Receiver can now use address 3 as the destination MAC address for sending data (and the router can then deconstruct it and forward the packet to the destination in the IP datagram)"
Wireless Networks.md,Wireless Personal Area Networks (WPANs),"Low power, short range low rate technology"
Wireless Networks.md,Bluetooth,"A network established without network infrastructure in a master-slave configuration. Channel is partitioned using [TDM](Notes/Link%20Layer.md#Channel%20Partitioning%20Protocols) and the channel is changed in a pseudo random manner from time slot to slot, known as frequency hopping spread spectrum (FHSS)"
Wireless Networks.md,Zigbee,"For even lower powered, lower data rate applications than Bluetooth such as home temperature and light sensors."
Wireless Networks.md,Cellular Internet Access,"WiFi 802.11 access points have a small coverage area. To combat this, cellular networks extended beyond voice communication to allow wireless internet connection."
Wireless Networks.md,2G: providing voice communication,"Service area is partitioned into cells:BSC: allocates BTS radio channels to subscribers, finds the cell which a mobile user is in and perform handoffs.MSC: performs user authorisation and accounting, call establishment, teardown and handoff."
Wireless Networks.md,3G: extending to support data,New cellular network operates in parallel with the voice network.
Wireless Networks.md,4G: all-IP core network,Both voice and data are carried in IP datagrams rather than on separate networks
White Box Testing.md,Control Flow Testing,
White Box Testing.md,The Control Flow Graph,
White Box Testing.md,Cyclomatic Complexity,Measurement of complexity based on the number of decision points.
White Box Testing.md,Test Coverage Levels,
White Box Testing.md,Statement Coverage,
White Box Testing.md,Branch Coverage,
White Box Testing.md,Basis Path Coverage,"> [!NOTE] Algorithm for creating basis paths> 1. Select the baseline path: Reasonably ""typical"" path of execution. If loops exists, take the path which does not enter the loop.> 2. For every decision point in the baseline path:> 	1. Change the outcome> 	2. This is the new basis path> [!important]> Infeasible paths may exist due to application logic: minimize them by changing multiple decision points at once."
White Box Testing.md,Path Coverage,
Virtualization.md,Purpose,More efficient utilisation of hardware- Cost effective hardware deployment and sharing- Low latency and agile execution environments- Failure mitigation (VM independence and migrations)Enables cloud computing
Virtualization.md,Hypervisor or VMM,
Virtualization.md,Concepts of Virtualisation,
Virtualization.md,Levels of Virtualisation,
Virtualization.md,Type 1 Virtualisation,
Virtualization.md,Type 2 Virtualisation,
Virtual Memory.md,Swapping,"A process can be swapped temporarily out of memory into a backing store.- Backing store: fast disk large enough to accommodate copies of all memory images for all usersSwapping time is primarily contributed by transfer time.# Demand PagingTo support implementation of virtual memory, demand paging is used. Each process is divided into pages and each page can be loaded when it is in demand. *Initially, all pages are not in memory.*"
Virtual Memory.md,Page Fault,"- 2,6: incurs context switch costs by the OS- 4: incurs disk I/O cost to bring in the page"
Virtual Memory.md,Thrashing,"Consider what occurs if a process does not have “enough” frames—that is, it does not have the minimum number of frames it needs to support pages in the working set. The process will quickly page-fault. At this point, it must replace some page. However, since all its pages are in active use, it must replace a page that will be needed again right away. Consequently, it quickly faults again, and again, and again, replacing pages that it must bring back in immediately. **This high paging activity is thrashing**.To prevent thrashing, we must provide a process with as many frames as it needs."
Virtual Memory.md,Working Set Model,Based on the concept of locality of reference: processes tend to refer to pages in a localised mannerTemporal locality: locations referred to recently are likely to be referenced againSpatial locality: code and data are usually clustered physicallyImplementation:Additional metrics:
Virtual Memory.md,Page Fault Frequency,- Establish an upper and lower bound for acceptable page-fault rate and allocate and deallocate frames accordingly
Virtual Memory.md,Page Replacement,"If there are no empty frames, OS needs to locate a victim to evict:"
Virtual Memory.md,Belady's Anomaly,> [!Inclusion Property]> Pages loaded in *n* frames is always a subset of pages in *n+1* frames>> An algorithm does not suffer from Belady's anomaly if it satisfies the inclusion property. This is because such an algorithm will only increase its total coverage of available frames and does not replace any frame that was previously loaded.>> An example with FIFO:>>
Virtual Memory.md,Policies,[Page Replacement Policies](Notes/Page%20Replacement%20Policies.md)
Virtual Memory.md,Allocation of Frames,
Virtual Memory.md,Fixed allocation,Give a process a fixed number of frames in memory for execution.
Virtual Memory.md,Variable allocation,Allow the number of frames allocated to the process to vary across its execution lifetime.
Virtual Memory.md,Scope of Replacement,
Virtual Memory.md,Global,Process selects a replacement frame from the set of all frames; one process can take a frame from another process. Implication: performance of the process depends on external processes.
Virtual Memory.md,Local,Each process selects only from its own set of allocated frame. Implication: may hinder other processes by not making available its less used pages/frames
Virtual Memory.md,Other Considerations,
Virtual Memory.md,Program Structure,"Specific choice in data structures used and program structure can affect the performance of demand paging.- Use of data structures like a hash set or linked list might offer *lesser* locality of reference than a data structure like an array, possibly reducing performance# Practice Problemsa. FIFO will replace the earliest loaded page. Page 2b. Replace the first page with R=0. Page 0.c. Replace the oldest accessed page. Page 1.| Tick | Page Ref | FIFO      | Clock     | LRU       | Loaded     | R       | Accessed   || ---- | -------- | --------- | --------- | --------- | ---------- | ------- | ---------- || 1    | 0        | 0         | 0         | 0         | 1          | 0       | 1          || 2    | 1        | 0,1       | 0,1       | 0,1       | 1,2        | 0,0     | 1,2        || 3    | 6        | 0,1,6     | 0,1,6     | 0,1,6     | 1,2,3      | 0,0,0   | 1,2,3      || 4    | 0        | 0,1,6     | 0,1,6     | 0,1,6     | 1,2,3      | 1,0,0   | 4,2,3      || 5    | 3        | 0,1,6,3   | 0,1,6,3   | 0,1,6,3   | 1,2,3,5    | 1,0,0,0 | 4,2,3,5    || 6    | 4        | 4,1,6,3 F | 0,4,6,3 F | 0,4,6,3 F | 6,2,3,5    | 0,0,0,0 | 4,6,3,5    || 7    | 0        | 4,0,6,3 F | 0,4,6,3   | 0,4,6,3   | 6,7,3,5    | 1,0,0,0 | 7,6,3,5    || 8    | 1        | 4,0,1,3 F | 0,4,1,3 F | 0,4,1,3 F | 6,7,8,5    | 0,0,0,0 | 7,6,8,5    || 9    | 0        | 4,0,1,3   | 0,4,1,3   | 0,4,1,3   | 6,7,8,5    | 1,0,0,0 | 9,6,8,5    || 10   | 3        | 4,0,1,3   | 0,4,1,3   | 0,4,1,3   | 6,7,8,5    | 1,0,0,1 | 9,6,8,10   || 11   | 4        | 4,0,1,3   | 0,4,1,3   | 0,4,1,3   | 6,7,8,5    | 0,0,0,1 | 9,11,8,10  || 12   | 6        | 4,0,1,6 F | 0,4,6,3 F | 0,4,6,3 F | 6,7,8,12   | 0,0,1,1 | 9,11,12,10 || 13   | 3        | 3,0,1,6 F | 0,4,6,3   | 0,4,6,3   | 13,7,8,12  | 0,0,1,1 | 9,11,12,13 || 14   | 4        | 3,4,1,6 F | 0,4,6,3   | 0,4,6,3   | 13,14,8,12 | 0,1,1,1 | 9,14,12,13 |The first 4 page loads are always page faults.a. $4+6=10$b. $4+3=7$c. $4+3=7$a.If N <= M:No more page faults will occur after all distinct pages are loaded into memory.Lower bound = N, Upper bound = NIf N > M:Lower bound occurs on the minimum number of page faults. Every unique page will result in a page fault = NUpper bound occurs when every page reference is a page fault = Lb. No. LRU does not guarantee optimality.a.| Page Ref | LRU     | Accessed        | Fault || -------- | ------- | --------------- | ----- || -        | 1,0,3,2 | 161,160,162,163 | N     || 4        | 1,4,3,2 | 161,164,162,163 | Y     || 0        | 0,4,3,2 | 165,164,162,163 | Y     || 0        | 0,4,3,2 | 166,164,162,163 | N      || 0        | 0,4,3,2 | 167,164,162,163 | N      || 2        | 0,4,3,2 | 166,164,162,167 | N      || 4        | 0,4,3,2 | 166,168,162,167 | N      || 2        | 0,4,3,2 | 166,168,162,169 | N      || 1        | 0,4,1,2 | 166,168,170,169 | Y      || 0        | 0,4,1,2 | 171,168,170,169 | N      || 3        | 0,3,1,2 | 171,172,170,169 | Y      || 2        | 0,3,1,2 | 171,172,170,173 | N      |4 Page Faults.b.| Page Ref | Working Set | Fault || -------- | ----------- | ----- || -        | 0,1,3,2     | -     || 4        | 1,3,2,4     | Y     || 0        | 3,2,4,0     | Y     || 0        | 2,4,0       | N     || 0        | 4,0         | N     || 2        | 0,2         | Y     || 4        | 0,2,4       | Y     || 2        | 0,4,2       | N     || 1        | 4,2,1       | Y     || 0        | 4,2,1,0     | Y     || 3        | 2,1,0,3     | Y     || 2        | 1,0,3,2     | N     |7 page faults.Illustrates a thrashing situation.a. F. CPU is already under utilisedb. ~~T. Increase available memory for pages to be loaded for processes~~ False. A larger paging disk does not allow for more page frames in memory.c. F. Already not enough memory for existing programsd. T. Swap out some programs to the backing store to allow for more pages for existing programse. T. More pages can be stored in memoryf. T. Less time spent in I/Og. F.- Increasing page size will result in fewer page faults if data is accessed sequentially- If data access is random, more paging actions could occur because fewer pages can be kept in memory"
User Datagram Protocol.md,UDP Segment,"Datagram: packets delivered via an unreliable service, without delivery guarantees and no failure notifications.UDP encapsulates user messages into its own packet structure on top of the [Internet Protocol](Notes/Internet%20Protocol.md):"
User Datagram Protocol.md,UDP Checksum,"Sender: Perform a 1s complement sum of all the 16 bit words in the UDP segment, with overflow being wrapped around.Receiver: Sum all the 16 bit words including the checksum, the result should be 16 1s, else an error is detected.*Note: this checksum is optional*"
User Datagram Protocol.md,Stateless,Each datagram is carried in a single IP packet with no support for bytestreams. Hence each read will yield the full message and datagrams are not fragmented.
User Datagram Protocol.md,Problems,"Each connection relies upon [Network Address Translation](Notes/Network%20Address%20Translation.md). Translation tables rely on the connection state in order to create and remove entries as needed, but UDP does not have any processes to define its state (no handshake, no termination sequence).One solution: UDP routing records are expired on a timer."
Use Case Diagrams.md,Associations,
Use Case Diagrams.md,extend,The **extending use case is dependent on the base use case**; it literally extends the behaviour described by the base use case. The base use case should be a fully functional use case in its own right without the extending use case’s additional functionality.
Use Case Diagrams.md,include,A **base use case is dependent on the included use case**; Extract use case fragments that are _duplicated_ in multiple use cases. The included use case cannot stand alone and the original use case is not complete without the included one. This helps to reduce redundancy in complex systems.
Use Case Diagrams.md,Example Breakdown of Requirements,Use Case for Library Management SystemGES Use Case Diagram:
Use Case Diagrams.md,Use Case Descriptions,
Union Find.md,Operations,These operations make it an efficient data structure to verify if a cycle exists in some __undirected__ graph.- Union find is unable to detect cycles in directed graphs: union operation cannot distinguish between the subcomponent that makes the edge from a -> b and that which makes the edge b->a
Union Find.md,Implementations,
Union Find.md,Quick Find,
Union Find.md,Pseudocode,
Union Find.md,Complexity,
Union Find.md,Quick Union,"Unions are too expensive using quick find, as we have to change the ids of all elements in the combining equivalence class for each union operation.Improvement: only store the parent of the node"
Union Find.md,Pseudocode,
Union Find.md,Complexity,
Union Find.md,Weighted Quick Union,Quick Union creates possible tall trees as the union operation does not necessarily occur between root nodes.Improvement: avoid tall trees by linking smaller tree root to larger tree root
Union Find.md,Pseudocode,
Union Find.md,Complexity,"Hence, besides initialization $O(n)$, all other operations take $O(logn)$"
Union Find.md,WQU with Path Compression,"Improvement: further reduce the effective path to reach the root node: for some node p which we are computing the root of, we can update id[] of nodes on the path from p to root."
Uniform Cost Search.md,Graph Traversal,_Assuming ties are handled in alphabetical order_Expansion Order:A > B > D > E > F > GFinal Path:A > B > D > E > F > G
TypeScript.md,Why TypeScript,"TypeScript helps to combat common errors:1. Uncalled Functions:```typescriptfunction flipCoin() {// Meant to be Math.random()return Math.random < 0.5;//Operator '<' cannot be applied to types '() => number' and 'number'.}```2. Logic errors```typescriptconst value = Math.random() < 0.5 ? ""a"" : ""b"";if (value !== ""a"") {// ...} else if (value === ""b"") {//This condition will always return 'false' since the types '""a""' and '""b""' have no//overlap.// Oops, unreachable}```"
TypeScript.md,Defining Types,
TypeScript.md,Type Inference,"```typescriptlet helloworld = ""Hello World"" //typescript uses the value as its type```This is in contrast to Java which types have to be explicitly defined:```javaString name = ""John"";```"
TypeScript.md,Type Aliases,A name for any type```typescripttype Point = {x: number;y: number;};type ID = number | string;```
TypeScript.md,Type Interfaces,"```typescriptinterface User {name: string;id: number;}```OOP type annotations:```typescriptclass UserAccount {name: string;id: number;constructor(name: string, id: number) {this.name = name;this.id = id;}}const user: User = new UserAccount(""Murphy"", 1);```__An interface is always extendable but an alias is not open for extension:"
TypeScript.md,Composing Types,
TypeScript.md,Unions,"```typescripttype WindowStates = ""open"" | ""closed"" | ""minimized"";type LockStates = ""locked"" | ""unlocked"";type PositiveOddNumbersUnderTen = 1 | 3 | 5 | 7 | 9;function getLength(obj: string | string[]) {return obj.length;}```TypeScript will only allow an operation if it is valid for every member of the union. For example, if you have the union `string | number`, you can’t use methods that are only available on string. Use `typeof <x> === ""<type>""` to check for types before calling type specific methods."
TypeScript.md,Types with Generics,"```typescriptinterface Backpack<Type> {add: (obj: Type) => void;get: () => Type;}// This line is a shortcut to tell TypeScript there is a// constant called `backpack`, and to not worry about where it came from.declare const backpack: Backpack<string>;// object is a string, because we declared it above as the variable part of Backpack.const object = backpack.get();// Since the backpack variable is a string, you can't pass a number to the add function.backpack.add(23);```"
TypeScript.md,Structural Type System,"_In a structural type system, if two objects have the same shape, they are considered to be of the same type._```typescriptinterface Point {x: number;y: number;}function logPoint(p: Point) {console.log(`${p.x}, ${p.y}`);}const point3 = { x: 12, y: 26, z: 89 };logPoint(point3); // logs ""12, 26""const rect = { x: 33, y: 3, width: 30, height: 80 };logPoint(rect); // logs ""33, 3""const color = { hex: ""#187ABF"" };logPoint(color); //fails as x and y are missing```An object need only to match all the type attributes for the code to pass (can have more but not less)."
TypeScript.md,Type Assertions,"Sometimes you will have information about the type of a value that TypeScript can’t know about.For example, if you’re using `document.getElementById`, TypeScript only knows that this will return _some_ kind of `HTMLElement`, but you might know that your page will always have an `HTMLCanvasElement` with a given ID.In this situation, you can use a _type assertion_ to specify a more specific type: `const myCanvas = document.getElementById(""main_canvas"") as HTMLCanvasElement;`Angle-bracket syntax (except if the code is in a `.tsx` file), which is equivalent: `const myCanvas = <HTMLCanvasElement>document.getElementById(""main_canvas"");`> [!REMINDER] Runtime behaviour> Because type assertions are removed at compile-time, there is no runtime checking associated with a type assertion. There won’t be an exception or `null` generated if the type assertion is wrong.TypeScript only allows type assertions which convert to a _more specific_ or _less specific_ version of a type. This rule prevents “impossible” coercions like: `const x = ""hello"" as number;`This rule can be too conservative and will disallow more complex coercions that might be valid. You can use two assertions, first to `any` or `unknown`,then to the desired type:`const a = (expr as any) as T;`"
TypeScript.md,Literal Types,"The types used in the union above include some literals e.g. ""locked"", 1"
TypeScript.md,Literal Inference (or lack thereof):,"When you initialize a variable with an object, TypeScript assumes that the properties of that object might change values later.`req.method` must have the type `string`, not `""GET""`:```typescriptconst req = { url: ""https://example.com"", method: ""GET"" };handleRequest(req.url, req.method);//error out as type 'string' is not assignable to parameter of type '""GET"" | ""POST""' for req.method.// Change 1:const req = { url: ""https://example.com"", method: ""GET"" as ""GET"" };// Change 2handleRequest(req.url, req.method as ""GET"");```"
Two Pass Algorithms.md,Sort Based Algorithms,"If data fits in memory, then we can use a standard sorting algorithm like quick-sort. If data does not fit in memory, then we need to use a technique that is aware of the cost of writing data out to disk."
Two Pass Algorithms.md,External Merge Sort,"Idea is similar to [Merge Sort](Notes/Merge%20Sort.md). By breaking the data into pairs, we can sort each pair and merge them recursively.We have 2 buffers for READ and 1 buffer for write: B(R) = 4Since each run may not fully fit inside memory, we can load segments of pairs of runs (e.g. 1 block from each run even though run contains 2 blocks), merge them and immediately write them back to the disk."
Two Pass Algorithms.md,Two-Phase Multiway Merge Sort,"We do not fully utilise all the buffers in external merge sort if M > 3.Idea:1. Divide data into $\frac{B}{M}$ sublists each of size $M$.2. Sort each chunk and write it back to the disk3. Use 1 input buffer for each sorted sublist. Implication: *hence there can only be M-1 sublists*1. Take the smallest of the head of each sublist (each in 1 buffer) and move into output2. If a buffer is empty: load tuples into the block from the same sorted sublist3. If no blocks remain in the sublist, ignore> [! Note]> Suppose R fits on B blocks. With M buffers each of 1 block, we can effectively sort M blocks of data each time. We can form $B(R)/M$ sorted sublists. In total we will only read and write B(R) blocks for this step.>> Since we need 1 input buffer to represent each head of a sublist, we will need $B(R)/M \le(M-1), \ or\  B(R)\le M\times (M-1)\approx M^2$>> E.g. Suppose blocks are 64K bytes, and we have one gigabyte of  main memory. Then we can afford M  of 16K. Thus, a relation fitting in B  blocks can be sorted as long as B is no more than $(16K)^2 = 2^{28}$. Since blocks  are of size $64K = 2^{14}$, a relation can be sorted as long as its size is no greater  than $2^{42}$ bytes, or 4 terabytes.If B(R) cannot be split into sublists of size M $i.e. B(R)/M \ge M$, first split B(R) into  sublists of size $M(M-1)$. Apply 2PMMS to each of these $\frac{B(R)}{M(M-1)}$ chunks to get M sorted sublists. This forms the input for a third pass to form a fully sorted relation."
Two Pass Algorithms.md,Cost,Let B be the number of blocks. B disk I/O to read in the first pass. B disk I/O to write sorted sublists. B disk I/O to read sorted sublists in second pass. **Total 3B disk I/O**.
Two Pass Algorithms.md,Sort-Merge Join,"Idea: If the tuples are sorted, we can more easily join them togetherAlgorithm:1. Sort R and S according to a key Y using 2PMMS2. Merge R and S using only 2 buffers, one for the current block of R and another for the current block of S.1. Find the least value of y that is currently at front of blocks R and S2. If y does not appear at the front of the other relation, remove the tuples with value y3. Else, find tuples from both relations having sort key y, If necessary, read blocks from R and S until we are sure that there are no more y's in either relation. **We can use up to M buffers for this purpose4. Output all the tuples that can be formed by joining these tuples5. If either relation has no more unconsidered tuples in main memory, reload the buffer for that relation"
Two Pass Algorithms.md,Implications,Tuples with a common value of the sort key from both relations together must fit in M blocks. Consider if there are more than M such tuples. We will not be able to load the needed tuples for joining in 1 pass.
Two Pass Algorithms.md,Cost,"3B disk I/O per relation in step 1 for sorting, 1B per relation additionally to write fully sorted back to disk1B disk I/O per relation in step 2Total: $5(B(R) + B(S))$ disk I/O"
Two Pass Algorithms.md,Refined Sort-Merge Join,"Notice that in sort-merge join, the 2 relations are sorted first and then merged in distinct passes creating the greatest possible numbers of buffers available for joining tuples with common value.If we do not need to worry about large number of tuples sharing common sort key, we can join the tuples in the merge phase of the sort:1. Created M sorted sublists using sort key Y2. Bring the first block of each sublist into a buffer3. Repeatedly find the smallest y value. Find tuples of both relations that have value y."
Two Pass Algorithms.md,Cost,"Step 1 will require 2 I/O per block in order to read, form the sorted sublists and write back to diskStep 2 will require 1 I/O per block in order to read and mergeTotal: $3(B(R) + B(S))$ disk I/O"
Two Pass Algorithms.md,Implications,We need all sorted sublists from both relations to be able to fit in memory.Number of sorted sublists = $(B_R+B_S)/M$$(B_R+B_S)/M \le M$$(B_R+B_S) \le M^2$
Two Pass Algorithms.md,Hash Based Algorithms,Idea: hash the data into M buckets in order to fit into M main memory buffers for operations.
Two Pass Algorithms.md,Grace Hash Join,"1. Hash both relations to M-1 buckets using join key2. Join every pair of matching hash key buckets in 1 pass1. Load all the buckets of one relation into M - 1 buffers2. One by one, load buckets from the other relation and join"
Two Pass Algorithms.md,Implications,"The relation which smaller number of buckets must at least fit into M - 1 buffers.$min(\frac{B(R)}{M-1}, \frac{B(S)}{M-1}) \le M-1 \approx min(B(R),B(S))\le M^2$"
Two Pass Algorithms.md,Cost,Step 1: 2 disk I/O per block to hash and write back to diskStep 2: 1 disk I/O per block as we read 1 block once only before joiningTotal: $3(B(R) + B(S))$ disk I/O
Two Pass Algorithms.md,Hybrid Hash Join,"What if the size of one of our relations is much smaller than the M? $B(S) << M^2$We can leave some buckets of S in memory without writing to disk such that we can join with B(R) immediately.1. Partition S into k buckets, keep $t$ buckets in memory and $k-t$ buckets in the disk2. Partition R into k buckets, first $t$ buckets are joined with S3. Join $k-t$ pairs of buckets"
Two Pass Algorithms.md,Implications,The $t$ hash buckets saved in memory + the blocks for each head of each hash bucket written to disk must fit entirely in memory.$$t\times\frac{B(R)}{k}+k-t\le M$$
Two Pass Algorithms.md,Cost,Average bucket size is $B(S)/k$. We save write and read of $t\times B/k$ blocks for each relationTotal cost: $3(B(R) + B(S)) - \frac{2\times t}{k}(B(R)+B(S))$
Two Pass Algorithms.md,Comparison between Hash & Sort based Join,
Two Pass Algorithms.md,Practice Problems,"We can eliminate duplicates through sorting.1. Divide data into M-1 buffers2. Sort each buffer and write it back to the disk.3. 2nd pass: read all the sorted sublists into M-1 buffers4. Move the smallest of the heads of each sublist into the output if the max of the current output does not already contain this incoming value.a. M = 3. Sort merge join of S and T. Sort merge join of R and the result of S and T. 1 block to read R, 1 block to read S, 1 block to hold intermediate result of $R\Join S$b.II: $M > B_T$III: $M > B_T+(\text{Number of sorted sublists of S})+1$. Need all sorted sublists in buffer, and need 1 additional buffer to comb through R~~Yes. Only refined sort merge join has comparable cost. In this case, refined sort merge join will not be applicable as both relations share multiple common values of attribute Y. Hence we are not able to fully load all tuples with the same attribute value into M buffers.~~No. Hash join works by partitioning the relations based on the distinct keys. Since there are very few distinct values of Y, there will be very little partitions. 2nd pass will not work well since each partition must be loaded fully into memory buffer.a.Strategy:1. Use R as the outer relation as it is smaller.2. Keep 1 hash bucket in memory. Need to be able to load all hash buckets from one of the relations into the remaining memory.$$\begin{aligned}\\\text{Let k be the number of hash buckets}\\\text{No. of buffers for S}=1\\\text{No. of buffers to write k-1 buckets to disk}=k-1\\\text{No. of buffers to keep 1 bucket on memory}=B(R)/k\\M\ge 1+k-1+B(R)/k=k+B(R)/k\end{aligned}$$b.We save the cost of writing and reading 1 bucket of R to/from disk during the hashing process.Each bucket contains $(400)/20=20blocks$Total cost: $3(B_R+B_S)-2\times20=2660$i. Set union operation involves duplicate elimination. Disk I/O = $B(R)+ B(R)\times B(S)=100010000$ii.For refined sort merge join:1. Read all blocks to perform 2PMMS: $2(B_R + B_S)=40000$2. Perform join on merge phase: $1(B_R + B_S)=20000$Total = 60000 i/oi. The minimum number of main memory blocks needed is that the relation with smaller number of hashed blocks must fit entirely into the main memory. Assuming that each block hashes to its own hash bucket, we need $M=10000$i. $3\times (B_R + B_S) =4500$ii. $3\times (B_R + B_S) =4500$Procedure:1. Sort R and S according to the attribute $x$ using two phase multiway merge sort2. Load the first block of sorted R and S into main memory3. Find tuple with the smallest value of $x$ and find matching tuples and write them to the output buffer4. Repeatedly find the smallest value of $x$ once all tuples with the current smallest value are considered5. Reload new blocks when either relation's blocks are fully consideredCost:$B_R=30000/30=1000$$B_S=9000/10=900$Sorting: $3(B_R+B_S)=5700$Joining: $2(B_R+B_S)=3800$Total: 9500i.$B_R=50$$(B_R+B_S)/10 \le10$$50+\frac{2000}{x}\le100$$x\ge40$ii.$3(B_R+B_S)=3(50+50)=300$iii.Use 9 buffers for one relation.$B_R+B_R\times B_S/9=328$iv.$3(B_R+B_S)=3(50+50)=300$v.Choose between refined sort merge join and grace hash join as they have the smallest I/O cost. Choose refined sort merge join because the output will also be sorted."
Travelling Salesman Problem.md,Problem Formulation,
Travelling Salesman Problem.md,Strategy,
Travelling Salesman Problem.md,Pseudocode,
Travelling Salesman Problem.md,Greedy Heuristics,Nearest Neighbour:
Transport Layer.md,Multiplexing and Demultiplexing,"Multiplexing and demultiplexing is the extension of the host-to-host delivery service provided by the network layer to a process-to-process delivery service for applications running on the hosts.A host can have multiple network processes running, and each can have 1 or more sockets for which data passes from the network.- Multiplexing: gathering data chunks at the source from different sockets, encapsulating each chunk with header information and passing the segments to the network layer- Demultiplexing: delivering data in a transport layer segment to the correct socket. In UDP a 2-pair {source port, dest port} can uniquely identify the destination socket. In TCP, a 4-pair {source port, source IP, dest port, dest IP} identifies the destination socket."
Transport Layer.md,Transport Layer Protocols,
Transport Layer.md,UDP,A barebones transport protocol for the Internet is [User Datagram Protocol](Notes/User%20Datagram%20Protocol.md)
Transport Layer.md,TCP,[Transmission Control Protocol](Notes/Transmission%20Control%20Protocol.md) provides a reliable channel service to the applications which invoke it.
Transport Layer.md,Principles of Reliable Data Transfer,
Transport Layer.md,Fundamentals,We can incrementally build a reliable data transfer (rdt) protocol using [[Finite State Machines|finite state machines]]
Transport Layer.md,RDT 1.0 reliable under channel,
Transport Layer.md,RDT 2.0 error checking,"Add error checking through checksum calculation, acknowledgements (ACKs) and negative acknowledgements (NAKs), and have the sender retransmit the corrupted sentence."
Transport Layer.md,RDT 2.1 ACK corruption,"*The fatal flaw in rdt 2.0 is that the ACK or NAK packets may in itself be corrupted. How should the protocol recover from such errors?* Here are some ideas:- Keep adding more ""ACK"" types. Reply with ""What did you say?"". This can continue infinitely- Add enough checksum bits to recover from bit errors- Retransmit packets if the sender receives a corrupted ACK or NAK. However, we won't be able to tell if its new data or a retransmission on the receiver side!We can add a new field to the data packet called the **sequence number**. The receiver need only check this to determine whether or not the received packet is retransmission.- For a stop-and-wait protocol, a 1 bit number will allow the sender to tell: no change -> retransmission, change -> new packet.Sender:Receiver:"
Transport Layer.md,RDT 2.2 NAK-less,"We can remove the need for NAKs by sending an ACK only for the last correctly received packet. If the sender receives 2 ACKs for the same packet, it knows that the following packet was not received correctly."
Transport Layer.md,RDT 3.0 Lossy channels,"The data packet, along with ACKs can be lost. We need ways to detect the loss, and actions to take to recover from the loss.Sender side recovery: Wait for some timeout delay to receive an ACK, else retransmit.1. Start a timer each time a packet is sent2. Respond to a timer interrupt by retransmission. This introduces duplicate packets which are handled by RDT 2.23. Stop the timer"
Transport Layer.md,Pipelining,RDT 3.0 has a big performance implication due to its stop-and-wait protocol.We can boost performance by allowing the sender to send multiple packets without waiting for an ACK.- The range of sequence numbers must be increased as more in-transit unacknowledged packets are allowed- Need to buffer the packets which have transmitted but unacknowledged
Transport Layer.md,Go-Back-N (GBN),"We need a way to determine the range of sequence numbers needed. This depends on how error recovery is performed. Go-Back-N allows the sender to transmit multiple packets without waiting for ACK but this number is constrained to no more than some maximum N.A **sliding window** protocol:- [0, base-1] packets which are sent and acked- [base, nextseqnum-1] packets sent and not acked- [nextseqnum, base+N-1] packets that can be sent immediately should data arrive- > [base+n] cannot be used until a packet is ackedSender:- The timer is for the oldest unacked packet. This is restarted when we get an ack for a new sequence number.- All packets in the window are retransmitted on timeoutReceiver:- Out of order packets are discarded. By default, it will send ACK for the expected sequence number - 1 which it keeps track of internally."
Transport Layer Security.md,TLS Handshake,"1. TLS runs ontop of an existing TCP connection. Hence, establishing a TLS handshake requires going through the full round trip to set up a [three-way handshake](Notes/Transmission%20Control%20Protocol.md#Three-way%20Handshake) first.2. The client sends a number of specifications in plain text such as the version of TLS and list of supported ciphersuites3. The server picks the TLS protocol version, decides the ciphersuite, attaches its certificate and sends the response back4. Client generates a new symmetric key and encrypts it with the servers public key. **Up until now, the data has been exchanged in clear text with exception of the new symmetric key**.5. Server decrypts the symmetric key, checks the message integrity by verifying the MAC and returns an encrypted *Finished*.The entire process can add a lot of extra latency!"
Transport Layer Security.md,Session Resumption,"One way to reduce the extra latency, is to add ways to share the same negotiated secret key data between connections."
Transport Layer Security.md,Session ID (session caching),"A session ID can be generated by the server and sent to the client as part of *ServerHello*. The server maintains its own cache of session IDs and the negotiated session parameters for each peer while the client stores the session ID and sends it in subsequent requests as an indication to the server. Modified handshake:The problem with session caching, is the requirement for servers to store and maintain IDs for thousands of unique connections every day. This causes memory issues, and challenges on cache eviction."
Transport Layer Security.md,Session Ticket (stateless resumption),"Session tickets removes the requirement for servers to keep client session state, by generating a session ticket record on the server and sending it to the client. This ticket includs all the session data encrypted with a secret key by the server. This ticket can then be stored on the client safely and must be presented by the client to reuse session state."
Transport Layer Security.md,Authentication: Chain Of Trust,"Encryption is not so useful, if one is communicating in an encrypted tunnel with an attacker. There needs to be a way to verify the peer's idenity.- Alice trusts Bob, and they each have each others public key- Charlies wants to communicate with Alice. He asks Bob to sign his public key with his own private key.- Alice can check first that Bob signed Charlie's public key, and since she trusts Bob, also trusts Bobs decision to sign Charlie. Alice can then check the message with Charlie's public keyAs long as nobody in the chain gets compromised, it allows us to build and grow the list of trusted parties."
Transport Layer Security.md,Certificate Authorities,"A trusted third party trusted by both the owner of the certificate and the party relying on the certificate. They help to store and verify each certificate, so one does not need to do so manually for every single website.The browser specifies which root CAs to trust, and the burden is then on the CA to verify each site they sign:"
Transmission Control Protocol.md,TCP Connection,
Transmission Control Protocol.md,Three-way Handshake,"Every TCP connection begins with a 3-way handshake:1. SYN: Client picks a random sequence number *x* and sends a TCP segment with SYN bit set to 1, which may also include additional TCP flags and options.2. SYN ACK: Server increments x by 1 and uses this in the acknowledgement number. It picks its own random sequence number for y, appends its own set of flags and options3. ACK: Client increments both x (sequence number) and y (acknowledgement number) by 1 and completes the handshake"
Transmission Control Protocol.md,Performance Implications,The client can send a data packet immediately after the ACK packet but the server must wait for the ACK packet before it can dispatch any data.Each new connection will have a full roundtrip of latency before any application data is transferred.
Transmission Control Protocol.md,TCP Keep-Alive,"Rather than having to complete 3-way handshake for each data transfer, allow long-lasting connections to immediately transfer application data."
Transmission Control Protocol.md,TCP Fast Open (TFO),Allow data transfer within the SYN Packet.
Transmission Control Protocol.md,Connection stream,- Maximum Segment Size (MSS): max data that can be placed in a segment- Maximum Transmission Unit (MTU): largest link layer frame that can be sent
Transmission Control Protocol.md,Closing,"Server sends the FIN when no more remaining data to send. With the client ACK, the connection is closed."
Transmission Control Protocol.md,TCP Segment Structure,
Transmission Control Protocol.md,Sequence Numbers,"Consider a 500,000 byte file with MSS = 1000 bytes:Sequence numbers are over the bytes and not the segments. The first segment gets a sequence number 0, second segment gets a sequence number 1000, etc."
Transmission Control Protocol.md,Acknowledgement Numbers,"It is the sequence number of the next byte of data that the host is waiting for.TCP provides **cumulative acknowledgements** where an acknowledgement number *y* represents all segments < y have been successfully delivered.*Example: If A received a segment (0-535) and a segment (900-1000), it places 536 as its acknowledgement number.*"
Transmission Control Protocol.md,Reliable Data Transfer,
Transmission Control Protocol.md,Estimating Timeout,TCP uses a timeout retransmit mechanism to recover from lost segments similar to [RDT 3.0 Lossy channels](Notes/Transport%20Layer.md#RDT%203.0%20Lossy%20channels).TCP calculates the estimated RTT based on samples of RTT which are taken approximately once every RTT.
Transmission Control Protocol.md,Retransmission,Scenarios:
Transmission Control Protocol.md,TCP Fast Retransmit,"The timeout period can be relatively long, delaying retransmission of the lost packet. The sender can detect packet loss well before the timeout event by noting **duplicate ACKs**. When *3* duplicate ACKs are received, a fast retransmit is performed before the timer expires."
Transmission Control Protocol.md,Flow Control,The TCP receiver places received bytes in a receive buffer. Flow control is a mechanism to prevent the sender from overwhelming the receiver with data it may not be able to process—the receiver may be busy.
Transmission Control Protocol.md,Receive Window (rwnd),Transmit a receive window value in each ACK packet between both sides to communicate the size of available buffer space to hold incoming data such that the buffer does not overflow:
Transmission Control Protocol.md,Silly window syndrome,"1. When the receiver consumes data slowly, the window becomes smaller to the point where the data transmitted is smaller than the packet header resulting in inefficient data transfer (thrashing).2. When the sender creates data slowly, a small packet that does not fully utilise the maximum segment size is sent also resulting in inefficient data transfer."
Transmission Control Protocol.md,Nagle's algorithm (case 1),"Applications such as telnet/rlogin generates a 41 byte TCP packet for each 1 byte of user data.1. Each TCP connection can have only one outstanding (i.e., unacknowledged) small segment (i.e., a tinygram)2. While waiting - additional data is accumulated and sent as one segment when the  ACK arrives, or when maximum segment size can be filled3. Self-clock: the faster ACKs come, the more often data is sent. Thus automatically on slow WANs fewer segments are sent"
Transmission Control Protocol.md,Delayed acknowledgements,"Rather than sending an ACK immediately, the TCP receiver waits up to 200ms. This prevents the sender from sliding its window. Traffic is reduced and potentially more data can be piggy backed on the ACK.*Host requirements RFC states the delay must be less than 500ms which is the standard timeout interval. This ensures that retransmit is not triggered.*"
Transmission Control Protocol.md,Stateful: Order of transmission,"TCP is capable of transmitting messages spread across multiple packets without explicit information from the packets themselves.How? TCP is stateful and connection state is allocated on both ends of the connection, allowing data to be sequenced, delivered in order and retransmitted when lost."
Transmission Control Protocol.md,Congestion Control,"A mechanism to throttle senders in the face of network congestion (rather than that of application processing speed in flow control).Issues caused by congestion:- Large queuing delays occur as sending rate nears the link capacity, as router buffers start to fill up- Sender must perform retransmissions for dropped packets- Unneeded retransmissions (premature timeout) use up available bandwidth"
Transmission Control Protocol.md,Congestion Window (cwnd),"Sender side limit on the amount of unacknowledged data the sender can send into the network.Since there is also the rwnd, the upper bound of unacknowledged data is:$LastByteSent - LastByteAcked \le min(cwnd,rwnd)$"
Transmission Control Protocol.md,Algorithm,"Approach: each sender limit the rate at which it sends traffic into its connection as a function of perceived network congestion. (increase if less, decrease if more congestion)- Duplicate ACKs implies a lost segment and and a lost segment implies congestion- ACK indicates successful receive, rate can be increased"
Transmission Control Protocol.md,Slow Start,
Transmission Control Protocol.md,Performance Implications,"The maximum amount of un-acked data is $min(rwnd, cwnd)$. The server can send up to that amount of network segments to the client at which point it must stop and wait for an acknowledgement.The performance of the connection is often limited by the round trip time (latency) or the congestion windowTime to reach a cwnd = N:"
Transmission Control Protocol.md,Congestion Avoidance,"When a timeout occurs, $cwnd = 1$ and `ssthresh` is set to `cwnd at duplicate ACK/2`, restarting the slow start process. We can use `ssthresh` to figure out when we are nearing a ""reckless"" value and stop the slow start process. How should cwnd be adjusted after this?When `ssthresh` is reached, TCP changes to a linear increase in the cwnd rather than exponential increase."
Transmission Control Protocol.md,Fast Recovery,"The slow start process does not restart (cwnd restart at 1 MSS), instead the cwnd is increased by 1 MSS for each duplicate ACK for the missing segment. This is because we can be sure that the receiver can handle at least the ssthresh + duplicate ACKs number of packets.3 duplicate ACKs occur at round 12. $ssthresh = 12/2=6$.- TCP Renoe: $cwnd = ssthresh + 3 = 9. Followed by linear increase- TCP Tahoe: $cwnd = 1$. Exponential increase followed by linear increase at ssthresh."
Transmission Control Protocol.md,Summary,
Transmission Control Protocol.md,Throughput,
Transmission Control Protocol.md,Bandwidth Delay Product,"If either the sender or receiver exceeds the maximum unacknowledged data, they will have to wait before they can send any more.To maximize throughput, *send so much data that there is always an ACK returning back to the sender at the same time we are about to send a data packet*.$BDP = \text{Data Link Capacity}\times\text{Round Trip Time}$Example, 10 Mbps available bandwidth and 100ms RTT$BDP=10\times10^6\times0.1=1\times10^6$ bitsThe window size needs to be at least this size to saturate the data link."
Transmission Control Protocol.md,Fairness,
Transmission Control Protocol.md,Head-of-Line Blocking,"If one of the packets is lost en route to the receiver, then all subsequent packets must be held in the receiver’s TCP buffer until the lost packet is retransmitted and arrives at the receiver. Because this work is done within the TCP layer, our application has no visibility into the TCP retransmissions or the queued packet buffers, and must wait for the full sequence before it is able to access the data. Instead, it simply sees a delivery delay when it tries to read the data from the socket.>[! Note]>Benefits>1. Applications do not need to deal with packet reordering and reassembly>>Cons>1. Introduces unpredictable latency variation in packet arrival times (jitter)>2. Application may not need reliable or in-order delivery"
Transmission Control Protocol.md,Exercises,"TCP uses delayed ACKs instead of sending and ACK directly after a correctly received packet. Answer the two following questions related to delayed ACKs in TCP.a) An ACK must not be delayed more than 500 ms. Why?500ms is the amount of time before retransmission timeout.b) Assume that a TCP segment arrives with the expected sequence number. The previous segment arrived in correct order and it has not been ACKed yet. What will the receiver do now?Receiver will send a TCP packet with acknowledgement number = the latest segment sequence number + 1TCP uses both flow control and congestion control. Explain the overall difference between these. What do they mean? What are their purposes?Flow control is used to throttle the sender in the case where the receiver is unable to handle the rate of data being sent. Congestion control is used to throttle the sender in the case where there is congestion in the network link.An application uses TCP and sends data in full size windows (65 535 bytes) over a 1 Gbps channel having a one-way delay of 10 ms. The transmission time can be neglected.a) What is the maximum throughput that can be achieved?1 window of data can be sent every RTT:$65535\times8/(20\times10^-3)=2621400 \ bits/s$b) What channel utilization can be achieved, i.e., how large part of the available bandwidth can be used?$\frac{26214000}{1\times10^9}=2.6\%$A client application establishes a TCP connection to a server application to transfer 15 kB of data. The (one-way) delay is 5 ms, RTT (round-trip time) is 10 ms, and the receive window (rwnd) is 24 kB. Assume that the initial congestion window is 2 kB. There is no congestion in the network, the transmission time can be neglected, and the connection establishment phase can be neglected. Calculate the total transfer time.$$\begin{align}\\ \text{Round 1 2kb of data sent}: cwnd = 2*2=4\\ \text{Round 2 4kb of data sent}: cwnd = 4*2=8\\ \text{Round 3 8kb of data sent}: cwnd = 8*2=16\\ \text{Last 1kb sent in round 4}: 10+10+10+5 = 35ms\\ \end{align}$$a,b)c. 3 duplicate ACKsd. Timeoute. 32 segmentsf. 21g. 15h. 7i. ssthresh = 4, cwnd = 7j. ssthresh = 21, cwnd = 4k. $1+2+4+8+16+21=52$. Round 22 will have sent 21 packets assuming that we are able to successfully send at least the number of data packets `ssthresh` dictates.a.$$\begin{align}\\&\text{Packets sent per cycle}=\frac{W}{2}+(\frac{W}{2}+1)+...+W\\&=\frac{3W}{2}\times(\frac{W}{2}+1)\div2\\&=\frac{3W^2}{8}+\frac{3W}{4}\\&\text{1 packet loss per cycle:}\\&L=\frac{1}{\frac{3W^2}{8}+\frac{3W}{4}}\end{align}$$b.$$\begin{align}&Rate = \text{Packets transferred per unit time}\\&\frac{1}L=\frac{3W^2}8+\frac{3W}4\\&\frac{1}L\approx\frac{3W^2}8\\&W=\sqrt\frac{8}{3L}\\&\text{Avg Rate} = \frac{3W}{4RTT}MSS\\&\approx\frac{1.22MSS}{RTT\sqrt L}\end{align}$$a.$$\begin{align}&\text{Max throughput}=10\times10^6\times150\times10^{-3}=150\times10^{4}\\&x\times MSS=150\times10^{4}\div8\\&x=125\end{align}$$b.$$\begin{align}&\text{Avg window size} = \frac{3}4W_{max}=93.75\\&\text{Avg throughput} = \text{Avg window size}/RTT\\&=93.75\times1500\times8/(150\times10^{-3})\\&=7,500,00=7.5\ Mbps\end{align}$$c.$$\begin{align}&\text{cwnd after packet loss}=125/2=62.5\\&62.5\times150\times10^{-3}=9.375s\end{align}$$"
Transaction Management.md,SQL Transactions,- A new transaction starts with `BEGIN`- Transactions are stopped with either a `COMMIT` or `ABORT`- `COMMIT`: changes are saved- `ABORT`: changes are undone
Transaction Management.md,ACID Properties,
Transaction Management.md,Atomicity,A transaction is either performed in its entirety (can be in steps) or not performed at all.
Transaction Management.md,Logging,DBMS logs all actions so that it can undo the actions of aborted transactions
Transaction Management.md,Shadow paging,"Make copies of pages, perform changes on these copies. Only when transaction commits, the page is made visible to others."
Transaction Management.md,Consistency,
Transaction Management.md,Database consistency,A database is in consistent state if it obeys all of the consistency (integrity) constraints defined over it. A database may be inconsistent in between states.
Transaction Management.md,Transaction consistency,Database is in consistent state even if there are a number of concurrent transactions
Transaction Management.md,Isolation,A transaction should appear as though it is executed in isolation from other transactions. An executing transaction cannot reveal its results to other concurrent transactions before its commitment.
Transaction Management.md,Concurrency control protocol,- Pessimistic: Do not let problems arise in the first place (prevention)- Optimistic: Assume conflicts are rare and deal with them when they happen (detection and recovery)
Transaction Management.md,Durability,Changes applied to the database by a committed transaction must persist in the database.
Transaction Management.md,Primitive Operations,
Transaction Management.md,Failures,
Transaction Management.md,Types of Failures,
Transaction Management.md,Transaction failures,"- Logical Errors: Transaction cannot complete due to some internal error condition (e.g., integrity constraint violation).- Internal State Errors: DBMS must terminate an active transaction due to an error condition (e.g., deadlock)."
Transaction Management.md,System failures,"- Software Failure: Problem with the DBMS implementation (e.g., uncaught divide-by-zero exception).- Hardware Failure: The computer hosting the DBMS crashes (e.g., power plug gets pulled, disk crash). They can be recoverable or non-recoverable"
Transaction Management.md,Recovery Algorithms,"Recovery must kick in to ensure the database satisfies the ACID properties in the case of failure. They must carry in 2 parts:1. Actions during normal transaction processing to ensure that the DBMS can recover from a failure.2. Actions after a failure to recover to a state that ensures atomicity, consistency and durability> Ensuring atomicity and durability -- Logging:> The database stores additional files called *log files*. Logs record every action of each transaction and are append only."
Transaction Management.md,Undo Logging,"Idea: undo the effects of transactions that may not have completed before failure.> [!Rules]> With undo logging, there are a set of rules the DBMS must follow. Order of writing to disk:> 1. Log records indicating changed database elements> 2. Changed database elements themselves> 3. COMMIT log record"
Transaction Management.md,Recovery without checkpoints,"All undo commands are idempotent, if failure occurs during recovery, we can simply restart."
Transaction Management.md,Checkpointing,Use periodic checkpoints to prevent having to read the entirety of the log file for recovery. Any transactions executed before the checkpoint will have finished and there will be no need to undo them.Problem- The database is frozen while performing checkpointing. Active transactions may take a long time to commit or abort and will result in variant performance of the DBMS.
Transaction Management.md,Non-quiescent Checkpointing,Start checkpointing at any time using the current incomplete transactions.
Transaction Management.md,Recovery with checkpoints,"1. Scan backwards identifying transactions which did not commit.2. If we reach END CKPT, we know that the only transactions which may not have committed must be those after the `START CKPT`. Thus, we can stop at `START CKPT`3. If we reach `START CKPT` first, we failed during checkpointing and need to search up till the earliest `START T` of those in the checkpoint, because we know those are the transactions active at the start of the checkpoint4. Undo uncommitted transactions and ignore those that have committed."
Transaction Management.md,Limitations,Cannot commit a transaction without first writing all its changed data to disk. This means we will need many disk I/O for each transaction.
Transaction Management.md,Redo Logging,"Do not write all changed data to disk before committing. Write to the main memory log file all the changes to the DB, and commit before any changes are written to disk. Perform recovery by redoing effects of committed transactions before the crash.> [!Rules]> If a transaction modifies X, then both <T,x,v> and COMMIT T must be written to disk before OUTPUT(X). Order of writing to disk:> 1. Log records indicating the changed elements> 2. COMMIT T log record> 3. Changes to the elements themselves"
Transaction Management.md,Recovery without checkpoints,
Transaction Management.md,Checkpointing,"Start checkpointing for all active transactions. When we see an `START CHECKPOINT`, we can be sure that all prior transactions have been recorded to the disk and there is no need to redo them.*Example where we do not need to redo the actions for T1:*"
Transaction Management.md,Recovery with checkpoints,"1. If we see a `END CHKPT` in the log, we can be sure that changes made by transactions that have committed before the `START CHKPT` is already in disk, and we can ignore them. If no `END CKPT` in logs, we need to search back to next to last `START CKPT`.2. From `START CKPT (T1, ... Tk)` we *cannot* be sure that any transaction that is among the $T_i$ or those started after this checkpoint log have been written to disk. **We must search back until the earliest $T_i$ and redo.**3. However, if $T_i$ 's commit message is not found, we write `ABORT Ti` as it must not be redone."
Transaction Management.md,Limitations,It requires all modified blocks to be kept in buffers until the transaction commits and the log records have been flushed. This increases the average number of buffers needed by transactions.
Transaction Management.md,Undo/Redo Logging,"Increase flexibility by maintaining more information on the log. <T,X,v,w> now stores both old and new values in the log.> [!Rule]Before modifying any DB element X, the log record must be written to the disk.We can commit at any time after the <T,B,8,6> record has been written to the disk.Recovery process:1. Redo all committed transactions top-down2. Undo all uncommitted transactions bottom-upThis is because we may have committed transactions with some of the changes not on disk as well as uncommitted transactions with some of the changes on disk."
Transaction Management.md,Checkpointing,- The `END CKPT` can appear anywhere after the `START CKPT`
Transaction Management.md,Recovery with checkpoints,"We are sure we do not need to check any further than the `START TRANSACTION` of active transactions in the checkpoint as the corresponding `END CHECKPOINT` ensures that all changes prior to the `START` have been flushed to the disk.1. If crash occurs at the end, T2 and T3 are identified as committed and we redo actions starting from `START CKPT`2. If crash occurs before `COMMIT T3`, T2 is committed but. T3 is not. Redo T2 from `START CKPT` and undo T3 until `START T3`.3. If crash occurs before `END CKPT`, any changes made by previous transactions may not have been written to disk. We need to search back to the next to last `START CKPT`, redo committed transactions and undo incomplete ones up till their corresponding `START T` logs."
Transaction Management.md,Practice Problems,"a. We know that at an `END CHKPNT`, all dirty buffers from the corresponding `START CKPT` are written to diskA: 21B: 40, 41C: 30,31,32,33D: 50,51,52b. All uncommitted transactions. T1, T6c. All committed transactions. T3, T4, T5d.A: 21B: 41. T4 redoneC: 31. T1 undone, T3 redoneD: 52. T5 redone."
Time Abstractions.md,Clock drift,Every clock $C$ has an error $\rho$. The error bounds for *1* unit of time is defined as: $$1-\rho\le \frac{dC}{dt}\le1+\rho$$
Time Abstractions.md,Leader-leases,"A way to support faster reads by allowing direct reads from the leader's local state without any communication with the followers.Problem: a leader can be disjoint from the rest of the network, causing another leader to be elected that changes the state of the systemLeader leases ensure that there can only be 1 leader at a time and during this time reads from local state are allowed.- $t_L$: time since prepare was sent out. Process $p_1$ must start counting from $t_0$ to give a conservative estimate of possibly how long it can stay the leader. The actual time it can be a leader is starts at $t_2$ when majority promise received and ends at $t_3$ which assumes the clock is faster (lesser time) i.e. 1 unit of time is $1-\rho_1$- $t_{prom}$: time since promise was sent out. Process $p_2$ will reject rounds assuming the clock is slower (waits longer) within the next 10s + time drift $10\rho$"
Time Abstractions.md,Interval Clocks,"$C_i$ represents an interval $[lo, hi]$- Wait until the current low bound of timestamp of $p_1$ crosses the previous high bound for the operation timestamp to ensure linearizability $t_1 < t_2$"
Threads.md,Implementation models,Want to support an arbitrary number of threads but the OS can only support a limited number due to physical constraintsLogical (user) threads: Created in user space and allows users to create as many threads as they wantKernel threads (physical): Created in kernel space and slower to create and manage than user threads;   Resources are eventually allocated in kernel threadsWays to map logical to physical:- Many to one: can result in blockage of thread when one is in use- One to one: creating user threads = creating kernel threads; not very efficient- Many to many: not easy to decide an efficient mapping
Threads.md,Practice Problems,"*Explain the difference between a single-threaded and a multi-threaded process.*- Threads in a process share code, data and heap regions of memory, whereas stack space is unique to each thread. Also, each thread has its own Thread Control Block (TCB), similar to a PCB.- In a single-threaded process, there is only one thread of execution, and hence it is identical to a process.- In a multi-threaded process, the individual threads can execute concurrently, thus increasing system throughput; when one thread of a process is blocked (“waiting” state), another thread can continue its execution (“running” state)."
Thread Level Parallelism.md,Multicore Processors,Not to be confused with *multiprocessors*:- Multiprocessors: 2 or more CPUs in the same computer. Executes multiple programs faster.- Multicore: 2 or more processors in a single CPU. Executes a single program faster through [](Notes/Threads.md#^7d353c%7Cmulti-threading)
Thread Level Parallelism.md,Challenges,
Thread Level Parallelism.md,ILP wall,
Thread Level Parallelism.md,Power wall,Overcome power wall using multiple slow cores- Cores running at lower clock frequency and lower voltage can still deliver the desired performance using less power- Scale up the number of cores rather than frequency
Thread Level Parallelism.md,Memory wall,Widening gap between compute bandwidth and memory bandwidth could not be bridged - resulting in memory latency.Overcome memory wall with memory parallelism via multiple threads
Thread Level Parallelism.md,Interconnect problem,
Thread Level Parallelism.md,Cache Coherence,
Support Vector Machines.md,Linear separation,A set of data points can be separated with a line (also called a *hyperplane*) with each group forming a class.Problem: there can be many acceptable solutions (*structural risk*)
Support Vector Machines.md,Reducing structural risk,"If we add *margins*, we can restrict the possible lines we can choose from.By finding the *maximum* margin we can apply on the linear separator, we are finding the largest distance between the edges of the 2 classes. This gives it a better chance of classifying new data correctly (i.e. if the new data point is on 1 side of the separator, it is closer to one set of data points, this closeness is the intuition for why it should be classified as the same as this set of data points)"
Support Vector Machines.md,What if we cannot find a linear separation?,"By introducing more dimensionality into the data, the idea is that we can find some dimension in which a linear separation can be found."
Support Vector Machines.md,Kernels,"How do we decide how to transform the data into higher dimensions? This is done through *kernel functions*. Kernels provide a possible way to increase the maximization of the margins we can find, but in exchange requires more computing resources.1. Choose a suitable kernel function2. Compute $a$3. Support vectors are the $x_i$ corresponding to $a_i \ne 0$. Support vectors with 1 class has indicator function output of 1 and for the other class the output is -1 (the margin of the street)4. Classify the new data points via the indicator function. > 0 is positive class, < 0 is negative classRather than actually computing the data points transformed to higher dimensions, kernel functions only require the original data be used (observe the common kernels only involve dot products of the original data), saving lots of computation.*What if it is already linearly separable? Are there any benefits for using the kernel?*Can find a wider margin of separation which would generalize better."
Support Vector Machines.md,Slack,"Although we can technically always find a separation, we might not want to do so as this could result in increased generalization. Allow some misclassifications or data to enter the margin to achieve slack.- A support vector has $0<a_i<C$ and lies on the margins.- A point between the boundary and the margin has $a=C$- Points outside the margin have $a=0$ and are useless"
Support Vector Machines.md,Advantages,"- Guaranteed to find the global minima, unlike the learning in [Neural Networks](Notes/Neural%20Networks.md)."
Strings.md,Immutability,"String assignment will not create a new copy of the string. Only string concatenation will create a new buffer for the string.```gofunc main() {x := ""hello""y := x// x and y share the same underlying memoryy += ""world""// now y uses a different buffer// x still uses the same old buffer}```Immutability means that individual characters or bytes cannot be reassigned directly.```gofunc main() {str := ""hello""fmt.Println(str[1]) //101 (ascii of ‘e’)str[0] = 'a' //compile error}```We can modify the string by creating a copy:"
Strings.md,Goroutine unsafety,
Strings.md,Runes,len(str) returns the number of bytes for the string. len(rune(str)) returns the actual number of characters.
Strategy Pattern.md,Problems we want to solve,1. A set of **interchangeable** algorithms or objects that can be decided at run-time2. Extensible set of strategies: [Open-Closed Principle](Open-Closed%20Principle)Context refers or uses the Strategy interface for performing the algorithm. A and B classes implement the Strategy interface which gives the concrete algorithms.
Strategy Pattern.md,Pros,1. Encapsulation2. Hides implementation3. Allows behaviour change at runtime
Strategy Pattern.md,Cons,1. Complexity if overused
Storage.md,Random Access Memory,Memory is said to be RAM if  the time to access the data is the same irrespective of the physical locations of the data.
Storage.md,Volatility,A volatile storage would mean that data is lost when it is unpowered.
Storage.md,Types of Storage,- [Register](Register)- [Cache](Notes/Cache.md)- [Main Memory](Main%20Memory)- [Disk](Notes/Disk.md)
Stock Valuation.md,Calculating stock price,
Statistics.md,Probability vs Likelihood,
SMTP.md,Difference with HTTP,- SMTP is a push protocol (sending mail pushes the file to the server)- SMTP requires each message to be in 7 bit ASCII format.- All objects are placed in one message rather than having each object on its own response message
SMTP.md,Message Format,RFC 5322 specifies the exact format for mail header lines which are different from the commands used in SMTP (MAIL FROM etc.).
SMTP.md,Exercises,"Describe shortly how an email message in sent from a sending to a receiving email client (user agent), such as MS Outlook or Mozilla Thunderbird . From the description, it should be clear what application protocols and what systems are involved in the transfer.User agent uses SMTP to send the email message to the user's mail server. The message is placed on a message queue to be sent. The user mail server uses SMTP to send the email message to the receiver mail server.An email message is sent from an email client (MUA) to a server for outgoing email .a) Which protocol is used for the transfer ?SMTPb) The communication delay between the client and the server is 2.5 milliseconds ( in other words, it takes 2.5 milliseconds from that the sender has started sending the message until the complete message has reached the receiver ) . How long time does it take before the entire transfer of the email message to the outgoing server has been completed ? The time it takes to set up and tear down a TCP connection should not be included in the calculation. The message has one recipient.$2.5 \times 6 \times 2 = 30ms$c) Suppose that the message has four recipients . How long time does it take then ?$(2.5\times2\times2) + (2.5\times2\times7) =45ms$"
Statistical Inference.md,Bayes Rule,
Statistical Inference.md,Maximum Likelihood Parameter Estimate,"The goal of MLE is to find the optimal way to fit a distribution to the data, i.e. the best distribution (parameters) to maximise the probability of observing our data."
Statistical Inference.md,Naïve Bayes Classifier,"Take the dimensions of the data as **independent of each other** such that the joint probability of the feature set is broken up into the product of the probabilities of each feature. Naive Bayes is thus naive due to its indiscretion towards these combined probabilities, even though they may in fact be correlated to each other."
Statistical Inference.md,MLE for Linear Regression,[Linear Regression](Notes/Regression.md#Linear%20Regression) used the intuition of minimizing the least squared error from the data to the model to find the best fit line. Here we can see that finding the maximum likelihood also leads us to the same conclusion:
Statistical Inference.md,MLE for Gaussian,
Statistical Inference.md,MLE for Bernoulli,
Statistical Inference.md,Maximum a posteriori Estimation,"Finding the most likely distribution parameter, given the data.- From the equation, we can see that MAP means maximising the product of the likelihood and the **prior** probability (some known information of the distribution)."
Statistical Inference.md,Classification,"With our parameters known, we can make classifications on new data.Will I play orienteering given the forecast? i.e. yes/no given that the new forecast is rainy."
Statistical Inference.md,Maximum Likelihood Classification,Classify based on the highest likelihood $P(x|y)\forall y$$$\begin{align}P(rainy|y=yes)=\frac{3}{9}\\P(rainy|y=no)=\frac{2}{5}\\y_{ML}=NO\end{align}$$
Statistical Inference.md,Maximum a Posteriori Classification,Classify based on the highest posterior probability $P(y|x)\forall y$$$\begin{align}P(yes|rainy)=\frac{P(rainy|yes)P(yes)}{P(rainy)}\\=\frac{(3/9)(9/14)}{5/14}=0.6\\P(no|rainy)=\frac{P(rainy|no)P(no)}{P(rainy)}\\=\frac{(2/5)(5/14)}{5/14}=0.4\\y_{MAP}=YES\end{align}$$
Statistical Inference.md,Naive Bayes Classifier,"Take each feature as iid, will I go play orienteering given x?:$$\begin{align}x&=(sunny, cool, high, true)\\P(y=yes|forecast=x)&= \frac{Pr(x|y=yes)P(y=yes)}{P(forecast=x)}\\argmax_{y\in Y}(P(yes|x))&=argmax_{y\in Y}(P(x|yes)P(yes))\\&=0.005\\argmax_{y\in Y}(P(no|x))&=argmax_{y\in Y}(P(x|no)P(yes))\\&=0.021\\y_{MAP}&=0.021=NO\end{align}$$"
Statistical Inference.md,Bayesian Estimation,"ML and MAP produce point estimates for $\theta$, which assumes that the true distribution follows these parameters. Bayes estimation uses the data to estimate a probability distribution for $\theta$ rather than just 1 point estimate. This gives a posterior estimate given the data rather than given the $\theta$."
Statistical Inference.md,References,- https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d
Signal Chain Subsystem.md,Factors affecting signal transfer,
Signal Chain Subsystem.md,Signal Skew,"When the signal of one or more data lines takes a different amount of time to reach the receiver, resulting in wrong data latched by the receiver."
Signal Chain Subsystem.md,Cross Talk,"The undesired coupling of signals from one circuit to another, resulting in electrical interference."
Signal Chain Subsystem.md,Parallel Data Transfer,"Multiple bits of data are transferred simultaneously between 2 devices.- Higher transfer rate- Prone to signal skew and crosstalkSynchronous data transfer only, as a strobe signal is needed to inform the receiver of when to latch into the data (e.g. rising edge of the signal)."
Signal Chain Subsystem.md,Serial Data Transfer,Data is transferred one bit at a time over a single data line.- Less effects of signal skew and crosstalk due to less wires supports higher frequency clocking- Lower data transfer rate
Signal Chain Subsystem.md,Modes,
Signal Chain Subsystem.md,Synchronous,A common clock signal between transmitter and receiver is used to synchronise the data transfer.Master-Slave configuration: master provides the clock signal
Signal Chain Subsystem.md,Serial Peripheral Interface (SPI) Communication,
Signal Chain Subsystem.md,Asynchronous,- No common clock is provided between transmitter and receiver.- Receiver must know transmitting clock rate prior to transmission and the number of bits in each data packet- Uses `SYNC` words within the data packets to indicate START/STOP
Signal Chain Subsystem.md,Universal asynchronous receiver-transmitter (UART),"A hardware device that is configurable to send data between a transmitter and receiver.The UART protocol:Parity bit workings- Even parity scheme: there should be even number of ""1s"" in the data field and parity field combined. If there are odd number of 1s in the data, parity bit transmitted should be 1- Vice versa for odd parity scheme"
Signal Chain Subsystem.md,RS232 Transmission standard,Uses UART protocol transmission but- uses a different electrical standard interface. i.e. Logic 1 is represented by a voltage smaller than Logic 0.- Logic 1 can be -15V while Logic 0 can be +15VInvert the receiving signal and we are able to read the data using the UART protocol equivalently.An excellent tutorial:https://www.youtube.com/watch?v=AHYNxpqKqwo
Signal Chain Subsystem.md,Data Transfer Mechanism,
Signal Chain Subsystem.md,Polling,
Signal Chain Subsystem.md,Interrupt Driven,[Interrupts](Notes/Interrupts.md)
Set Theory.md,Defining a set,
Set Theory.md,Operations,
Set Theory.md,Set stuff,
Set Theory.md,Types,
Set Theory.md,Power Set,"A power set is a set S whose elements are all subsets of S.$P(\{1,3,5\})=\{\emptyset,1,3,5,(1,3),(1,5),(3,5),(1,3,5)\}$"
Set Theory.md,Cardinality,Number of elements in a set
Set Theory.md,Ordered-Pair,
Set Theory.md,Cartesian Product,
Set Theory.md,Type Constructor,
Set Theory.md,Relations,
Set Theory.md,Domain,
Set Theory.md,Restriction,
Set Theory.md,Subtraction,
Set Theory.md,Range,
Set Theory.md,Restriction,
Set Theory.md,Subtraction,
Set Theory.md,Functions,
Set Theory.md,Partial functions,
Set Theory.md,Total function,
Set Theory.md,Injective,
Set Theory.md,Surjective,
Set Theory.md,Bijective,
Sequence Diagrams.md,Common notation errors,"**Activation bar too long:** The activation bar of a method cannot start before the method call arrives and a method cannot remain active after the method has returned. In the two sequence diagrams below, the one on the left commits this error because the activation bar starts _before_ the method `Foo#xyz()` is called and remains active _after_ the method returns.**Broken activation bar:** The activation bar should remain unbroken from the point the method is called until the method returns. In the two sequence diagrams below, the one on the left commits this error because the activation bar for the method `Foo#abc()` is not contiguous, but appears as two pieces instead."
Sequence Diagrams.md,Object Creation,Notation:-   The arrow that represents the constructor arrives at the side of the box representing the instance.-   The activation bar represents the period the constructor is active.
Sequence Diagrams.md,Object Deletion,"**UML uses an `X` at the end of the lifeline of an object to show its deletion.**Although object deletion is not that important in languages such as Java that support automatic memory management, you can still show object deletion in UML diagrams to indicate the point at which the object ceases to be used.Notation:"
Sequence Diagrams.md,Loops,"Notation:The `Player` calls the `mark x,y` command or `clear x y` command repeatedly until the game is won or lost."
Sequence Diagrams.md,Self Invocation,"**UML can show a method of an object calling another of its own methods.**Notation:In this variation, the `Book#write()` method is calling the `Chapter#getText()` method which in turn does a _call back_ by calling the `getAuthor()` method of the calling object."
Sequence Diagrams.md,Alternative Paths,**UML uses `alt` frames to indicate alternative paths.**Notation:
Sequence Diagrams.md,Calls to Static Methods,"Method calls to `static` (i.e., class-level) methods are received by the class itself, not an instance of that class. You can use `<<class>>` to show that a participant is the class itself.In this example, `m` calls the static method `Person.getMaxAge()` and also the `setAge()` method of a `Person` object `p`."
Risk Analysis.md,Goal Structured Notation,"A graphical notation to build a safety case, which states that the system is safe in a specific environment."
Risk Analysis.md,Fault Tree Analysis,Top down approach: We start from the event that we want to avoid and analyse the factors that can contribute to its occurrence.Represented with boolean (AND/OR gates) logic:
Risk Analysis.md,Event Tree Analysis,Bottom up approach by mapping responses to events to the outcome success/failure
Regression.md,Linear Regression,"Take the function as a form of linear combination of variables, where d is the number of variables or dimension.$$f(x)=\sum_{i=0}^dw_ix_i=w^Tx$$"
Regression.md,Least Squares,Least squares regression attempts to find the best fit line based on the quantity of w which minimizes the MSE.
Regression.md,Mean Square Error (MSE),"One method to measure the error of N samplesWe can measure the distance of each predicted value and the actual observed value. The sum of these distances is called the **Residual Sum of Squares (RSS)**. The RSS depends on the slope and intercept of the line. To minimize the error, we can take the derivative of the RSS to find the minimum point.Problem: high influence of outliers to the error value."
Regression.md,Ridge regression,"When we only have a small number of data points, least squares will give us an estimate that is high in [variance](2421%20Machine%20Learning.md#Bias%20vs%20Variance). Rather than trying to minimize RSS alone, we can introduce some penalty (bias). We can use the slope of the line (the estimated weights) as the penalty.As $\lambda$ increases, the penalty caused by the slope becomes greater. When minimizing the quantity, ridged regression creates a line that has a gradient that is asymptotically close to 0."
Regression.md,Lasso Regression,"Similar to ridge regression, but the minimized term is now:$$RSS+\lambda\sum_{i=1}^d|w_i|$$which is the sum of the weights rather than the squared sum of weights. This allows some coefficients to be able to shrink to 0 rather than just being asymptotically close to 0 as $\lambda$ increases.*Variable selection property*: for models which include a lot of ""useless"" parameters, the ability to shrink them to 0 makes lasso regression better than ridge regression."
Regression.md,Link to bias vs variance trade off,"$$\sum_{i=1}^d|w_i|\le s$$When s = 0, all weights are zero; the model is extremely simple, predicting a constant and has no variance with the prediction being far from actual value, thus with high bias.As s increases, all wi increase from zero toward their least square estimate values:- Steadily decreasing the training error to the ordinary least square RSS- Bias decreases as the model continues to better fit training data.- Values of wi then become more dependent on training data, thus increasing the variance.- The test error initially decreases as well, but eventually start increasing due to overfitting to the training data."
Regression.md,k-NN Regression,"Rather than trying to find a best fit line (equation), we use the training data as sample points. For new data, consider the k closest points to a its given value of X and take the average of the values as the prediction$$f(x)=\frac1k\sum_{x_i\in N_i}y_i$$- Non parametric approach may be better if its the true form of the data- Parametric methods are worse when there are smaller number of observations per predictor and are also less interpretable"
Regression.md,Random Sampling Consensus (RANSAC),"RANSAC allows for a robust model fit to a dataset which contains outliers.1. Randomly select a sample of points from the data and build the model with this subset2. Determine the set of data points which are within a distance threshold of the model. This is called the inliers.3. If the number of inliers is greater than some threshold, re-estimate using all points in this threshold and stop4. Else select a new random subset and repeat"
Recurrence Equations.md,Iteration Method,Solve using algebra> [!NOTE] Important formulas to remember> Arithmetic summation series:> $$a+a+d+a+2d...+a+(n-1)d=\frac{n}{2}(2a+(n-1)d)$$> Geometric series summation:> $$a+ar+ar^2+...ar^{n-1}=a(\frac{1-r^n}{1-r})$$
Recurrence Equations.md,Substitution Method,Guess and check strategy1. Guess the solution form2. Prove the base case (use a base case which is true)3. Prove by mathematical induction:1. Assume true for n = k2. Prove true for k+1
Recurrence Equations.md,Master Method,
Recurrence Equations.md,Examples,
Recurrence Equations.md,Linear Homogeneous Recurrence Relations,
Recurrence Equations.md,Distinct roots,
Recurrence Equations.md,Single Roots,
Real Time Operating Systems.md,Real Time Process,Notice that C is often impossible to determine except for small specific applications.
Real Time Operating Systems.md,Recurrent RTOS process,"Usually executes some function or goes through a set of steps in a regular manner repeated over time. e.g. Collect data from sensors, execute control laws, send actuator commands"
Real Time Operating Systems.md,Periodic RTOS Process,
Real Time Operating Systems.md,Sporadic RTOS Process,
Real Time Operating Systems.md,Real Time Process Scheduling,[Classical scheduling algorithms](Notes/Process%20scheduling.md) will fail to schedule RTOS processes as they do not take into account the deadlines of the processes.
Real Time Operating Systems.md,Fixed priority scheduling,"Priorities are fixed across instances of recurrent processes. Easy to implement with low time complexity. If all types of processes are known and small, a [hash map](Notes/Hash%20Tables.md) is able to do this in constant time."
Real Time Operating Systems.md,Rate Monotonic Scheduler,"Since the scheduler is based on periodic time, it still does not prioritise based on deadlines. P3 misses the deadline if it is <20,2,14>: less time to complete the process in first appearance."
Real Time Operating Systems.md,Deadline Monotonic Scheduler,A natural solution to RM scheduler:
Real Time Operating Systems.md,Dynamic Priority Scheduling,
Real Time Operating Systems.md,Earliest Deadline First Scheduler,Priority is based on the instance level rather than the process level.
Real Time Operating Systems.md,Comparisons,
Real Time Operating Systems.md,Practice Problems,"a. False. Real time does not mean fast or very short time, but rather to guarantee response times within a pre-defined deadline in the worst caseb. True.c. False. It is the host OS that does this. The guest OS interacts with the hypervisor and provides services to the applications running on it.d. False. Hypervisor manages interactions between H/W and guest OSe. True.Yes:Yes?"
Rabin-Karp Algorithm.md,General Idea,"1. Convert the pattern (length m) to a number p2. Convert the first m-characters (the first text window) to a number t with a hash function3. If p and t are equal, there is possibility of pattern: verify against the actual pattern4. Rolling hash function: shift the text window one character to the right and convert the new string5. Repeat until pattern is found or exit"
Rabin-Karp Algorithm.md,Rolling hash function,The new hash value can be calculated in $\theta(m)$ time.
Rabin-Karp Algorithm.md,Improving the hash function,
Rabin-Karp Algorithm.md,Pseudocode,
Rabin-Karp Algorithm.md,Complexity,
Rabin-Karp Algorithm.md,Examples,Rabin-Karp can support wildcard matches by simply considering the position of the wildcard to be a value of 0:
Quick Sort.md,General Idea,"1. Select a pivot element2. Partition the input into 2 halves (1 that is lower than the pivot, 1 that is higher than the pivot)- At each partition we end up with an array that is sorted relative to the pivot (pivot is in the final position)3. Recursively do the same for each half until we reach subarrays of 1 element."
Quick Sort.md,Pseudocode,
Quick Sort.md,Partition Method,"The middle element is usually chosen as the pivot:1. Swap mid with low, placing the pivot at the start of the array-2. If an element is smaller than the pivot: swap the position of a big element $(++lastsmall)$ with this smaller element $(i)$. __(This step results in an [](005%20Sorting%20Algorithms.md#^85ee66%7Cunstable) algorithm)__.3. If an element is larger or equal to the pivot: increment the index $(i)$4. Once last index is reached: Swap the pivot position $low$ with $lastsmall$, placing the pivot in the middle> [!NOTE] Key equality> Note that when a key $x$ is equal to the pivot, we do not perform any swaps or increment the lastsmall counter; __we simply treat it as it was bigger.__>> This means that at the end of the partition, $x$ will be placed on the right of pivot. <u>Leaving the right subarray non-empty.</u>>> If we want the left subarray to be empty, the pivot must be $\le$ all keys.> If we want the right subarray to be empty, the pivot must be $>$ all keys"
Quick Sort.md,Complexity,This results in $O(nlogn)$ complexity
Quick Sort.md,Examples,Forming a worst case array:
Quick Sort.md,Overall Evaluation,
Query Processing.md,Example,"`Select B, D From R, S Where R.A = “c” AND S.E = 2 AND R.C=S.C`Some possible solutions:1. Cartesian product -> Select tuple -> Project2. Select tuple -> Natural join ->Project3. Index to select R tuples -> Use S.C index to select found R.C values -> Remove S.E != 2 -> Join matching"
Query Processing.md,Operator Analysis,"Notations:- R: a relation/table- T(R): number of tuples in R- B(R): number of blocks needed to hold tuples of R- V(R, a): number of distinct values of attribute a in RAssumptions for analysis:1. Main memory is organized as input buffers (for holding data for computations) and output buffers (for storing results) each being the size of 1 block. Let M represent the number of input buffers available.2. Relation R is clustered if its tuples are packed into as few blocks as possible. *If T(R) = 1,000 and each block can hold 1000 tuples, B(R) = 100*- We assume R is clustered. If not clustered, it may take T(R) disk I/Os rather than B(R) to read all the tuples3. Ignore the final I/O cost for writing result in output buffer back to disk. Irrelevant to our calculations."
Query Processing.md,Notable Algorithms:,- [One Pass Algorithms](Notes/One%20Pass%20Algorithms.md)- [Two Pass Algorithms](Notes/Two%20Pass%20Algorithms.md)- [Index Based Algorithms](Notes/Index%20Based%20Algorithms.md)
Query Processing.md,Comparisons,
Query Optimisation.md,Algebraic Laws for Improving Query Plans,
Query Optimisation.md,Laws Involving Join,
Query Optimisation.md,Converting Selection and Product to Joins,
Query Optimisation.md,Laws Involving Selection,If R is not a set (it is a bag which can contain duplicates) then the union operation will not eliminate duplicates correctly.
Query Optimisation.md,Pushing selection,
Query Optimisation.md,Projection Laws,"Pushing projection is less useful than pushing selection. This is because projection keeps the number of tuples the same and only reduces the length of the tuples.> [! Basic idea:]> can introduce a projection anywhere in an expression tree, as long as  it eliminates only attributes that are neither used by an operator above  nor are in the result of the entire expression."
Query Optimisation.md,Logical Query Plan,Use of operator trees to represent the execution plan.
Query Optimisation.md,Optimising the Operator Tree,We can use the algebraic laws to devise a more optimal logical query plan.
Query Optimisation.md,Physical Query Plan,A physical query plan are the actions for which to execute the logical query plan.It includes:- Order and grouping of operations- [Algorithms for each operator](Notes/Query%20Processing.md)- [Argument passing from operator to operator: pipelining vs materialisation](Notes/Query%20Execution.md)We need a way to make choices for each of these components.
Query Optimisation.md,Cost estimation,"The cost of a plan is the sum of the cost of each operator in the tree. However, to know the cost of an operator often requires the input sizes to be known. This is often not available for intermediary operators which are executed after other operators such as `SELECT` are done.Assumptions:1. Uniform distribution of values in domain2. Independent distribution of values in different columns3. Independence of predicates for select and join"
Query Optimisation.md,Estimating Selection,"3 is used as an intuition for how a range operation would usually return less than half of all the tuples.For negation, we can estimate all tuples in the relation to satisfy the condition. Alternatively, notice that the number of distinct tuples that satisfy the relation is equal to $V(R,A)-1$Notice that the equation relates to converting an OR relation into an equivalent AND notation. $A|B=!(!A \cap !B)$"
Query Optimisation.md,Estimating Joins,"Assumptions:- Containment of value: satisfied when Y is a key in S and the corresponding foreign key in R. Approximately true due to probability since if Y appears in S, it is likely to appear in R as well since S is large.- Preservation: violated when there are tuples in R which join with no tuples in S."
Query Optimisation.md,Using Statistics,The assumption of uniform distribution is not accurate since real data is not uniformly distributed.We can maintain a histogram for each relation to help us improve the estimation:Statistics from each bucket can be used to determine the number of tuples in a range:Example:Example for join:Sampling can be used to increase performance:
Query Optimisation.md,Join Order Selection,"Many  [](Notes/Two%20Pass%20Algorithms.md#Sort-Merge%20Join%7Cjoin%20algorithms) are asymmetric, the role played by the two argument relations are different and the cost depends on which relation plays which role.- Build relation: the relation stored in main memory- Probe relation: relation which is read a block at a time to match the tuples in the build relation*Assumptions: the left argument is the build relation and the right argument is the probe relation*As the number of relations involved in the query increases, the number of possible join orders increases rapidly. We can use *join trees* to represent each of these possibilities:"
Query Optimisation.md,Left-deep tree,"It has a few advantages that make it a good default option:- The number of possible left-deep trees with a given number of leaves is  large, but not nearly as large as the number of all trees. Thus, searches  for query plans can be used for larger queries if we limit the search to  left-deep trees.- Left-deep trees for joins interact well with common join algorithms —  nested-loop joins and one-pass joins in particular.- Intermediate results are not written to temporary files.Example from the above image:*Left-deep tree*:1. Right children are the probe relations, we start building the join from the left leaf node R32. We keep R3 in main memory and perform R3 $\Join$ R1. This uses B(R) + B($R3\Join R1$) buffers3. No need to keep R3 in main memory anymore, can use the space to store the result from B($R3\Join R1\Join R5$).*Right-deep tree*:1. R3 is the build relation from the root, load R3 into memory2. Need $R3\Join R1\Join R5\Join R2\Join R4$ as the probe relation. To compute this, we need R1 to be loaded into main memory to compute $R1\Join R5\Join R2\Join R4$ as the probe relation.3. So on and so forth...The right-deep tree will require alot more space for the intermediate relations."
Query Optimisation.md,Join Algorithm Selection,
Query Optimisation.md,Heuristics,
Query Optimisation.md,Dynamic Programming,"Since we are concerned about minimising the cost of the query plan, we can utilise [DP](003%20Dynamic%20Programming.md) to remember the costs at each intermediate step for each enumeration. **The cost is the size of the intermediate relation.**Example:The cost for 2 relations is still 0, as no intermediate results are generated:There are $4\choose{3}$ ways to select 3 out of 4 of the relations. If we only consider left-deep trees, each option has $3\choose2$ permutations. We use the cost of the double relations to find the min cost of each permutation.Finally, when we consider all 4 relations, there are only 4 permutations if we only select left-deep trees. That is $4\choose2$$\times$ $2\choose1$. The first 4 rows represent these options:"
Query Optimisation.md,Practice Problems,"$$\begin{aligned}T(R\Join S)&=T(R)\times T(S)/max(V(R,a),V(S,a))\\T(R_1\Join R_2\Join R_3) &=T(T(R_1\Join R_2)\Join R_3)\\&=(1000\times1500/1100)\times750/100\\&=10227.27\end{aligned}$$$$\begin{align}\text{Size of Y}=20\times128&=2560B\\B(Y) &=5\\ \text{Size of X}&=60\times64=3840B\\B(X)&=7.5=8\\\text{Nested loop join}&=5+(5/5-1)\times8=15\\\text{Joined tuple size}&=128+64=192\\\text{T(T)}&=20\text{(because b is key in X,there can be at most 20 matching values)}\\\text{Write B(T) to disk: }&20/(512//192)=10\\\text{Read B(T) for selection}=&10\\\text{Block access for selection}&=0.5\times10=5\\\text{Total Disk Access}&=15+10+10+5=40\end{align}$$a.1. Select movie with year > 1990 and rating = 102. Join Movie and Studiob.$$\begin{align}P(Year>1990)=1/3,P(Rating=10)=1/10\\\text{Size after Select Movie}=\frac{1}{10\times3}\times 24000=800\\\text{Size after Join}=T(M)\times T(S)/max(V(M,name),V(S,name))\\=800\times1000/800=1000\end{align}$$i. $6000/20=300$ii. Tuples in $\sigma_{c>25}(S)=6000/3=2000$Tuples in $R\Join \sigma_{c>25}(S)=10000\times2000/200=100000$iii.Number of tuples of S which satisfy the condition: $\frac{2}{100}\times6000=120$These tuples will fit at best in $120/4=30$ blocksAt worst, each key will take up additional 1 tuple block: $30+1+1=32$Number of block access to find the 2 keys: $2\times3=6$Total I/O: $32+6=38$iv.1. Use $101-1=100$ buffers to repeatedly load blocks of R.2. Join every block of S with these 100 blocks of R3. Repeat until all blocks of R are loadedCost: $1000+1500\times(1000/100)=16000$Left deep plans:$(Emp \Join_{Dno}Dept)\Join_{Job}Job$$(Dept \Join_{Dno}Job)\Join_{Job}Job$$(Emp \Join_{Job}Job)\Join_{Dno}Dept$$(Job \Join_{Job}Emp)\Join_{Dno}Dept$i. The join order for a set of relations can be built with the sub problem of the set of relations -1. Can use DP to store the information of the minimal cost of each set of relations.ii.| Relation | {A,B}                     | {A,C} | {A,D} | {B,C}                   | {B,D}                   | {C,D}                   || -------- | ------------------------- | ----- | ----- | ----------------------- | ----------------------- | ----------------------- || Size     | $1500\times1000/50=30000$ | -     | -     | $1000\times2000/50=40000$ | $1000\times1000/50=20000$ | $2000\times1000/50=40000$ || Relation | {A,B,C}                        | {A,B,D}                      | {B,C,D}                      || -------- | ------------------------------ | ---------------------------- | ---------------------------- || Size     | $30000\times2000/50=1,200,000$ | $30000\times1000/50=600,000$ | $40000\times1000/50=800,000$ || Min Cost | (BC)A: 40000                   | (BD)A:20000                  | (BC)D                             || Relation | {A,B,C,D}                      || -------- | ------------------------------ || Size     | $1,200,000\times1000/50=24,000,000$ || Min Cost | (ABD)C: 600,000                   |Final order: $((B\Join D)\Join A)\Join C$i.Selectivity on condition a: $B(R)/V(R,a)=1000/20=50$Selectivity on condition b: $T(R)/V(R,b)=5000/1000=5$Selectivity on condition c = 3: $T(R)/V(R,c)=5000/5000=1$Best query plan: select on condition c=3 -> b=2 -> a = 1Expected disk i/o : 1ii.Selectivity on condition c < 3: $T(R)/3=5000/3=1666$Best query plan: select on condition b=2 -> a=1 -> c < 3Expected disk i/o : 5i.$$\begin{aligned}S\Join_{sid} R&=T(S)\times T(R)/max(V(S,sid),V(R,sid))\\&=1000\times10000/1000=10,000\\(S\Join_{sid}R)\Join_{bid}B&=10000\times100/100=10000\\\sigma_{size>5 \& \ day=''}&=10000/(3\times500)=6.66\end{aligned}$$ii.```mermaidgraph TBJ4((Join))-->J5((Join))J4-->B2((B))J5-->R2((R))J5-->S((S))J1((Join))-->J2((Join))J1-->B1((B))J2-->S1((S))J2-->R1((R))J6((Join))-->J7((Join))J6-->S2((S))J7-->R3((R))J7-->B3((B))J8((Join))-->J9((Join))J8-->S3((S))J9-->B4((B))J9-->R4((R))```iii.For {R,S}:Hash join: $3(B_R+B_S)=3(250+50)=900$Sort merge join: $3(B_R+B_S)=3(250+50)=900$Block based NL join:R outer loop = $B_R+B_S\times B_R/(M-1)=250+50\times250/(50-1)=506$S outer loop = $B_S+B_R\times B_S/(M-1)=50+250\times50/(50-1)=306$Blocked based NL join would work best$$\begin{aligned}IO(\sigma_{scity=Seattle})&=100\\B(\sigma_{scity=Seattle})&=100/20=5\\T(\sigma_{scity=Seattle})&=5\times20=100\\IO(\sigma_{srank<10})&=10\\B(\sigma_{srank<10})&=10/3=3.33\approx4\\T(\sigma_{srank<10})&=10/3=100/3\approx34\\&\text{Sort-merge join in-memory:}\\IO\Join_{sid=sid}&=0 \ \text{(inputs are in memory)}\\T(\Join_{sid=sid})&=100\times34/100=34\\&\text{We can ignore the cost of the index lookup}\\&\text{But we still need to access data pages for Major}\\&\text{We assume each input tuple needs 1 Major page}\\IO\Join_{id=id}&=34\\\text{Total Cost}&=100+10+34=144\end{aligned}$$i. $100/(3\times10)=3.33\approx4$ii.$P(b!=25)=9/10$$P(d!=13)=49/50$$100\times(1-\frac{9}{10}\times\frac{49}{50})=11.8\approx12$iii.$100\times500\times(\frac{1}{50}\times\frac{1}{100})=10$i.$(10\times30)+(40\times100)+(100\times200)/30=810$ii.$300\times600\times\frac{1}{30}=6000$$$\begin{align}&T(Student)=10,000\\&IO(Student)=1000\\&T(Checkout)=300,000\\&IO(Checkout)=15,000\\&T(\text{Nested Loop})=10,000\times300,000/10,000=300,000\\&IO(\text{Nested Loop})=1000+1000\times15000=15,001,000\\&T(Book)=50,000\\&IO(Book)=5000\\&T(\text{Tupled Based})=50000\times300,000/50,000=300,000\\&IO(\text{Tupled based})=0 \text{ (inputs are in memory)}\\&T(\sigma)=\frac{300,000}{500}\times\frac{20-12-1}{24-7}\approx234\end{align}$$```mermaidgraph TB;J1-->J2((Join S.b=T.b))J1((Join R.b = S.b))-->RJ2-->J3((Join T.b = U.b))J2-->SJ3-->TJ3-->U```"
Query Execution.md,Expression Evaluation,Represent queries in the form of an expression tree.
Query Execution.md,Processing Models,The processing model defines the organisation and execution of the operators.
Query Execution.md,Iterator Model - Top Down Approach,"> [!Pipelining:]> Each query operator makes its output tuples available to the next operator as soon as they are produced, rather than waiting until it has ﬁnished producing all of its output tuples.As a result, several operations share main memory at any time and each tuple is processed 1 at a time."
Query Execution.md,Materialisation Model - Bottom Up Approach,Each operator processes its input all at once and then emits the output all at once.
Query Execution.md,Vectorisation Model - Hybrid,"Each operator implements a next function similar to the iterator model, but each operator emits a batch of tuples rather than just 1 by 1."
Query Execution.md,Data Access Methods,How can the database management system access the data stored in tables?
Query Execution.md,Sequential Scan,"Simply iterate through each page in the table. However, not all blocks need to be accessed."
Query Execution.md,Optimisation strategies:,
Query Execution.md,Zone Maps,
Query Execution.md,Heap Clustering,A heap is a table that is stored without any underlying order in the pages
Query Execution.md,Bitmap Heap Scan,Use the bitmap created from Bitmap Index Scan to scan through the heap and find the corresponding data.```sql+---------------------------------------------+|___________X_______X______________X__________|+---------------------------------------------+seek------->^seek-->^seek--------->^|       |              |------------------------only these pages read```
Query Execution.md,Index Scan,Pick an index to find the tuples needed.Dependent on:-  What attributes the index contains-  What attributes the query references-  The attribute's value domains-  Predicate composition-  Whether the index has unique or non-unique keys
Query Execution.md,Multi Index,
Query Execution.md,Bitmap Index Scan,"1. Create a [bitmap](Notes/Bitmap.md) while doing an index scan2. When index key matches the search condition, the heap address pointed to by that index entry is looked up as an offset into the bitmap, and that bit is set to 1```Bitmap scan from customers_pkey:+---------------------------------------------+|100000000001000000010000000000000111100000000| bitmap 1+---------------------------------------------+One bit per heap page, in the same order as the heapBits 1 when condition matches, 0 if not```"
Query Execution.md,Practice Problems,"Pipelining refers to how tuples are passed from one tuple to another.An operator is blocking if it requires all of its children to emit all of their tuples (consumes all of its input tuples before producing any output tuples). E.g. a sort operation needs all of its child tuples to begin execution, and the entire execution must be complete before any tuples can be emitted to the next operator. The result of a sort is only known at the end.a.- Index scan: pick an index to find tuples needed- Bitmap heap scan: sort data in a heap using a clustering index order, select tuples using this index- Bitmap index scan: when query is to find tuple according to multiple attributes, use 1 index to find a set of tuples and another index to find another set. Take the intersectionb.- Heap: a heap is a set of unordered data pages- Clustered index: data is packed in as few blocks as possible according to sorted order of this index key- Heap clustering: sort a heap using a clustered index"
Query Compiler.md,1. [Query Parsing and Preprocessing](Query%20Parsing%20and%20Preprocessing),
Query Compiler.md,2. [Query Optimisation](Notes/Query%20Optimisation.md),
Pseudo-Polynomial Time Complexity.md,References,https://stackoverflow.com/a/19647659/12523715
Propositional Logic.md,Operator Precedence,
Processes.md,Process in memory,"- Sizes of text and data sections are fixed- Stack can grow and shrink dynamically: each time a function is called, an activation record is pushed to the stack and popped when the function returns- Heap can grow and shrink dynamically: changes as memory is dynamically allocated in runtimeInformation related to each process is then stored in a Process Control Block (PCB)"
Processes.md,Process states,"1. New: The process is being created2. Running: Instructions are being executed3. Waiting: The process is waiting for I/O or event4. Ready: The process is ready to run, but is waiting to be assigned to the CPU5. Terminated: The process has completed> [!Notes]> - Timer interrupt is used in multiprogramming systems to switch between ready processes> - Running -> waiting is due to some interrupt source from the currently running process> - Process always goes through the ready state before running (no direct from waiting to running)"
Processes.md,[Process scheduling](Notes/Process%20scheduling.md),
Processes.md,Process Operations,
Processes.md,Creation,This creation process can have 2 types:- Parent and child execute concurrently- Parent waits for all children to terminate before continuing execution. This is done through system calls `wait()` aka `join()`
Processes.md,Termination,1. Exit: Process asks the OS to delete it2. Abort: Parent terminates children processes
Processes.md,Inter-Process Communication (IPC),
Processes.md,Shared memory,Can be faster than message passing as less system calls required. Only ones are to establish the shared memory regions after which all access is treated as routine memory access.
Processes.md,Message passing,No use of shared variables.Works through 2 system calls:1. `send(message)2. `receive(message)Direct: processes must name each other explicitlyIndirect: messages are sent and received through a mailbox or port
Processes.md,Practice Problems,"a. False. That is the ready state. The waiting state is for processes waiting for some I/O operation or eventb. Truec. False. It is used by the parent to wait for children to finish.d. False. To support message passing, extra kernel memory needs to be allocated for process mailboxWhat are two main differences between the data and stack regions of a process memory?1. Data region is used to store global variables while stack region is used to store the currently executing local functions and parameters.2. Data region is fixed, while the stack can grow and shrink as the program executes.P0: Ready -> Running -> Waiting -> ReadyP1: Running -> Ready -> Running[Context Switch](Notes/Context%20Switch.md)A: Save state of P1 into PCB1B: Load state of P0 from PCB0C: Save state of P0 into PCB0D: Reload state of P1 from PCB1"
Process scheduling.md,Types,All processes are stored in queue structuresJob queue: set of all processes with the same state in the system- ready queue: processes in the ready state- device queue: processes waiting for specific I/O deviceA scheduler will be in charge of handling these queues
Process scheduling.md,Objectives,__System__:1. Maximize CPU utilization- Increase the % of time which CPU cores are busy executing processes- $\frac{\text{Total execution time}}{\text{Total time}}$2. Maximize throughput- Number of processes that complete their execution per unit time (number of exit transitions)__Process__:1. Minimize turnaround time- Amount of time to finish executing a particular process (e.g. time between admitted and exit transitions)2. Minimize waiting time- Amount of time process is in the __ready__ state- __Turnaround time - CPU burst time3. Minimize response time- Time between admission and first response (assume to be start of execution)
Process scheduling.md,Uni-Core Algorithms,
Process scheduling.md,First Come First Serve (FCFS),Non pre-emptive type: processes have to voluntarily release CPU once allocated__Convoy effect:__ Short processes suffer increased waiting times due to earlier arrived long processes
Process scheduling.md,Shortest Job First (SJF),"How to handle the convoy effect from FCFS? __Prioritize processes based on CPU burst lengths____Non pre-emptive__: a process cannot be stopped. Preemption only after a process is completed__Pre-emptive__ (Shortest Remaining Time First): processes in the midst of execution can be rescheduledThis algorithm is optimal to achieve minimum average waiting time. _However, this algorithm is often not used in practice as it is difficult to know the burst length of a process._"
Process scheduling.md,Priority Based,CPU is allocated to the process with highest priority1. Priority based on arrival order (FCFS)2. Priority base on CPU burst length (SJF)Starvation problem: lower priority processes may never execute. Need to use aging to slowly increase priority of processes that have been in the pipeline longer.
Process scheduling.md,Round Robin,Use a fixed time quantum (q) for scheduling. A process is allocated CPU for q time units and after that it is preempted and inserted at the end of the ready queue.Large q: degenerates to FCFSSmall q: many context switches leading to greater overhead_This is the algorithm that is used most commonly in practice_
Process scheduling.md,Multi-Queue,
Process scheduling.md,Multi-Core Algorithms,
Process scheduling.md,Partitioned Scheduling,"Each process are partitioned at process creation time among CPU coresEach process is mapped to one core__Asymmetric scheduling__: each CPU can have a separate scheduling strategy/algorithmHow to map cores to processes?- Burst lengths are not easy to know- For a CPU capacity, we need to maximize a certain property: similar to [Knapsack Problem](Notes/Knapsack%20Problem.md)- Thus, partitioned scheduling suffers from unbalanced loading of cores"
Process scheduling.md,Global Scheduling,Maintain 1 or more ready queues for the entire system without mapping any queue to any CPU core__Symmetric scheduling__: one scheduling strategy/algorithm across all cores
Process scheduling.md,Practice Problems,"Under Round-Robin scheduling, if quantum size is q, average CPU burst length is B, average number of CPU bursts per process is N, and average number of processes in the ready queue is R, then the average response time for a process is?$$\frac{0+q+2q+3q+...+(R-1)q}{R} = \frac{\frac{R}{2}(R-1)q}{R}=(R-1)q$$a. False. If the CPU cannot be removed from the process, it is non-preemptiveb. False. Only need to run scheduler when a process exits or changes to waitingc. False. Response time is time to first start of execution. Turnaround time is time to finish the process. Waiting time is time in the ready state. Turnaround - waiting time is just the time in the waiting and running states combined.d. False. Migration overheads occur in global scheduling when a process partially executes on one core and then migrates to another. In partitioned scheduling processes don’t migrate between cores. However, partitioned scheduling has the problem of unbalanced loading of the cores depending on the process-core mapping.abc.d.Uni-core: RRDuo-core: RR, SRTFEfficiency is total process time over total process time + total overhead: $\frac{T}{T+kS}$where k is the total number of context switches*we should also include the 1st context switch needed to start process*a. If $Q->\infty$, there will be 0 context switches$Efficiency=\frac{T}{T+S} = 1$b. If Q >T, average process will run without context switching$Efficiency=\frac{T}{T+S}= 1$c. S < Q < T. Average process will switch T/Q times.$Efficiency=\frac{T}{S\times\lceil\frac{T}{Q}\rceil}$c. Q = S. Average process will switch T/S times.$Efficiency=\frac{T}{T+\frac{T}{S}S} = \frac{T}{2T}=0.5$d. Q -> 0, number of switches tend to infinity$Efficiency=0$"
Process Synchronization.md,Critical Section Problem,"One method to solve the race condition is to divide processes into critical sections which are segments that shared data is accessed. **One process must be writing.**Problem: design protocol to ensure that no 2 processes are executing their critical section at the same time.We need to satisfy 3 properties:1. **Mutual exclusion**: if process is executing in critical section, no other process can be executing in its critical section at the same time.*Why is mutual exclusion not enough?- It can be achieved naively by preventing any process from entering critical section2. **Progress**: if no process is executing in its critical section and another process needs to enter their critical section, selection of this process to enter cannot be postponed indefinitely3. **Bounded waiting**: if a process needs to enter their critical section, all other processes are allowed to enter their own critical section only a bounded number of times"
Process Synchronization.md,User-level Solutions,"Following examples show how it is possible for 2 processes. *For more processes, it becomes unfeasible*"
Process Synchronization.md,Turn variable,"Progress is violated:1. P1 finish critical section and pass the turn over to P02. P0 finish its critical section and pass the turn over to P13. P1 runs in a long remainder section and never passes turn back to P04. Context switch occurs, P0 needs to enter critical section but P1 is stuck running remainder for a long period"
Process Synchronization.md,Flag variable,
Process Synchronization.md,Perterson's Solution,"Mutual exclusion:- Pi enters its critical section only if either flag[j] == false or turn == i. Also note that, if both processes can be executing in their critical sections at the same time, then flag[0] == flag[1] == true. These two observations imply that P0 and P1 could not have successfully executed their while statements at about the same time, since the value of turn can be either 0 or 1 but cannot be both. Hence, one of the processes—say, Pj —must have successfully executed the while statement, whereas Pi had to execute at least one additional statement (“turn == j”). However, at that time, flag[j] == true and turn == j, and this condition will persist as long as Pj is in its critical section; as a result, mutual exclusion is preserved.Progress:- Pi can be prevented from entering the critical section only if it is stuck in the while loop with the condition flag[j] == true and turn == j- If Pj is not ready to enter the critical section, then flag[j] == false, and Pi can enter its critical section. If Pj has set flag[j] to true and is also executing in its while statement, then either turn == i or turn == j. If turn == i, then Pi will enter the critical section. If turn == j, then Pj will enter the critical section.Bounded Waiting:- When $P_i$ exits its critical section, flag[j] == false and $P_i$ is allowed to enter its critical section. Assume $P_j$ resets flag[j] == true, it will subsequently set turn == i. Since $P_i$ cannot change the turn value while in the loop, it is allowed to enter the critical section after at most 1 entry by $P_j$"
Process Synchronization.md,Hardware Solution,
Process Synchronization.md,Synchronization Hardware,"Race condition is a result of context switches. We can prevent that in hardware to have atomic instructions.*Difficult to control the disabling of context switches in user level as there may be many critical regions and regions execute for unknown amounts of time*TestAndSet is now an assembly instruction which can be used to acquire a lock:- No context switches can occur while setting the lock value- This means that whoever runs this instruction first will run first, no other process will be able to enter critical region- If lock == true, someone is in the critical section: we are blocked- If lock == false, we set lock to true and move into the critical sectionHardware has no memory of process trying to access the lock. P0 able to indefinitely take the lock without giving P1 a chance."
Process Synchronization.md,Operating System Solution,
Process Synchronization.md,Mutex Locks,
Process Synchronization.md,Semaphore,- A binary sempahore behaves similar to mutex locks.- A counting semaphore is used to control access to a given resource consisting of a finite number of instances. **It is initalised to the number of resources available**.
Process Synchronization.md,Busy waiting *solution*,"Also known as a *spinlock*, where a thread trying to acquire a lock is caused to wait in a loop (""spin"") while repeatedly checking if the lock is available.Atomicity is not possible for this solution on a single-core. If a process P0 must loop to execute `Wait(S)`, no other processes can execute `Signal(S)` in order to allow P0 to continue. If we cannot context switch then there is no solution."
Process Synchronization.md,Blocking Solution,- Process is in the waiting state because the process cannot use the CPU (and following which enter its critical section) if another process is currently in its critical section- Process woken up is changed to ready state but the CPU may not switch from the currently running process to this newly ready process depending on schedulingAtomicity needed for these system calls:
Process Synchronization.md,Common Patterns,
Process Synchronization.md,Signalling,One thread sends a signal to another thread to indicate something has happened- Ensure a1 before b2- Ensure b1 before a2```aArrived = Semaphore(0)bArrived = Semaphore(0)```| Thread A            | Thread B || ------------------- | -------- || `aArrived.signal()` | `bArrived.signal()`          || `bArrived.wait()`   |`aArrived.wait()`          |
Process Synchronization.md,Classical Problems of Synchronization,
Process Synchronization.md,Bounded Buffer,"The order of access to the shared variables matter. Logically, each process should check if the resource is available (in the case of consumer) OR if there is currently no instances (for producers), before trying to access the buffer through the mutex lock.Producer:```godo{// wait until empty > 0 and then decrement 'empty'wait(empty);// acquire lockwait(mutex);/* perform the insert operation in a slot */// release locksignal(mutex);// increment 'full'signal(full);}while(TRUE)```Consumer:```godo{// wait until full > 0 and then decrement 'full'wait(full);// acquire the lockwait(mutex);/* perform the remove operation in a slot */// release the locksignal(mutex);// increment 'empty'signal(empty);}while(TRUE);```"
Process Synchronization.md,Dining Philosophers,"- If each process executes the first `wait(chopstick)` and context switches, every process only has 1 chopstick and is stuck in a deadlockSolutions:"
Process Synchronization.md,Readers-Writers,Writer:```gowait(wrt);//writesignal(wrt);```
Process Synchronization.md,Second Readers-Writers Problem,"It is possible that a reader *R1* might have the lock, a writer *W* be waiting for the lock, and then a reader *R2* requests access. It would be unfair for *R2* to jump in immediately, ahead of *W*; if that happened often enough, *W* would [starve](https://en.wikipedia.org/wiki/Resource_starvation ""Resource starvation""). Instead, *W* should start as soon as possible. This is the motivation for the **second readers–writers problem**, in which the constraint is added that *no writer, once added to the queue, shall be kept waiting longer than absolutely necessary*. This is also called **writers-preference**.```cint readcount, writecount;                   //(initial value = 0)semaphore rmutex, wmutex, readTry, resource; //(initial value = 1)//READERreader() {<ENTRY Section>readTry.P();                 //Indicate a reader is trying to enterrmutex.P();                  //lock entry section to avoid race condition with other readersreadcount++;                 //report yourself as a readerif (readcount == 1)          //checks if you are first readerresource.P();              //if you are first reader, lock  the resourcermutex.V();                  //release entry section for other readersreadTry.V();                 //indicate you are done trying to access the resource<CRITICAL Section>//reading is performed<EXIT Section>rmutex.P();                  //reserve exit section - avoids race condition with readersreadcount--;                 //indicate you're leavingif (readcount == 0)          //checks if you are last reader leavingresource.V();              //if last, you must release the locked resourcermutex.V();                  //release exit section for other readers}//WRITERwriter() {<ENTRY Section>wmutex.P();                  //reserve entry section for writers - avoids race conditionswritecount++;                //report yourself as a writer enteringif (writecount == 1)         //checks if you're first writerreadTry.P();               //if you're first, then you must lock the readers out. Prevent them from trying to enter CSwmutex.V();                  //release entry sectionresource.P();                //reserve the resource for yourself - prevents other writers from simultaneously editing the shared resource<CRITICAL Section>//writing is performedresource.V();                //release file<EXIT Section>wmutex.P();                  //reserve exit sectionwritecount--;                //indicate you're leavingif (writecount == 0)         //checks if you're the last writerreadTry.V();               //if you're last writer, you must unlock the readers. Allows them to try enter CS for readingwmutex.V();                  //release exit section}```"
Process Synchronization.md,Practice Problems,"a.1. Mutual exclusion2. Progression3. Bounded waitingb.- Mx not satisfied:- P1: turn = 0, while(flag and turn == 0) *flag is false*, critical section- context switch- P2: flag = true, while(turn == 1), critical section- Progress is not satisfied. P1 can only run if flag = false but flag is only set to false after the critical section of P1.a. False. If all instructions can complete in 1 cycle, there will not be instructions being executed halfway before context switch occurs. There are also other reasons for race condition, not only due to translation. If implementation using a temporary variable, a race condition can also occur.b. True. If no context switch can occur during critical section, only 1 process will be in its critical section at one time.c. False. Satisfies progress only means that 1 program will always be chosen to enter critical section. To satisfy bounded waiting, each program must have a chance to enter its critical section. A program where only 1 process always enter critical section while another is waiting indefinitely satisfies progress but not bounded waiting.hmm not too sure abt this questionidea:1. use a boolean lock value initialized to false as a shared variable, and a register boolean as a flag2. continuously try to swap `true` into the lock3. if register becomes false, we got the lock4. critical section5. set the lock to false```gowhile(1){register = truewhile (register) {swap(&lock, register)}  //entry sectioncritical section...lock = falseremainder section...}```a. -4b. -6. All `Wait(S)` runs before a single `Signal(S)`. Each process is added to blocked queue until OS chooses to execute a critical region.c. 2. There can be at most 2 processes holding S simultaneously as 2 processes are able to complete `wait(S)` (*not blocked*) before the block list starts to increase.To ensure there is no deadlock, there should not be any nesting i.e. two semaphores are not acquired together:```gowait(A)apple++signal(A)wait(O)if (at least 2 oranges in basket){oranges += 2} else {signal(O)wait(A)apple--signal(A)}```Identify critical section for each function and use TestAndSet to acquire lock into the section:```goWait(S){while (TestAndSet(&lock));S.value--if (S.value < 0) {S.L = append(S.L, process)*lock = falsesleep(process)} else {*lock = false}}Signal(S) {while (TestAndSet(&lock));S.value++if (S.value <= 0) {p = S.L[0]*lock = false//add to ready queue} else {*lock = false}}```"
Probabilistic Analysis and Randomised Algorithms.md,Exercises,
Probabilistic Analysis and Randomised Algorithms.md,5.1.2,"Describe an implementation of the procedure RANDOM(a, b) that only makes calls to RANDOM(0, 1). What is the expected running time of your procedure, as a function of a and b?```Random(a,b):left = aright = bwhile left < right:middle = (left+right)/2choice = Random(0,1)if choice == 0:right = middle - 1else:left = middle + 1return left```$O(log_2(b-a))$"
Probabilistic Analysis and Randomised Algorithms.md,5.2.1,"Probability of hiring 1 time occurs when the best candidate is presented first. This occurs with probability$\frac{1}{n}$. Probability of hiring n times occurs when the order is strictly increasing. This is one order out of $n!$ orders, $\frac{1}{n!}$."
Probabilistic Analysis and Randomised Algorithms.md,5.2.2,You hire twice when you first hire is the candidate with rank i and all the candidates with rank k >i come after the candidate with rank n. There are $n - i$ better suited candidates and the probability of the best one coming first is $1/(n-i)$. Let $T_i$ be the indicator variable for the event that the ith candidate is hired twice. $$Pr(T)=\sum_{i=1}^{n-1}Pr(T_i)=\sum_{i=1}^{n-1}\frac{1}{n\times (n-i)}=\frac{1}{n}log(n-1)+O(1)$$
Probabilistic Analysis and Randomised Algorithms.md,5.2.5,"Let $X_i$ be the indicator random variable for the event which the ith customer gets back his own hat. $Pr(T_i)=1/n$ since the hats are given back in random order, each customer has an independent 1/n chance of getting back his own hat. $$E[T]=E{\sum_{i=1}^n T_i }=E(n\times1/n)=1$$"
Principle of Optimality.md,Application to Dynamic Programming,"> [!NOTE] Paraphrasing...> A problem is said to satisfy the principle of optimality if the subsolutions of an optimal solution of the problem are themselves optimal solutions for their subproblems.__The [Shortest Path Problem](Notes/Shortest%20Path%20Problem.md) satisfies the principle:__If $a,x1,x2,...,xn,b$ is a shortest path from node a to node b in a graph, then the portion of $xi \to xj$ on that path is a shortest path from $xi \to x$j. [](Notes/Shortest%20Path%20Problem.md#^c4528f%7CCan%20be%20proven%20by%20contradiction.)__The longest path problem does not satisfy:__Take for example the undirected graph G of nodes a, b, c, d, and e, and edges $(a,b) (b,c) (c,d) (d,e) and (e,a)$. That is, G is a ring. The longest (noncyclic) path from a to d to a,b,c,d. The sub-path from b to c on that path is simply the edge b,c. But that is not the longest path from b to c. Rather, $b,a,e,d,c$ is the longest path. Thus, the subpath on a longest path is not necessarily a longest path."
Prim's Algorithm.md,General Idea,It is a greedy algorithm to generate a [Minimum Spanning Tree](Notes/Minimum%20Spanning%20Tree.md).1. Select the minimum weight edge from tree vertex to fringe vertex2. Update the fringe vertex distances with the minimum weight edge and the parent node for this min weight edge3. Repeat until all vertices are in the tree
Prim's Algorithm.md,Pseudocode,
Prim's Algorithm.md,Complexity,
Prim's Algorithm.md,Proof,Combine with proof of [](Notes/Minimum%20Spanning%20Tree.md#A%20tree%20is%20a%20MST%20if%20and%20only%20if%20it%20has%20the%20MST%20property%7CTheorem%201)
Prim's Algorithm.md,Examples,
Pipelining.md,Datapath,"Pipelining makes use of extra registers between each pipeline in order to store the necessary data and control signals needed by the current instruction for the next stage. Without it, the next instruction will override the information."
Pipelining.md,Clock Period,Pipelining allows us to achieve smaller clock period (and hence higher frequencies) by reducing the time required for a single stage to complete.
Pipelining.md,Limitation,- Single cycle: $1.2+0.8+0.7+1.3=4ns$- Pipelined: $1.3+0.3=1.6ns$ (register delay needs to be taken into account)Clock period of a single stage = $4/1000 = 0.004ns$Maximum delay = $0.004+0.3=0.304ns$Speedup = $4/0.304=13$*A 1000x increase in pipeline registers only results in a 13x increase in speedup!*
Pipelining.md,Hazards,A hazard is a drop in efficiency in the pipeline due to stalling.We can measure the effect of stalls using steady state CPI$$\text{Steady State CPI} = (No.Instructions+No.Stalls)/No.Instructions$$
Pipelining.md,Data hazard,When either the source or destination register of an instruction is not available at the time expected in the pipeline.RAW dependencies are difficult to handle and results in stalling for a pipeline architecture.
Pipelining.md,Detect and Wait,Wait until the required value is available in the register file by stalling (hardware) or inserting NOPs (software)
Pipelining.md,Data Forwarding,"__Data forwarding via register__: We can write and read from register in the same clock cycle. This means that WRITE-BACK and DECODE stage can happen at the same time> [!note]> Forwarding via register is easy to implement. Even when we say there is no forwarding, forwarding via register is considered to still be in placeFor other stages, we can only forward from the _previous_ clock cycle:Notes- SUB needs X1 value latest during the execute stage- AND can obtain X1 latest through register forwarding"
Pipelining.md,Dynamic Scheduling,Out-of-order execution and completion:Reordering introduces the possibility of WAR and WAW hazards which were not possible in an in-order execution pipeline. These can be solved via __register renaming__:
Pipelining.md,Loop Unrolling,"Further optimizations can be made for looping code. Loop segments contain a high level of overhead *(lines that work on the loop variable and branch commands)*, which are not directly contributing to the work of the loop body.```assemblyfor (i=999; i>=0; i=i–1)x[i] = x[i] + s;""""""""""""""L.D F0,0(R1)DADDUI R1,R1,#-8 //overheadADD.D F4,F0,F2stall //overheadstall //overheadS.D F4,8(R1)BNE R1,R2,Loop //overhead```Combine unrolling with dynamic scheduling:Example of 4 factor unrolling:"
Pipelining.md,Control hazard,"Conditional and unconditional jumps, subroutine calls, and other program control instructions can stall a pipeline because of a delay in the availability of an instruction.A naive (conservative) way would be to stall the pipeline whenever we encounter a branch instruction. Depending on hardware, this results in number of stalls (_branch penalty_) based on which stage of the pipeline the branch address is determined.Worst case: One clock cycle stall or flush of one instruction after each branch.$\text{Pipeline Stall Cycles per Instruction due to Branches} = \text{Branch frequency} \times \text{Branch Penalty}$"
Pipelining.md,Static Branch Prediction,"Delayed Branching: schedule an independent instruction in the branch delay slot. If branch penalty is 1, we will have 1 branch delay slot."
Pipelining.md,Dynamic Prediction,Rely on some measure of past behaviour to predict the future
Pipelining.md,Structural hazard,When two instructions require the use of a given hardware resource at the same time that will lead to a stall in the pipeline (one instruction has to wait at least for a clock cycle).Consider we have only one memory. For a case when write stage access the memory for writing the result and the instruction fetch stage tries to fetch the instruction from the memory at the same time.
Pipelining.md,Practice Problems,"a. Pipelined minimum is the max of all the stages: 500psNon-pipelined min is the sum of all stages: 1650psb. LDUR: IF -> ID -> EX -> MEM -> WBNon-pipelined sum all stages: 1650psPipelined: Each stage must take 500ps leading to 2500psc. MEM stage. Clock period: 400psd.1. Used in LDUR and STUR 25%2. Used in ALU and LDUR 65%a. 2 stall cycles per hazard = 6 totalb.1. LDUR instruction requires X0 latest at the execute stage, where the ALU calculates the memory address value. X0 is known at the E stage of ADDI: 0 stalls required2. ADD instruction requires X2 latest at the E stage. X2 is known at the MEM stage of LDUR when it is loaded from data memory. Forward from M -> E: **1 stall3. STUR instruction requires X3 latest at the Mem stage where it is put into data memory. X3 is updated at the E stage of ADD: 0 stalls requiredc.No Forwarding CPI: $(6+6)/6 =2$Forwarding CPI: $(6+1)/6 =1.17$a.Always taken: 75%, 60%Always not taken: 25%, 40%b.1. 0%2. 20%c.1. Only 1 not taken states, resulting in predictor staying in the 11 and 10 states. 75%2. 40%. Cycle from 11->10->01a. How to do this one?Number of stalls per loop = 2 + 2 + 1 = 5Total useful instructions: $1+6\times x= 1+6x$Total instructions: $1+(6+5)\times x=1+11x$x = 5: $\frac{31}{56}=0.55$x = 100: $\frac{601}{1101}=b.$$\begin{align}&\text{Branch Penalty Unconditional}=1&\text{Branch Penalty Conditional}=2\\&\text{Stall cycles Unconditional}=0.01\times1=0.01\\&\text{Stall cycles Conditional}=0.15\times0.6\times2=0.18\\&CPI = 1+0.18+0.1=1.19\end{align}$$"
Network Address Translation.md,Solution,"Introduce NAT devices at the edge of the network, each of which would be responsible for maintaining a table mapping of local IP and port tuples to one or more globally unique (public) IP and port tuples (Figure 3-3)."
Network Address Translation.md,NAT Traversal,
Network Address Translation.md,Problems while using a NAT,"1. Client may not know the public IP address: if the client communicates its private IP address as part of its application data with a peer outside of its private network, then the connection will inevitably fail.2. NAT table may not have the mapping of a public IP of a packet"
Network Address Translation.md,Session Travel Utilities for NAT (STUN),Helps the application obtain the public IP and port tuple of the current connection:1. Discover the IP address and port tuple for the connection2. Establish the NAT routing entry for the host application3. Keepalive pings to keep NAT entries from timing out
Network Address Translation.md,Traversal Using Relays around NAT (TURN),"When STUN fails (blocked by firewall etc.), we can use a relay to transmit the data between peers."
Network Address Translation.md,Interactive Connectivity Establishment (ICE),Establishes a set of methods to find the most efficient tunnel between participants: direct connection where possible using STUN if needed and TURN if failure.
Page Replacement Policies.md,First In First Out,
Page Replacement Policies.md,[Least Recently Used Policy](Notes/Least%20Recently%20Used%20Policy.md),
Page Replacement Policies.md,[Clock (or Second Chance) Policy](Notes/Clock%20(or%20Second%20Chance)%20Policy.md),
P and NP Problems.md,Classification of Problems,
P and NP Problems.md,Decision vs Optimization,"Additionally:- What is the minimum number of colours needed to colour a given undirected graph G such that each node is assigned a different colour from all its neighbours?- Given an undirected graph G and a natural number k, can G be coloured with more than k colours such that each node is assigned a different colour from all its neighbours?"
P and NP Problems.md,P Problems,P is the set of all decision problems which can be **solved** in _polynomial time_ by a _deterministic Turing machine_.$P\in NP$ because if a problem is solvable in polynomial time then a solution is also verifiable in polynomial time by simply solving the problem.
P and NP Problems.md,NP Problems,"Non-deterministic polynomial time problems.The class of decision problems for which they are _solvable_ in __polynomial time by a  nondeterministic algorithm.__ Equivalently:- A guess of the solution is generated by a nondeterministic algorithm- It can be _verified_ in polynomial time by a deterministic algorithm- A solution by a deterministic algorithm in polynomial time has not yet been foundWhat is a nondeterministic algorithm:> [!NOTE] Why is the [Knapsack Problem](Notes/Knapsack%20Problem.md) in NP?> Verification:> There are $2^n$ subsets of n objects: to check all subsets we would need $O(2^n)$ time.> However, given a guess of a subset: to check this subset we would need $O(n)$ time.> Solution:> DP Solution is [Pseudo-Polynomial Time Complexity](Notes/Pseudo-Polynomial%20Time%20Complexity.md)"
P and NP Problems.md,NP Complete Problems,"- NP complete problems are equal to each other in difficulty- Hardest problems in NP: if a NP-complete problem D can be solved in a certain amount of time, e.g. $O(f(n))$, every NP problem can be solved in $O(f(n))$ time. Hence no NP problem is harder than an NP-complete problem D.- If a polynomial time solution can be found for an NP complete problem: P = NP"
P and NP Problems.md,NP Hard Problems,"""At least as hard as the hardest problems in NP"".__NP-Hard problems do not have to be in NP and do not have to be decidable.__A problem D is NP-Hard when for every problem L in NP, there is a polynomial time reduction from L to D. Equivalently:- There is a polynomial time reduction from an NP-Complete problem G to D. _Since all problems in NP can be reduced to G in polynomial time, G is also reducible to D in polynomial time_."
P and NP Problems.md,Example reductions,"_Show that the [Longest Path Problem](Notes/Longest%20Path%20Problem.md) is NP-Hard_.Reduction from the [Travelling Salesman Problem](Notes/Travelling%20Salesman%20Problem.md): For a weighted complete graph G with non negative weights, the path that passes through all the vertices once must also be one of the longest paths because the longest path must include all vertices. Forgo the shortest path optimization in TSP and we obtain LPP.Reduction from the [Hamiltonian Path Problem](Hamiltonian%20Path%20Problem): For an unweighted graph G, it has a Hamiltonian path if and only if its longest path has length n − 1, where n is the number of vertices in G. Because the Hamiltonian path problem is NP-complete, this reduction shows that the decision version of the longest path problem is also NP-complete. In this decision problem, the input is a graph G and a number k; the desired output is ""yes"" if G contains a path of k or more edges, and no otherwise."
P and NP Problems.md,Too hard...use greedy heuristics,- [](Notes/Knapsack%20Problem.md#Greedy%20Heuristics)- [](Notes/Travelling%20Salesman%20Problem.md#Greedy%20Heuristics)
One Pass Algorithms.md,Example using Select,- I/O Cost: B(R)- Space: M >= 1
One Pass Algorithms.md,Duplicate Elimination,- I/O Cost: B(R)- Space: $M-1 \ge B(distinct(R))$
One Pass Algorithms.md,Natural Join,"- I/O Cost: B(R) + B(S)- Space: $M-1 \ge min(B(R),B(S))$"
One Pass Algorithms.md,Nested Loop Join,"Can be considered ""one and a half pass algorithm"": One argument is read only once while another is read repeatedly"
One Pass Algorithms.md,Simple Nested Loop Join,Sacrifice some I/O time in order to save on memory:- I/O Cost: $B(R) + B(S)\times B(R)$- Space: $M \ge2$
One Pass Algorithms.md,Block based nested loop join,We can improve the I/O cost by utilising all the buffers available:- I/O Cost: $B(R) + B(S)\times (B(R)/(M-1))$- Space: $M \ge2$If M is large -> devolves into the one pass algorithmIf M is 2 -> devolves into the simples nested loop join algorithm
One Pass Algorithms.md,Practice Problems,"a. One pass algorithm means that at least one of the relations must be fully loaded into the input buffer. M must be at least 100 + 1 = 101I/O cost: 100 + 150 = 250b.S in the outer loop: same as in (a)R in the outer loop: $150+100\times150/100=300$c.When the smaller relation is put in the outer loop and fits within the available M-1 buffers, the block based algorithm becomes a one-pass algorithm and they share the cost.We can load 999 blocks of R and 1 block of S at a time.$Cost = (20000/999)\times5000+20000\approx120101$"
Observer Pattern.md,Problems we want to solve,1. Tight coupling due to a 1-many dependency2. We need a number of dependent objects to update automatically when one object changes state3. We need an object to notify a number of other objects
Observer Pattern.md,Push / Pull Mechanisms,**Pull- 2-way communication: Subject sends a notification and the observer calls back for details explicitly (can be used for selective notification based on interest)**Push- 1-way communication: Subject sends the detailed information whether the observer wants it or not (can be used for sending updates based on location etc.)
Observer Pattern.md,Example Class Diagrams,
Observer Pattern.md,Pros,1. Abstracts coupling between Subject and Observer2. Supports broadcast communication3. Enable reusability of subjects and observers
Observer Pattern.md,Cons,1. Slower performance2. Possible unnecessary complexity
Neural Networks.md,Perceptron Learning,"A method to find separating hyperplanes, for classification:<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/OFbnpY_k7js"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe>- Only limited to linear separable datasets- Guaranteed to converge- Initial weights do not affect convergenceA single neuron is limited, we need a multi-layer perceptron network for more than a linear separation:"
Neural Networks.md,Error Backpropagation,A way to iteratively update the weights based on the target output at the final node.1. Forward propagation: calculate output nodes2. Backward propagation: calculate the errors3. Update: update weights locally- Prone to converge to local minima- Bad weights can affect convergence
Network Layer.md,Forwarding Table,"In the case of a 32 bit IP address, a brute force implementation of the forwarding table would have 1 entry for each of the 4 billion possible addresses. To make things scale, forwarding tables use ranges instead and match based on the longest prefix for which interface to forward to:"
Network Layer.md,Internet Protocol (IP),"The [Internet Protocol](Notes/Internet%20Protocol.md) handles addressing conventions, datagram format and packet handling conventions."
Network Layer.md,Network Address Translation (NAT),[Network Address Translation](Notes/Network%20Address%20Translation.md) allows local home subnets to grow bigger without having to request for additional address blocks from the ISP.
Network Layer.md,Internet Control Message Protocol (ICMP),
Network Layer.md,Routing Algorithms,"The goal of the routing algorithm is to determine the least cost path to be populated in  the forwarding table.What is the ""least cost""?:- Number of hops- Bandwidth- Delay- Cost/Load"
Network Layer.md,Classifications,- Centralised: algorithm has complete information about connectivity and link costs (link-state algorithms)- Decentralised: no node has complete information about all network links. Each node exchanges information with its neighbours.- Static: routes change very slowly over time- Dynamic: routes change as the network traffic loads or topology change
Network Layer.md,Link-State Algorithm,"Each node is able to have the identical and complete view of the network by broadcasting link-state packets to all other nodes in the network. With this, [Dijkstra's Algorithm](Notes/Dijkstra's%20Algorithm.md) can be used to find the least cost paths."
Network Layer.md,Distance Vector Algorithm,"Let $d_x(y)$ be the cost of the least-cost path from node x to node y. Then the least costs are related by the Bellman-Ford equation, namely, $dx(y) = min_v \{c(x, v) + dv( y)\}$, which takes the minimum across all neighbours v of x, of the sum of x to v and v to y.[[Bellman-Ford Algorithm]]Each node x begins with $D_x(y)$, an estimate of the cost of the least-cost path from itself to node y, for all nodes, y, in N. Let Dx = [Dx(y): y in N] be node x’s distance vector, which is the vector of cost estimates from x to all other nodes, y, in N. Each node x maintains the following routing information:- For each neighbour v, the cost c(x,v) from x to directly attached neighbour, v- Node x’s distance vector, that is, Dx = [Dx(y): y in N], containing x’s estimate of its cost to all destinations, y, in N- The distance vectors of each of its neighbours, that is, Dv = [Dv(y): y in N] for each neighbour v of x"
Network Layer.md,Link cost change,"a. ""Good news travels fast"":- t0: y detect the cost change (4 to 1), updates its DV and sends it to x and z- t1: z receives the update and recompute its DV. $D_zx=2$- t2: y receives update from z, but computation results in the same distance vector. Stability is achieved in 2 iterations.b. ""Bad news travels slow"":- t0: y detects cost change (4 to 60) and updates its DV according to $D_y(x)=min\{c(y,x)+D_x(x), c(y,z) + D_z(x)\}=min\{60+1, 1+5\} = 6$This is wrong as $D_z(x)\ne5=50$- t1: z receives an update from y and computes its DV. $D_z(x)=min\{50+0, 1+6\}=7$- t2: y receives an update from z and recomputes its DV. $D_y(x)=min\{60+0, 1+7\}=8$This routing loop is because in order to get to x, y routes through z. But in order to get to x, z goes through y. This continues on and on for 44 iterations."
Network Layer.md,Poisoned reverse,"If z routes through y to get to destination x, then z will advertise to y that its distance to x is infinity, that is, z will advertise to y that Dz(x) = ∞ (even though z knows Dz(x) = 5 in truth). z will continue telling this little white lie to y as long as it routes to x via y. Since y believes that z has no path to x, y will never attempt to route to x via z, as long as z continues to route to x via y (and lies about doing so).*Loops involving 3 or more nodes will not be detected by this technique*"
Network Layer.md,Intra-AS: Open Shortest Path First (OPSF),
Network Layer.md,Autonomous Systems,"In above routing algorithms, the model of the network of routers was too simplistic- Scale: as number of routers become large, up to 600 million routers today, it would incur large amounts of overhead to store all destination routing tables. A DV algorithm iterated among them would surely never converge- Administrative autonomy: each network admin may want to control the routing in its own network, such as which routing algorithm to useThis can be solved by organizing routers into autonomous systems (ASs), with each AS consisting of a group of routers that are under the same administrative control.Each AS needs to be unique, and is identified by its AS number, which is assigned by ICANN regional registries"
Network Layer.md,Routing Information Protocol (RIP),RIP uses the distance vector algorithm with its metric being the number of hop counts.
Network Layer.md,OSPF,"OSPF uses IP directly, without UDP or TCP, meaning it has to implement functionality such as reliable message transfer.Each router1. Actively test the status of all neighbours/links2. Build a Link State Advertisement (LSA) from this information and propagate it to all other routers within an area.3. Using LSAs from all other routers, compute a shortest path delivery tree, typically using Dijkstra shortest path algorithm."
Network Layer.md,Inter-AS: Border Gateway Protocol (BGP),"To route a packet across multiple ASs, we need an inter-autonomous system routing protocol. Since an inter-AS routing protocol involves coordination among multiple ASs, communicating ASs must run the same inter-AS routing protocol. In the Internet, all ASs run the same inter-AS routing protocol, the BGP.Each router sends messages over these connections. For example, when a new subnet x appears in AS3, the gateway router in AS3 sends a message to AS2 ""AS3 x"". AS2 then uses iBGP to advertise the existence of x amongst internal AS2 routers before sending a message over eBGP ""AS2 AS3 x"" to AS1.- Uses TCP- Uses augmented distance-vector algorithm called path-vector."
Multi Key Index.md,Geographic Data,Build index on y and ax iteratively until each partition contains at most 2 records:
Multi Key Index.md,Grid Index,Issues:- Records may not be allocated into the cells evenly depending on the partitioned ranges: result in overflows or inefficient use of space
Multi Key Index.md,Partitioned Hash,Idea: combine the hash value of each key from [hash index](Notes/Hash%20Index.md) to form a single index.
Model-View-Controller Architecture.md,Design Problems,1. Tight coupling between UI and application logic2. [Observer Pattern](Notes/Observer%20Pattern.md): Need for UI to update when state changes3. [Strategy Pattern](Notes/Strategy%20Pattern.md): Need for UI to support different functionalities depending on the user inputView:- Manage how data is presented- Observes the Model and updates their graphical representationModel:- Provides the operations to register and unregister observers- Implements a notify method to call observers updateController:- Captures input and passes them to the view and model
Model-View-Controller Architecture.md,Pros,1. Support for simultaneous development2. Support for multiple views with just 1 Model3. High cohesion: grouping of related actions4. Low coupling
Model-View-Controller Architecture.md,Cons,1. Code navigability2. Maintaining multi-artefact consistency: decomposition of features results in scattering
Minimum Spanning Tree.md,Spanning Trees with the MST property have the same total weight,
Minimum Spanning Tree.md,A tree is a MST if and only if it has the MST property,Tree is an MST if it has the MST property:Tree with MST property is an MST:
Minimum Spanning Tree.md,"Uniqueness property: if a graph has unique edge weights, there can only be 1 MST","<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/Ftkv1Ijp5Jw?start=100"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen></iframe>"
Minimum Spanning Tree.md,Cut Property,"For any cut C of the graph, if the weight of an edge e in the cut-set of C is strictly smaller than the weights of all other edges of the cut-set of C, then this edge belongs to all MSTs of the graph.Proof: Assume that there is an MST T that does not contain e. Adding e to T will produce a cycle, that crosses the cut once at e and crosses back at another edge e' . Deleting e' we get a spanning tree $T∖{e'}∪{e}$ of strictly smaller weight than T. This contradicts the assumption that T was a MST.By a similar argument, if more than one edge is of minimum weight across a cut, then each such edge is contained in some minimum spanning tree.Misinterpretation of the cut property:Answer is No. For any node u in $M_2$  there can be some other path P that connects u to v in $M_2$ which passes through the cut multiple times (enters $M_1$). This path can have edge weights smaller than the path $P'$ that connects u to v through $M_2$"
Merge Sort.md,General Idea,"Divide and Conquer1. Divide the problem recursively into smaller (n/2) sub problems2. When only 1 element remains, this subproblem is considered trivially sorted3. Combine sorted subproblems together (backtracking) using a _merge function_."
Merge Sort.md,Merge function,**Case 3 shows how Mergesort achieves [](005%20Sorting%20Algorithms.md#^85ee66%20%7Cstability).** Equal elements are placed in the merged array in the order that they appear in:
Merge Sort.md,Pseudocode,
Merge Sort.md,Complexity,"Complexity at each recursive call is due to the application of the merge function> [!NOTE] Worst Case> - At least 1 element is moved to the new merged list after each comparison> - The final comparison will result in 2 elements moving to the merged list> - Hence, key comparisons needed to merge n elements is at most n-1> -> [!NOTE] Best Case> - When both subarrays are already sorted relative to each other, each comparison will move one element from the smaller array into the merged list until it is empty> 	- [1,2,...,n-1, n] or [n,n-1,...,2,1] increasing or decreasing ordered arrays> - The bigger array will be appended to the end without further comparisons> - If both arrays are equal size (n/2), there will be at best $\frac{n}{2}$ key comparisons> -> - $$T(n)=2T(\frac{n}{2})+\frac{n}{2} = \frac{n}{2}logn $$"
Merge Sort.md,Examples,
Merge Sort.md,Overall Evaluation,
Memory Organisation.md,Address Binding,"A program needs to be loaded into memory to run. The machine code that is generated needs to be mapped to memory addresses in the system.Possible binding stages:1. Compile time: memory location is known a priori, generating **absolute code** that must be recompiled if the starting location changes2. Load time: compiler generates **relocatable code** and the binding is performed by the loader.3. Execution time: binding is delayed until run time and the process can be moved during its execution from one memory segment to another. Uses a logical address that is relative to a starting 0 point."
Memory Organisation.md,Fragmentation,">[! ]>Internal Fragmentation: allocated memory may be larger than requested memory, this results in unusable memory within the partition>>External Fragmentation : unusable memory between partitions# Memory AllocationHow to assign memory to different processes?"
Memory Organisation.md,Contiguous allocation,Logical address space of process remains contiguous in physical memory.
Memory Organisation.md,Fixed partitioning,"Memory is partitioned into regions with fixed boundaries. OS decides which partition to assign a process to depending on the available partitions.*Internal fragmentation*: as each partition is fixed, a process assigned to a partition might not take up the entire space of the partition, resulting in wasted unusable memory internal to the partition."
Memory Organisation.md,Dynamic partitioning,"Do not partition the memory. Rather, the OS allocates the exact chunk of memory which a process requires, and keeps tracks of *holes* or available blocks of memory.*External fragmentation*: memory space between partitions (the holes) may be enough to satisfy a new request but is not contiguous and cannot be used. This can be solved by performing compaction, which shuffles memory contents to produce contiguous block of available memory."
Memory Organisation.md,Dynamic Allocation Policies,
Memory Organisation.md,Paging,^b8969eAllow process to be allocated physical memory whenever it is available.> [!Idea:]> 1. Divide the physical memory into fixed sized *frames*.> 2. Divide logical memory of the **process** into *pages* the same size as frames.> 3. Use a page table to map the logical memory to physical memory.
Memory Organisation.md,Fragmentation,- Eliminating external fragmentation as every available physical memory space can be utilised.- Internal fragmentation still possible as the last page may not use up the entire frame.
Memory Organisation.md,Address Translation,Split the logical address to map page to frame:Offset necessary to locate the byte-addressable piece of physical memory.
Memory Organisation.md,Hardware Support,The page table is stored in main memory with a page table base register (PTBR) pointing to it.
Memory Organisation.md,Translation Look-aside Buffers (TLB),"The page table is stored in memory and thus, to access a piece of physical memory, we require 1 memory access to the page table and 1 memory access to the actual memory. We can speed this up with a specialised cache for the page table."
Memory Organisation.md,Effective access time,With TLB:- 1 TLB access and 1 mem access on hit- 1 TLB access and 2 mem access on miss
Memory Organisation.md,Shared Pages,Reentrant or code which never changes during execution can be shared among processes by having processes used the same pages.
Memory Organisation.md,Multi-level Paging,A page table can be large. Not efficient to have to fetch the entire page table for every memory LOAD/STUR instruction.
Memory Organisation.md,Paging the page table,"- Final level represents the physical memory space, 12 bits is needed for byte-addressing the 4KB memory. This leaves 20 bits for indexing pages.- Second level represents the index to the first level page: There are $2^{20}$ pages. Each page in this level is 4 bytes so as to map to the address of a 4byte page address.- The root level represents the index to the 2nd level page: Since 1 block can store 4KB, to store information about the 4MB page table in 1 block will require $2^{20}\times4/2^{12}=2^{10}$ entriesHence, 10 bits to access 1 out of $2^{10}$ pages which itself contains 10 bits to access 1 out of $2^{10}$ pages. Total of $2^{20}$ pages."
Memory Organisation.md,Inverted Page Table,"> [! Idea:]> Rather than each process having its own page table, we only keep a single table with 1 entry for each physical frame.> 1. Each entry contains the `process-id` and `page-number`> 2. To find a matching entry for a logical address, we need to find the entry that has the equivalent proces_id and page_number pairWe gain in terms of memory, as we no longer have a page table size that is proportional to logical addressing space. We lose in terms of speed, as we need to search the page table rather than addressing it directly with a page index."
Memory Organisation.md,Non Contiguous Allocation (Segmentation),
Memory Organisation.md,Addressing,Similar logic as with a regular paging scheme:
Memory Organisation.md,Fragmentation,"- *External fragmentation*: as processes leave the system, occupied segments become holes in the memory"
Memory Organisation.md,Practice Problems,"c. First fit lower overhead. Only moved 1 block compared to 3 blocks in best fit.a. Fixed partitioningb. Process memory is based on absolute addresses and is unable to be relocated for memory compaction.a. 200ns to access the page table. Another 200ns to access the memory frame. Total time: 400nsb. $$\text{Total time}=0.75\times200+0.25\times400=250ns$$a.To address a byte in a 1-Kbyte page will require 10 bitsLogical address: 22 bit page index, 10 bit offset1 Gigabyte ($2^{30}$) physical memory will require, 30 bits to represent each byte addressb.$2^{22}$ pages.c.Number of entries = Number of physical frames: $2^{30}/2^{10}=2^{20}$a.There are 8 pages, 3 bits are required to determine the page index.Remaining 7 bits are used for the page offset, to address the individual byte in the page.1000011011 -> Page 100 Offset 0011011 -> Page = 4 -> Frame number 01001Physical address: $010010011011$b.There are 4 segments, 2 bits required to determine segment numberRemaining 8 bits used to address the physical memory unit1000011011 -> Segment 10 Offset 00011011 -> Segment = 2a.Number of pages = $2^{30}/1024=2^{20}$Size of table = $2^{20}\times4=2^{22}$b.$$\begin{align}&\text{Size of page table 1st level}=2^{22}\\&\text{Number of pages 2nd level}=2^{22}/2^{10}=2^{12}\\&\text{Size of 2nd level}=2^{12}\times4=2^{14}\\&\text{Number of pages 3rd level}=2^{14}/2^{10}=2^4\\&\text{Size of 3rd level}=2^4\times4=2^6<1024B\\&\text{Total no. of levels}=3\end{align}$$c.3rd level holds $2^4$ pages and requires 4 bits to indexEach page table can hold $2^{10}/2^2=2^8$ entries8 bits needed to index into the 2nd levelEach page table in the 2nd level holds $2^8$ entries8 bits needed to index into the final level"
Markov Decision Process.md,Components of MDP,
Markov Decision Process.md,Formulating an MDP,
Markov Decision Process.md,The Bellman Equation,"$$ V_{i+s}(s) =max_a(\sum_{s'} P(s'|s,a)(r(s,a,s')+\gamma V(s')) $$"
Markov Decision Process.md,Value iteration,"[Grid World Scenario](Notes/Grid%20World%20Scenario.md):| $V_i$ | (1,1) | (1,2)                                               | (1,3) | (2,1)                                                 |                                                                                 (2,2)                                                                                  | (2,3) || ----- | ----- | --------------------------------------------------- | ----- | ----------------------------------------------------- |:----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----:|| $V_0$ | 0     | 0                                                   | 0     | 0                                                     |                                                                                   0                                                                                    |   0   || $V_1$ | 0     | 0                                                   | 0| 0| $Up = 0.8\times0+0.1\times5+0.1\times0+0.9\times0$ $Down=0.8\times0+0.1\times5+0.1\times0+0.9\times0$ $Left=0.8*0+0.1*5+0.1*0=0.5$ $Right=0.8*5+0.1*0+0.1*0=4$ Max = 4 |   0   || $V_2$ | 0     | $Up=0.8\times(0+0.9*4)+$<br>$0.1\times0+0.1\times-5=2.38$ | 0     | $Right=0.8\times(0+0.9*4)+$<br>$0.1\times0+0.1\times0=2.88$ |$Right=0.8\times(5+0.9*0)+$<br>$0.1\times(0+0.9*4)=4.36$| 0      |"
Markov Decision Process.md,Obtain the optimal policy,"> [!NOTE]> Once we have V*,  we can use V* in the bellman equation for each State and Action to obtain the corresponding Q(S,A) value. $\pi*$ is the action which obtains max Q(S,A) i.e. V(S)."
Markov Decision Process.md,Policy iteration,"> [!NOTE] Steps> 1. Start with random policy and V(s)=0 for 1st iteration> 2. Policy evaluation (calculate V) until stable> 	1. For each state s calculate V(s) using the action in the policy _(this is different from calculating V(s) using max $Q(s,a)$)_> 3. Policy Improvement (calculate new policy)> 	1. For each state s calculate $Q(s,a)$ of all actions using the stabilized V(s) values.> 	2. Update the policy with the actions which maximize $Q(s,a)$ of each stateAsynchronous refers to in series: value calculated from previous steps (same iteration) are used in the next state calculations.Synchronous refers to in parallel: All V(s) values are calculated using the previous iteration V(s) estimates.Without the transition function or probabilities we need [Monte Carlo Policy](Notes/Monte%20Carlo%20Policy.md) reinforcement learning."
Making Change.md,Problem Formulation,"> [!NOTE] Note that> The minimum number of coins to form value x, is 1 + the minimum of the solutions to value $x-d_1, x-d_2....x-d_k$We can form the following recurrence equations:Base case (when n-d less than 0):$$change(n)=0, \forall i\in\{1,...,m\},n-d_i < 0$$Finding the minimum of subproblems:$$change(n)=\min_{i=1...m,d_i\le n}(1+change(n-d_i))$$"
Making Change.md,Strategy,
Making Change.md,Pseudocode,"```javaint coinchange (int n) {int change[n+1]; // initialize dp arrayfor (int j=0; j<=n; j++) {change[j] = MAXINT // MAXINT is the maximum integer valuefor(int i=0;i<m;i++){if(j-val[i]>=0) {change[j] = min(change[j],change[j-val[i]]+1)}}if(change[j]== MAXINT) {change[j]=0; // If no changes means base case 0}}return change[n]}```"
Longest Path Problem.md,References,- https://en.wikipedia.org/wiki/Longest_path_problem
Longest Increasing Subsequence.md,Example,
Longest Increasing Subsequence.md,Problem Formulation,Satisfaction of the Principle of Optimality:The solution to a subsequence ending at index j can be found using the solutions from subsequence ending at index 0 to j-1.Base Case (longest subsequence ending at index 0):$$LIS(0)=1$$Case 1: the character at index j is smaller than the character at index k (for some k)$$LIS(j)=1$$Case 2: the character at index j is bigger than the character at index k (for some k)$$LIS(j)=LIS(k)+1 $$Solution can be found by taking the maximum:$$LIS(j)=\max_{k=0\to j-1} \{LIS(K)+1\}$$
Longest Increasing Subsequence.md,Strategy,
Longest Increasing Subsequence.md,Pseudocode,
Longest Common Subsequence.md,Problem Formulation,"Idea: The longest common subsequence up to certain position $i$ in the first sequence and $j$ in another sequence, can be found using the longest common sub sequences of the sequences up till $i-1$ and $j-1$.The base case _when either subsequence is empty, there will not be any common characters_:$$LCS(i,j)=0 $$Common character found at (i,j):$$LCS(i,j)= LCS(i-1,j-1)+1 $$No common character at position (i,j):$$LCS(i,j)=max(LCS(i-1,j), LCS(i,j-1)) $$"
Longest Common Subsequence.md,Strategy,
Longest Common Subsequence.md,Pseudocode,
Longest Common Subsequence.md,Examples: Obtaining the subsequence characters,
Link Layer.md,Multiple Access Problem,How to coordinate the access of multiple sending and receiving nodes to a shared link? The link layer implements multiple access protocols.
Link Layer.md,Channel Partitioning Protocols,"Partition the broadcast channel's bandwidth among nodes sharing the channel.- **Time-division multiplexing** (TDM) splits up time into time frames with each frame split amongst the nodes. Each node is allowed to transmit during this time.- **Frequency-division multiplexing** (FDM): splits up bandwidth into available frequencies. Each node takes 1 frequency for transmissionProblem: what if only 1 node wants to transmit something? For a link bandwidth R, it is capped at R/N bps.- **Code division multiple access** (CDMA): assign each node a unique code used to encode the data. Each node can transmit simultaneously and the receiver can use the code to determine the sender."
Link Layer.md,Random Access Protocols,"Each node transmit at full rate, when a collision occurs, independently choose a random delay before retransmission. Probable for the node to *sneak* in its packet."
Link Layer.md,Carrier Sense Multiple Access Collision Detection (CSMA/CD),Rather than independently making a decision:- Carrier sensing (listen before speaking): node listens to the channel before transmitting and waits until it is quiet before transmission- Collision detection (stop talking if someone else begins talking): node listens to the channel while transmitting and stops if there is interference.
Link Layer.md,Taking turns,Each node take turns to transmit some data.- Polling: a master node polls each node in a round robin fashion to tell them when and how much they can transmit.- Token passing: each node passes a token to another in a fixed order.
Link Layer.md,Link layer addressing,"The link layer requires its own addresses (MAC address) in order to forward datagrams to the correct network adapter. Each MAC address is unique no matter the location, unlike [IP addresses](Notes/Internet%20Protocol.md#Addressing) which change depending on the network the host is connected to.*Why link layer address + network layer address (IP)*?: the network is not meant for just IP, without MAC addresses, it cannot easily support other protocols."
Link Layer.md,Address Resolution Protocol (ARP),"When a host wants to send a datagram to another over IP, it must give the IP datagram and the destination MAC address. ARP is used to determine the MAC address given an IP address **on the same LAN or subnet**.What if there is no entry in the ARP table for a desired destination?1. Sender construct ARP packet and indicate to use the MAC broadcast address `FF-FF-FF-FF-FF-FF` before passing to the adapter (consequently, this also reaches any other connected switches, populating their address tables)2. Adapter encapsulates the packet into the link layer frame and transmits3. Each adapter on the subnet passes the frame to its own ARP module which checks if the destination IP is its own. If it is, sends the packet back with the desired mapping"
Link Layer.md,Sending across different subnets,"ARP can only be used to obtain MAC addresses of IP addresses on the same subnet. To send a packet outside:1. Sender construct the datagram with the IP address of the final destination but the destination MAC address of the router interface.2. Router deconstructs the datagram (because its own MAC address was found), and forwards the datagram to the MAC address of the final destination IP address (obtained with ARP) using its forwarding table."
Link Layer.md,Link Layer Protocols,
Link Layer.md,Ethernet,- Unreliable: ethernet drops frames which do not pass the CRC check without sending any acknowledgements. The layer above implements reliability while Ethernet transmits unawares of whether it is a retransmission or not.- Connectionless: no handshaking requiredAn ethernet frame:- Preamble: used to synchronise clock rates between nodes- Destination/Source address: a MAC address- Type: identifier for the type of network protocol it is meant for- CRC: used for detecting bit errors in the frame
Link Layer.md,Link Layer Switches,A switch receives incoming link-layer frames and forwards them onto outgoing links.
Link Layer.md,Forwarding and filtering,"Filtering determines whether a frame should be forwarded or droppedForwarding takes the frame's destination MAC address and forwards it to the interface. This is unlike a [router which forwards based on IP addresses](Notes/Building%20Blocks%20of%20the%20Internet.md#Forwarding%20Tables%20and%20Routing%20Protocols)Switch table:The switch table is built automatically without any intervention:1. Switch table is initially empty2. For each incoming frame received on an interface, store the MAC address of the frame's source address, interface which it arrived and the current time3. Delete address if no frame received with that address as its source after some period of timeElimination of collisions: a switch buffers frames and never transmits more than 1 frame on a segment at any 1 time. This means that no collision can occur and the [Multiple Access Problem](Notes/Link%20Layer.md#Multiple%20Access%20Problem) is solved."
Least Recently Used Policy.md,LRU-K,"Rather than just taking the most recent access time for consideration, having a history of timestamps allows us to calculate the interarrival between references.- Consider an item with the most recently accessed time. However, the access before that is longer ago, this could mean that the next access is more likely to not be this item compared to another item whose previous access was closer.- Item with longest interarrival should therefore be dropped.LRU-1 is just the normal LRU algorithm1. Keep track of the K previous access timestamps of the item.- This can be done with a HISTORY array2. When item on memory is accessed, we update the history array with the latest access time in a stack manner3. When item needs to be evicted, we will choose the one that has the oldest Kth timestamp"
Least Recently Used Policy.md,Implementation,
Least Recently Used Policy.md,Stack,Swap used pages to the top of stack. Push out pages at the bottom.
Least Recently Used Policy.md,LRU Approximation,Hardware support is necessary as exact algorithms are expensive and generally not available. It may be better to simply approximate LRU using an easier more supported implementation.
Least Recently Used Policy.md,[Clock (or Second Chance) Policy](Notes/Clock%20(or%20Second%20Chance)%20Policy.md),
Layered Architecture.md,Pros,"1. Separation of concern: This allows us to focus on a smaller scope of problems in each layer, as opposed to looking at the problem across different layers2. Isolation: Each layer is decoupled, thus modifications in one layer will not affect downstream layers3. Changeability: Can easily replace one whole layer with another, while interface is still maintained4. Scalability"
Layered Architecture.md,Cons,1. Performance overhead: more pronounced when more layers are added
Kruskal's Algorithm.md,General Idea,A greedy algorithm to generate a [Minimum Spanning Tree](Notes/Minimum%20Spanning%20Tree.md) using the [Union Find](Notes/Union%20Find.md) data structure.1. Sort edges in increasing order of weight (a priority queue using a [Heaps](Notes/Heaps.md))2. Add the edge to the minimum spanning tree unless it creates a cycle- Cycle check uses union find
Kruskal's Algorithm.md,Pseudocode,
Kruskal's Algorithm.md,Complexity,Time complexity is $O(ElogE)$ mainly due to the contribution of the [Heaps](Notes/Heaps.md) implementation of priority queue to obtain the least cost edge.
Kruskal's Algorithm.md,Examples,Manually:Update the id directly to the root (D) when connecting with other equivalence classes when using [](Notes/Union%20Find.md#Weighted%20Quick%20Union):
Knowledge Representation.md,Resolution Strategies,__Resolution by Refutation__
Knapsack Problem.md,Problem Formulation,"Let $P(C, j)$ be the max profit by selecting a subset of j objects with knapsack capacity C.Base case: Capacity or number of items is 0$$P(C,0)=P(0,j)=0 $$Otherwise, the solution to a knapsack of capacity C can be found using the solutions to the subproblems of capacity $C-w_i$ for items $i=1 \to n$. For each item, we can choose either include or not include it in the knapsack:$$P(C,j)=max(P(C,j-i), P(C-w_j,j-1)+v_j) $$"
Knapsack Problem.md,Strategy,Profit table:
Knapsack Problem.md,Pseudocode,
Knapsack Problem.md,Greedy Heuristics,Maximizing by the profit per weight:
Knapsack Problem.md,Exercises,|     | 0   | 1   | 2   | 3   | 4   || --- | --- | --- | --- | --- | --- || 0   | 0   | 0   | 0   | 0   | 0   || 1   | 0   | 0   | 0   | 0   | 0   || 2   | 0   | 0   | 0   | 0   | 0   || 3   | 0   | 0   | 0   | 0   | 50  || 4   | 0   | 0   | 40  | 40  | 50  || 5   | 0   | 10  | 40  | 40  | 50  || 6   | 0   | 10  | 40  | 40  | 50  || 7   | 0   | 10  | 40  | 40  | 90  || 8   | 0   | 10  | 40  | 40  | 90  || 9   | 0   | 10  | 50  | 50  | 90  || 10  | 0   | 10  | 50  | 70  | 90  |
IO Subsystem.md,I/O Hardware,The hardware communication between I/O devices and the CPU is done through the [signal chain subsystem](Notes/Signal%20Chain%20Subsystem.md).
IO Subsystem.md,Kernel I/O Subsystem,- Device drivers are the only aspects which interface directly with the hardware.- This layering system allows devices to be added and removed without having to change the kernel
IO Subsystem.md,I/O Scheduling,"The OS is responsible for using hardware efficiently. Schedule I/O requests by rearranging the order of services.- For disk drives, we need to minimise the seek time as sequential access is much faster. OS needs to perform [](Notes/Disk.md#Disk%20Scheduling%7Cdisk%20scheduling)."
IO Subsystem.md,Buffering,Store data in memory while transferring between devices to handle device speed mismatch and transfer size mismatch.
IO Subsystem.md,Caching,Cache copies of data to improve efficiency for files that are being written and reread rapidly.
IO Subsystem.md,Spooling,"Store the output for a device to be used when a separate device can serve it, e.g. for devices that can serve only 1 request a time such as printers."
IO Subsystem.md,Performance,I/O operations require a lot of overhead- CPU needs to execute device driver code- Context switch due to interrupts- Data copying between controllers and memory
IO Subsystem.md,Asynchronous I/O,> [!Non-blocking IO]> The process remains in the running state (not ready state!)
IO Subsystem.md,Double buffer,
IO Subsystem.md,Practice Problems,"a. False. Buffers are used to support different device transfer speeds. What is described is the role of cacheb. False. Non-blocking I/O call puts the process back in the ready or running statec. False.a. Double buffer and async i/o```cbuffer2 <- async read block;while (not end of file) {while iO;buffer1 <- buffer2;buffer2 <- async read block;process buffer1;}```b. Contiguous file allocation is best as the data being read is the entire file. If this entire file is stored contiguously, the time needed for the disk to access the data is minimised.a. Not necessarily, if requests are issued one at a time, the disk driver has no opportunity for SCAN optimisation (SCAN = FCFS). This can be solved by concurrently generating IO requests.b. Under light load, the overhead for scheduling might become greater than the average seek time. ~~Performance of a disk scheduling algorithm can be affected based on the file allocation system utilised. For example, a SCAN algorithm on a linked file allocation method would have poor performance. In linked file allocation, the data blocks accessed are located all over different sectors in the disk. This means that a complete file access operation might require the operation to wait for the disk arm to move from one end to the other.~~~~Seek order: 4,10,23,35,35,40,45,50,70,132$$\begin{align}&\text{Single cylinder seek time}=20.1ms\\&\text{Total seek time}=(4+6+13+12+5+5+5+20+52)\times20.1=2452.2ms\\&\text{Total rotational latency and transfer time}=(8+2)\times10=100ms\\&\text{Average time}=2452.2+100=2552.2ms\end{align}$$"
Interrupts.md,Interrupt Service Routine,
Interrupts.md,Interrupt Handling,1. [Context Switch](Notes/Context%20Switch.md)2. Determines the type of interrupt that has occurred using the segments of code3. Executes the appropriate ISR based on the interrupt vector tableThe ISR is a very short routine so as to not suspend the main program for too long. This usually means no usage of loops.- Allows for efficient use of CPU as it does not need to monitor I/O device status- More hardware is required between I/O and processor to interface with each other
Internet Protocol.md,IPv4,
Internet Protocol.md,Datagram Fragmentation,"Not all link-layer protocols can carry network layer packets of the same size. For example, Ethernet frames can carry up to 1500 bytes while wide area links can carry no more than 576 bytes. This maximum amount of data is called **Maximum Transmission Unit** (MTU).When the router determines that the outgoing link has an MTU that is smaller than the length of the IP datagram:1. The payload is fragmented into 2 or more pieces. Each fragment shares the same identification number, but have different fragment offset bits2. Using this information, end systems can identify fragmented packets and piece them back together."
Internet Protocol.md,Addressing,"A host typically has 2 links (Ethernet and wireless 802.11). A router has multiple links. Each boundary between a host/router and the physical link is an *interface*, and each interface needs its own unique IP address."
Internet Protocol.md,Subnets,The isolated islands of network interfaces formed when disconnected from the router/host is a subnet. A topology with 6 subnets:- Example: the subnet address 223.1.2.0/24 indicates a 24 bit subnet mask which says that the leftmost 24 bits define the subnet address.> [!Note]> 1 IP address is reserved for the network and 1 is reserved for the broadcast address> A /24 netmask will only have $2^8-2=254$ available host addresses
Internet Protocol.md,Obtaining an IP address,An organisation can request for a block of IP addresses from an ISP. An ISP can split up its own allocated block in this way:The ISP itself obtains its set of IP addresses through a global authority called Internet Corporation for Assigned Names and Numbers (ICANN).
Internet Protocol.md,Dynamic Host Configuration Protocol (DHCP),"Once an organisation has obtained a block of addresses, it can assign individual addresses to the host an router interfaces. This can be done manually, but DHCP allows the host to obtain an IP address automatically from a DHCP server.1. Discover: client sends a discover message with UDP on port 67 to the broadcast address 255.255.255.255 since it does not know the IP address of the network.2. Offer: server broadcasts a message (on 255.255.255.255) containing transaction ID of the received message, proposed IP address and IP address lease time3. Request: client chooses from 1 or more offers and responds4. ACK: server responds to the request messageDHCP can also provide:- address of first-hop router for client- name and IP address of local DNS server- network mask (indicating network versus host portion of address)"
Internet Protocol.md,IPv6,"The IPv4 32 bit address space was beginning to be used up. In February 2011, IANA allocated out the last remaining pool of unassigned IPv4 addresses."
Internet Protocol.md,Datagram,"- Address: 128 bit address space- Flow label: identify datagrams of the same ""flow"" (maybe such as audio or video transmission)- Next header: identifies the protocol which the contents of the data will be delivered (to TCP or UDP)- Hop limit: decremented each time each router forwards the datagram (discarded at 0)Some fields from the IPv4 datagram were also removed:- Fragmentation: IPv6 does not allow for fragmentation and reassembly at intermediate routers. This can only be done at the source and destination. If the datagram is too large for the outgoing link, it is dropped and the sender is asked to resend using a smaller datagram size.- Checksum: removed to reduce processing time- Options: moved out of the headers portion"
Intelligent Agents.md,Characteristics of Environments,
Instructions.md,Register Type,All data values are located in registersAddressing Mode: __register addressing mode__Rm: First source registerRn: Second source registerRd: Destination registershamt: Shift amount for use in shift operations
Instructions.md,Datapath,
Instructions.md,Data transfer type,Addressing Mode: __Base/Displacement addressing__
Instructions.md,Datapath,"LDUR1. Rn contains the information about WHERE the data in memory is2. Offset Rn by address value  to get the memory address3. Store the data from this memory address into RtSTUR1. Rn register contains the information about WHERE to store the data2. Offset the information in Rn by the address value (22 + 64) = 90, to get the destination memory address3. Store the data inside Rt into this offset value> [!We can utilize a set of extra multiplexers to reuse components for both types]>"
Instructions.md,Immediate type,Addressing mode: Immediate addressing
Instructions.md,Datapath,
Instructions.md,Conditional Branch type,PC relative addressing mode
Instructions.md,Datapath,
Instructions.md,What's with the shift left by 2? question,"Each instruction word is 32 bits (4 bytes) long. If we want to move by 2 instructions, we need to move 8 bytes. Thus, left shift by 2 to multiple the address by 4 to navigate the correct number of bytes."
Instructions.md,Unconditional Branch type,Addressing mode: PC relative addressing
Instructions.md,Combine all types into a single datapath,
Instructions.md,R-Type,"Critical path:```mermaidgraph LR;A(Reg2Loc Mux) --> T(""2 x REG(read)"") --> C(ALUSrc Mux) --> ALU --> E(""Mem2Reg Mux"") --> F(""REG(write)"")```Notes:- Reg2Loc (0) used to select Rm as a source register- ALUSrc (0) to select register data rather than sign-extended address- Mem2Reg (0) to select data from ALU rather than memory"
Instructions.md,I-Type,"Critical path:```mermaidgraph LR;A(""REG(read)"") --> T(Zero Extend) --> C(ALUSrc Mux) --> ALU --> E(""Mem2Reg Mux"") --> F(""REG(write)"")```- Immediate address is zero extended and hence there is no delay here"
Instructions.md,Load,"Critical path:```mermaidgraph LR;A(""REG(read)"")  --> C(ALUSrc Mux) --> ALU --> T(D-MEM)--> E(""Mem2Reg Mux"") --> F(""REG(write)"")```Notes:- Reg2Loc not used as only Rn is used- ALUSrc (1) to select  sign-extended address- Mem2Reg (1) to select data from memory"
Instructions.md,Store,"```mermaidgraph LR;A(""REG(read)"")  --> C(ALUSrc Mux) --> ALU --> T(D-MEM)```- Reg2Loc used to select Rt as the read register 2. Rt data is passed into the D-MEM and not used by the ALU. Hence, the RegFile + Reg2Loc Mux delay is overshadowed by the ALU."
Instructions.md,Conditional Branch,"Critical path:```mermaidgraph LR;T(Reg2Loc Mux)--> A(""REG(read)"")--> C(ALUSrc Mux) --> ALU -->  E(Branch MUX) -->AND-->OR-->F(PCin/out)```Notes:- Reg2Loc (1) to read Rt- ALUSrc (0) to use data from register rather than address- Zero-flag in AND-gate together with Branch-flag to select address to add for branching rather than default +4 to load into PC"
Instructions.md,Unconditional Branch,Critical path:```mermaidgraph LR;T(Sign Extend)--> A(Shift left)--> C(ADD)-->E(Branch MUX)-->F(PCin/out)```Notes:- Additional OR-gate to always select the address for branching
Instructions.md,Practice Problems,"i. All instructionsii. All instructionsiii. All except unconditional branch instructionsiv. ALU instructions, Load/Store instructions and Conditional Branch. *Why unconditional branch don't need?*v. Load/Store instructionsPC++, PCin -> PCout and I-MEM is used for all datapathsPropagation delay is the time delay for the signal to reach its destination.Some signals are sent out in parallel (e.g. PC++) and the delay there is overshadowed by the overall delay by main logic.i.Reg2Loc Mux -> 2 x REG(read) -> ALUSrc Mux -> ALU -> Mem2Reg Mux -> REG(write)2 Reg read signals are done in parallel.$500+50+200+50+2000+50+200=3050ps$ii.REG(read) -> Zero Extend -> ALUSrc Mux -> ALU -> Mem2Reg Mux -> REG(write)The delay from ALUSrcMux is overshadowed by the REG(R)$500+200+2000+200+50=2950ps$iii.REG(read) -> ALUSrc Mux -> ALU -> D-MEM -> Mem2Reg Mux -> REG(write)ALUSrc MUX delay is overshadowed by the delay in REG(read)$500+200+2000+2000+50+200=4950$iv.STUR is LDUR but without the Mem2Reg Mux and REG write$4950-200-50=4750ps$v.Reg2Loc Mux -> REG(read) -> ALUSrc Mux -> ALU -> BranchMUX -> PCin/out$500+50+200+50+2000+50+100=2950$vi.Sign extend -> Shift -> Add -> BranchMUX -> PCin/out$500+25+0+1500+50+100=2175$i.Minimum clock period must allow types of instructions to complete without that clock period.Hence the minimum clock period is the time needed to complete the longest instruction: 4950psii.Minimum clock period of a specific cycle must allow the longest stage to complete. Hence, the longest stage is EX or MA which has 2000ps."
Instruction Set Architecture.md,RISC vs CISC,__RISC__: Reduced Instruction Set Architecture__CISC__: Complex Instruction Set Architecture
Instruction Set Architecture.md,ARM ISA,Advanced RISC Machine (ARM)
Instruction Set Architecture.md,Register specification,
Instruction Set Architecture.md,Register File,A register file is a set of registers that can be read and written by supplying a register number.This is done using [](Notes/Combinational%20Circuits.md#Multiplexer%7Cmultiplexers) to choose source registers and using a [](Notes/Combinational%20Circuits.md#Decoder%7Cdecoder) to select a destination register.
Instruction Set Architecture.md,Memory organization,
Instruction Set Architecture.md,Instruction memory,
Instruction Set Architecture.md,Data memory,
Instruction Set Architecture.md,Instructions Format,Based on the system we can design a set of computer [Instructions](Notes/Instructions.md).
Instruction Set Architecture.md,Addressing Modes,
Instruction Set Architecture.md,Practice Problems,"```ADDI X2, X3, #101 ;save loop termination indexloop:LDUR X4, [X11, #0] ;x4 = b[i]ADDI X11, X11, #8 ;x11 = x11 + 8ADD X4, X4, X1 ;x4 = b[i] + cSTUR X4, X10, #8 ;a[i] = x4ADDI X10, X10, #8 ;a[i] = a[i+1]SUBI X2, X2, 1 ;x2 = x2-1CBNZ X2, loopexit END```ii.1 is run once: 12 -> 8 is run 101 times = 1 + 707 = 708iii.Line 1 and 4 are memory references, each done 101 times: 202 references."
Instruction Level Parallelism.md,Approaches,Hardware approach: Superscalar processorSoftware approach: compiler based using a Very Long Instruction Word (VLIW) processor
Instruction Level Parallelism.md,Superscalar Processing,Way-2 can be assigned load and store instructions while Way-1 will handle other instructions:We can introduce previous techniques to reduce data hazard and improve overall CPI:
Instruction Level Parallelism.md,Very Long Instruction Word,
Instruction Level Parallelism.md,Practice Problems,a.P1 in order:| Time | Lane 1 | Lane 2 || ---- | ------ | ------ || 1    | A      |        || 2    | B      |        || 3    | C      | D      || 4    | E      | F      || 5    | G      |        || 6    | H       |        |P2 out of order:| Time | Lane 1 | Lane 2 || ---- | ------ | ------ || 1    | A      | D      || 2    | B      | E      || 3    | C      | F      || 4    | G      |        || 5    | H      |        |b. $Speedup = 6/5=1.2$
Insertion Sort.md,General Idea,"Incremental approach1. Iterate through the array and maintain a sorted array _in-place_, at the start of the array.2. For every element, loop through the sorted array backwards. Compare and swap the elements until it is in the correct spot.3. Repeat until the entire array has been iterated through."
Insertion Sort.md,Pseudocode,_Equal elements are not swapped in position_. This means that Insertion Sort is [](005%20Sorting%20Algorithms.md#^85ee66%20%7Cstable).
Insertion Sort.md,Complexity,"> [!NOTE] Best Case> - Occurs when the array is already sorted> 	- 1,2,3,4,5> - 1 Key comparison is still required at each iteration> - Total of $n-1$ comparisonsObserve that the time complexity is $O(n+I)$ where $I$ is the number of inversions. This also means that the time complexity to sort an array that is almost sorted i.e. _small number of inversions_, is linear.> [!NOTE] Worst Case> - Occurs when the array is reversely sorted, $\theta(n^2)$ inversions> 	- 5,4,3,2,1> - Each iteration requires iterating through the entire sorted array> - Total: $$1+2+...+(n-1)=\sum_{i=1}^{n-1}i=\frac{(n-1)n}{2} $$"
Insertion Sort.md,Examples,
Insertion Sort.md,Overall Evaluation,
Information Theory.md,Shannon Information Content (Surprise),Can be interpreted as the *surprise* of an outcome. A lower probability outcome is more surprising!$$Surprise=log_2\frac{1}{p(x)}$$
Information Theory.md,Entropy,A measure of uncertainty. It is the expected surprise of an event.$$\begin{aligned}Entropy&= E(Surprise)\\&=\sum xP(X=x); \ x=surprise\\&=\sum log\frac{1}{p(x)}\times p(x)\\&=\sum p(x)\times (log(1)-log(p(x)))\\&=\sum-p(x)log(p(x))\end{aligned}$$
Information Theory.md,Information Gain,Can be interpreted as the reduction in entropy (made things more certain/less surprising).
Information Theory.md,Example,Drawing 3 cards out of a standard deck of 52 cards with replacement. Win = all cards are the same colour. Lose = not all the same colour.$$\begin{aligned}&P(Win)=2\times(\frac{1}{2})^3=\frac{1}{4}\\&P(Lose)=\frac{3}{4} \\&Entropy_{game}= -\frac{1}{4}log(1/4)-\frac{3}{4}log(3/4)=0.811\\\end{aligned}$$What is the information gain in the event you drew 2 cards (i.e. know what 2 of 3 cards suites are)?$$\begin{align}\text{If both are same color}:\\P(Win)=1/2\\P{Lose}=1/2\\Entropy=1\\\text{If both are different color}:\\P(Win)=0\\P(Lose)=1\\Entropy = 0\\E(Entropy)= \frac{1}{2}(1)+\frac{1}{2}(0)=0.5\\Gain=0.811-0.5=0.311\end{align}$$
Information Theory.md,Gini Impurity,
Index Based Algorithms.md,Index Based Selection,For a given selection operation $\sigma_{a=v}(R)$ (_meaning select all tuples in R where attribute a = v_) we can use an index on attribute a to gain cost savings.
Index Based Algorithms.md,Clustered Index,"Since each tuple with the same attribute value are packed in as little blocks as possible, a clustered index will have savings averaging:$$IO =B(R)/V(R,a)$$The actual value can be higher due to:1. All tuples with $a=v$ might be spread across more than 1 block"
Index Based Algorithms.md,Non-clustered Index,"We can assume that each tuple will be on a different block, a non-clustered index will have savings averaging:$$IO =T(R)/V(R,a)$$"
Index Based Algorithms.md,Index Based Joining,"Assume an operation to join S and R over an attribute $C$.If only S has an index on C, algorithm:1. Iterate over all the blocks of R2. For each tuple $t$ in each block $b$ with an attribute value $t.C$ use the index to find the matching attribute value in S3. Join and output"
Index Based Algorithms.md,Cost,"Step 1: incur a cost of B(R) as we need to read all blocks of RStep 2: each tuple of R requires a read of the index- Clustered index: $T(R)\times B(S) / V(S,C)$- Non-Clustered index: $T(R)\times T(S) / V(S,C)$Total cost: $B(R) + \text{index cost}$"
Index Based Algorithms.md,Sorted Index Join - Zig Zag Join,"With a sorted index on both relations, we can just perform the final step of [](Notes/Two%20Pass%20Algorithms.md#Sort%20Based%20Algorithms%7Csort-based%20joining).Index allows us to ignore retrieving data blocks where there are no matching keys."
Index Based Algorithms.md,Cost,"From example 15.12, the number of disk I/O's if R and S both have sorted indexes, the total cost would simply be that cost to read all blocks of R and S: 1500. Consider that a large fraction of R or S cannot match tuples of the other relation, then the total cost will be considerably less than 1500."
Index Based Algorithms.md,Practice Problems,Cost of access the data blocks:There are $\frac{k}{10}$ distinct values that we must access.There are a total of $B(R)/k=1000/k$ blocks having distinct valuesTotal block access = $\frac{1000}{k}\times\frac{k}{10}=1000$
HTTP.md,A brief history rundown,"1. HTTP 0.9 began in 1991 with the goal of transferring HTML between client and server.2. HTTP 1.0 evolved to add more capabilities such as header fields and supporting more than HTML file types, becoming a misnomer for hypermedia transport.A typical plaintext HTTP request3. HTTP 1.1 introduced critical performance optimisations such as keepalive connections, chunked encoding transfers and additional caching mechanisms.4. HTTP 2.0 aimed to improve transport performance for lower latency and higher throughput."
HTTP.md,HTTP message format,
HTTP.md,Request,"GET, POST, PUT, UPDATE, DELETE, HEAD methods are available"
HTTP.md,Response,
HTTP.md,User-Server State,"HTTP is a *stateless* protocol, and does not maintain information about the clients. This simplifies server design and allow for high-performance web servers."
HTTP.md,Cookies,"It is often desirable for a Web site to identify users, either to restrict user access or serve specific content.Cookie technology consists of 4 components:1. Cookie header line in the HTTP response message2. Cookie header line in the HTTP request message3. Cookie file kept on the user’s end system and managed by the user’s browser4. Back-end database at the Web site"
HTTP.md,Web Caching,A proxy server acts as a Web cache with its own disk storage and keeps recently requested objects. The proxy server sits on the LAN and reduces response time for a request.
HTTP.md,Optimisations in HTTP 1.1,
HTTP.md,HTTP Keepalive,Reuse existing TCP connections paired with [TCP Keep-Alive](Notes/Transmission%20Control%20Protocol.md#TCP%20Keep-Alive) to save 1 roundtrip of network latency
HTTP.md,HTTP Pipelining,"Persistent HTTP implies a strict FIFO order of client requests: *dispatch request, wait for full response, dispatch next request*. Pipelining moves the queue to the server side, allows the client to send all requests at once, and reduces server idle time by processing requests immediately without delay."
HTTP.md,Why not do server processing in parallel?,"The HTTP 1.x protocol enforces a requirement similar to that encountered in [TCP Head-of-Line Blocking due to its requirement for strict in-order packet delivery](Notes/Transmission%20Control%20Protocol.md#Head-of-Line%20Blocking), where there must be strict serialization of returned responses. Hence, although the CSS response finishes first, the server must wait for the full HTML response before it can deliver the CSS asset."
HTTP.md,Parallel TCP Connections,"Rather than opening one TCP connection, and sending each request one after another on the client, we can open multiple TCP connections in parallel. In practice, most browsers use a value of 6 connections per host.These connections are considered independent, and hence do not face the same head-of-line blocking issues in parallel server processing."
HTTP.md,Domain Sharding,"Although browsers can maintain a connection pool of up to 6 TCP streams per host, this might not be enough considering how an average page needs 90+ individual resources. If delivered all by the same host, there will be queueing delays:Sharding can artificially split up a single host *e.g. www.example.com into {shard1,shard2}.example.com*, helping to achieve higher levels of parallelism at a cost of additional network resources."
HTTP.md,Enhancements in HTTP 2.0,"HTTP 2.0 extends the standards from previous versions, and is designed to allow all applications using previous versions to carry on without modification."
HTTP.md,Binary Framing Layer,"At the core of the performance enhancements, is this layer which dictates how the HTTP messages are encapsulated and transferred. Rather than delimiting parts of the protocol in newlines like in HTTP 1.x, all communication is split into smaller frames and encoded in binary:> [! Frames, Messages and Streams]> Frames: The smallest unit of communication in HTTP 2.0, each containing a frame header, which at minimum identifies the stream to which the frame belongs. It contains data such as HTTP headers, payload etc.>> Messages: A complete sequence of frames that map to a logical message. It can be a request or response.>> Stream: A bidirectional flow of bytes within an established connection>> *Each TCP connection can carry any number of streams which communicates in messages consisting of one or multiple frames.*"
HTTP.md,Request and Response Multiplexing,"In [Why not do server processing in parallel?](Notes/HTTP.md#Why%20not%20do%20server%20processing%20in%20parallel?), we find that only one response can be delivered at a time per connection. HTTP 2.0 removes these limitations.With this, workaround optimisations in HTTP 1.x such as domain sharding is no longer necessary."
HTTP.md,Request Prioritisation,"The exact order in which the frames are interleaved and delivered can be optimised further by assigning a 31 bit priority value (0 represents highest, $2^{31}-1$ being the lowest). HTTP 2.0 merely provides the mechanism for which priority data can be exchanged, and *does not implement any specific prioritisation algorithm*. It is up to the server to implement this."
HTTP.md,Server Push,"A document contains dozens of resources which the client will discover. To eliminate extra latency, let the server figure out what resources the client will require and push it ahead of time. Essentially, the server can send multiple replies for a single request, without the client having to explicitly request each resource."
HTTP.md,Header Compression,"Each HTTP transfer carries a set of headers that describe the transferred resource and its properties. In HTTP 1.x, this metadata is always sent as plain text and adds anywhere from 500–800 bytes of overhead per request, and kilobytes more if HTTP cookies are required."
HTTP.md,Header table,A header table is used on both the client and server to track and store previously sent key value pairs. They are persisted for the entire connection and incrementally updated both by the client and server. Each new pair is either appended or replaces a previous value in the table.This allows a new set of headers to be coded as a simple difference from the previous set:
Heaps.md,Implementations,Binary Heap built with a [Binary Tree](Notes/Binary%20Tree.md):
Heaps.md,Operations,
Heaps.md,Fix Heap (maximising),"Method to obtain retain the heap structure after the root is removed. Assumes both left and right subtrees are already maximising heaps.```javafixHeap(H,k){if (H is a leaf)insert k in root of H;else {compare left and right child;largerSubHeap = the larger child of H;//key is the largest elementif (k >= key of root(largerSubHeap))insert k in root of H;//key is not the largest, move the largest element upelse{insert root(largerSubHeap) in root of H;//recursively find the spot where key is the largestfixHeap(largerSubHeap, k)}}}```"
Heaps.md,Example,"If there are 2 child nodes, this operation will take 2 comparisons:"
Heaps.md,Heapify,Method to obtain the maximising heap property from an arbitrary tree.Fix the heap from the bottom up:
Heaps.md,Complexity for heap construction:,
Heap Sort.md,General Idea,"Data structure based sorting algorithm using [Heaps](Notes/Heaps.md).Makes use of the Partial Order Tree property:A tree is a maximising partial order tree if and only if each node has a key value greater than or equal to each of its child nodes.1. Create a maximizing heap2. Remove the root node, giving us the largest value. Fill an array from the back with this max value3. Fix the heap structure4. Repeat until we obtain a sorted array"
Heap Sort.md,Pseudocode,Remove the max element and retain the heap structure using [](Notes/Heaps.md#Fix%20Heap%20maximising)
Heap Sort.md,Complexity,"Summary of complexities for Heapsort and corresponding heap structure methods:Heapsort has a best, worst and average case time complexity of $O(nlogn)$"
Heap Sort.md,Examples,
Hash Tables.md,Hash functions,"A hash function allows us to compute the slot from the key and reduce the amount of storage required to store all the keys.The time complexity for searching an element is O(1)_A good hash function satisfies (approximately) the assumption of simple uniform hashing: each key is equally likely to hash to any of the m slots, independently of where any other key has hashed to._"
Hash Tables.md,Division method,Divide the key by some prime number not too close to a power of 2 (bits are in powers of 2)$h(k)=k\ mod\ m$
Hash Tables.md,Multiplication method,"_I am not too sure how this works_. Refer to “Introduction to algorithms”, 2009, p. 263"
Hash Tables.md,Collision Resolution,
Hash Tables.md,Linked lists,"We can combine the hash table with a [Linked Lists](Linked%20Lists), placing all the elements that hash to the same slot into the same linked list:| Search                                 | Insert | Delete || -------------------------------------- | ------ | ------ || Proportional to the length of the list | O(1)   | O(1) if doubly-linked       |"
Hash Tables.md,Probing,"The extra memory freed by not storing pointers provides the hash table with a larger number of slots for the same amount of memory, potentially yielding fewer collisions and faster retrieval."
Hash Tables.md,Open addressing,"We systematically examine table slots until either we find the desired element or we have ascertained that the element is not in the table.```HASH-INSERT(T, k)i = 0repeatj = h(k,i)if T[j] == NILT[j] = kreturn jelse i++until i==merror ""hash table overflow""```"
Hash Index.md,Static Hash,
Hash Index.md,Insertion and Deletion,Bucket overflow can be handled naively by adding additional pointer to a separate block.
Hash Index.md,Extensible Hash Index,"If the number of buckets is fixed from the start, we will end up with many additional pointers to additional overflow buckets. This incurs more I/O as more records are added.Idea: use only the first i bits output from the hash function to point to a directory."
Hash Index.md,Insertion and Deletion,Increase i when overflow:Update the directory structure:Delete implementations:1. Merge blocks when removing record2. Do not merge blocks at all
Hash Index.md,Duplicate Keys,Unable to allocate different directory for duplicate keys:
Hash Index.md,Practice Problems,i. 2 I/Oii. All blocks needs to be searched: 11 I/Oa. Hash index on IDb. B+ tree index on ID to support better range queryc. Multi key index on ID and name
Hardware Protection.md,Dual mode operation,Differentiates between at least 2 modes of operations1. User mode: execution of user processes2. __Monitor mode__ (supervisor/system/kernel mode): execution of operating system processes> [!Kernel mode vs root/admin]> Kernel mode is not the same as root/admin privileges. Kernel or user modes are hardware operation moes while the root/admin is just a user account in the OS.>> The root/admin may execute code in kernel mode indirectly.
Hardware Protection.md,I/O Protection,_All I/O instructions are privileged instructions._ The OS will ensure that they are correct and legal.
Hardware Protection.md,Memory Protection,OS needs to set the range of legal addresses a program may access. This is done using 2 registers.Base register: holds the first legal memory addressLimit register: contains the size of the legal range
Hardware Protection.md,Practice Problems,"Given a base register value of 0x1000 and a limit register value of 0x1000, access to memory location 0x1FFF will generate a trap.False. Each access to memory by a process must be in the range [base, base+limit-1]. In this case, it translates to the range [0x1000, 0x1FFF]."
Greedy Best First Search.md,Graph Traversal,_Assuming ties are handled in alphabetical order_Expansion Order:A > B > C > GFinal Path:A > B > C > G
GPU Architecture.md,CUDA,
GPU Architecture.md,Architecture,
GPU Architecture.md,Programming Model,CUDA works on a heterogeneous programming model that consists of a host and device. Host calls the device to run the program.- Host: CPU- Device: GPU
GPU Architecture.md,Programming Language,The source code is split into host (compiled by standard compilers like gcc) and device components (compiled by nvcc).
GPU Architecture.md,Kernel,
GPU Architecture.md,Threads and Thread Blocks,We can access important properties of the kernel:- Block ID: `blockIdx.x` gives us the ID of the thread block- Thread ID: `threadIdx.x` gives us the ID of the thread within a thread block- Dimension: `blockDim.x` gives us the number of threads per blockThe exact thread number can be found using `blockIdx.x* blockDim.x + threadIdx.x`Multi-dimensionality:
GPU Architecture.md,Synchronisation,
GPU Architecture.md,Memory management,"The above code does not take advantage of GPU parallelism in the CUDA core. We can create 1 block with 3 threads to achieve parallelism: `vector_add_cu<<<1,3>>>(d_c, d_a, d_b);`Use the threadIdx to access the memory:> [! Threads vs Blocks]> The example can also be achieved using 3 blocks each with 1 thread. However, parallel threads have the advantage to directly communicate and synchronise with each other due to shared hardware. Sharing memory between blocks would require *global memory access*"
GPU Architecture.md,Example,"```c++//initialize 1 block and 3 threads. We cannot use 3 blocks for this implementation as blocks would nt be able to share the local variable memoryDot_prod_cu<<<1,3>>>(d_c, d_a, d_b);__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){//use __shared__ to allow threads to share data__shared__ int tmp[3];int i = threadIdx.x;tmp[i] = d_a[i] * d_b[i];//wait for all threads to complete to prevent premature entering into if block__syncthreads();if (i==0){int sum = 0;for (int j = 0; j < 3; j++)sum = sum + tmp[j];*d_c = sum;}}```"
GPU Architecture.md,Internal Operations,"- Each SM contains multiple SP cores. Each core can only execute 1 thread.- Each block of threads can be scheduled on any available SM by the runtime system, but 1 block can only exist on 1 SM."
GPU Architecture.md,Warps,
GPU Architecture.md,SIMT,"Warps enable a unique architecture called Single Instruction Multiple Thread. This means each warp executes only one common instruction for all threads.Within a single thread, its instructions are- pipelined to achieve instruction-level parallelism- issued in order, with no branch prediction and speculative executionIndividual threads in a warp start together, at the same instruction address- but each has its own instruction address counter and registers- free to branch and execute independently when the thread diverges, such as due to data-dependent conditional execution and branch."
GPU Architecture.md,Thread Divergence,"Branch statements will result in some threads in a warp wasting their clock cycles. This is because the threads in the warp must all execute the same instruction. For some which satisfy the condition, computation is done else NOP."
GPU Architecture.md,Practice Problems,"```c__global__ void stencil(int N, int *input, int *output) {blockNum = blockIdx.x;i = threadIdx.x + blockNum * N;int sum = input[i];for(int i = 1; i < 3; i++) {sum += input[i-i]sum += input[i+i]}}int N = len(input) / BLOCK_SIZEoutput = (int *) malloc(N * sizeof(int))stencil<<<N, BLOCK_SIZE>>>(N, input, output)```"
Games as Search Problems.md,Minimax Search,"Maximize own utility and minimize opponent's1. Generate game tree to terminal state or a certain depth2. Calculate the utility from the bottom-up (MAX turn will maximize own utility, MIN turn will minimize MAX utility)3. Select the best move (we can assume that we start as MAX)Tic-Tac-Toe Tree Example:[Reflective and rotational symmetries:](https://courses.cs.duke.edu/cps100e/current/assign/ttt/#:~:text=There%20are%20four%20reflective%20symmetries,the%20board%20on%20the%20left.&text=This%20means%20there%20are%20eight,board%20on%20each%20line%20above)"
Formal Specification.md,Motivation,*Boehm’s First Law: Errors are more frequent during requirements and design activities and are more expensive the later they are removed.*Formality helps us to obtain higher quality specifications which are able to detect serious problems in original informal specifications. It also enables automated analysis of the specification.
Formal Specification.md,Problem Abstraction,Process of simplifying the problem at hand and facilitating our understanding of a system.- focus on intended purpose- ignore details of how the purpose is achieved
Formal Specification.md,Systems,"- Application: A physical entity whose function and operation is being monitored and controlled- Controller: Hardware and software monitoring and controlling the application in real time- Actuator (effector): A device that converts an electrical signal from the output of the computer to a physical quantity, which affects the function of the application.- Sensor: A device that converts an application’s physical quantity into an electric signal for input into the computer"
Formal Specification.md,Example on cold vaccine storage,"A system that stores vaccine at a temperature that *should not exceed -70 degrees*- Application: storage chamber- Sensor: temperature sensor- Actuator: cooling engine- Controller (software):- checks measurements- sets the cooling engineMight also:- output information on a display- Write to log file and send it over networkSafety property: $temp+\delta\le-70$[Fault Tree Analysis](Notes/Risk%20Analysis.md#Fault%20Tree%20Analysis):Safety invariants (things that should *always* hold) that need to be verified:1. Always after controller has reacted, if sensor is not OK then alarm is raised and actuator is in decr2. Always after controller reacted, if sensor is OK and temp + Δ ≥ -70 then cooler is in decr"
Formal Specification.md,Formal Specification Frameworks,[[Event-B]]
First Order Logic.md,Forming FOL sentences,"“All dogs are mammals""General form:  $\forall xDog(x)\implies Mammal(x)$Use conjunction? $\forall x Dog(x) \land Mammal(x)$ : **this is means everything is a dog and a mammal!**""John owns a dog""General form: $\exists x Dog(x)\land Owns(John,x)$Use implication? $\exists xDog(x)\implies Owns(John,x)$: **this can mean that John owns things which are not dogs as well**"
First Order Logic.md,Inference Rules,Using substitutions is also called _Generalized Modus Ponens_.The substitution used is called the _unifier_.
First Order Logic.md,Getting to CNF,"$$\begin{align}\exists xStudent(x)\land \neg Takes(x,AI)\tag1\equiv Student(K)\land\neg Takes(K,AI) \\\tag2 \exists xStudent(x)\land Takes(x,AI)\land\neg pass(x,AI)\equiv \\Student(F)\land Takes(F,AI)\land \neg pass(F,AI)\tag3\\\forall x,y \neg Student(x)\lor\neg pass(x,y)\lor\neg hard(y)\lor diligent(x)\models\\\tag4\neg Student(x)\lor\neg pass(x,y)\lor\neg hard(y)\lor diligent(x) \\3+4:\neg pass(x,y)\lor\neg hard(y)\lor diligent(x)\tag5 \\\tag6 3+4+5: Takes(x,AI)\lor\neg hard(AI)\\6+Subst\{x/K\}\ Takes(K,AI)\lor\neg hard(y)\tag7\\1+7: \neg hard(AI)\tag8\\8+iv:\emptyset\end{align}$$"
File Systems.md,File,A file is an unstructured sequence of bytes. Each byte is individually addressable from the beginning of the file.
File Systems.md,File access,- Sequential: information is processed from the beginning of the file one byte after the other- Direct access: bytes can be read in any order by referencing the byte number
File Systems.md,Protection,"- Owner: Permissions used by the assigned owner of the file or directory- Group: Permissions used by members of the group that owns the file or directory- Other: Permissions used by all users other than the file owner, and members of the group that owns the file or the directory"
File Systems.md,Data Structures,
File Systems.md,File Control Block,
File Systems.md,Open File Table,"`open()` syscall:- First searches the system wide OFT to see if it is being used by another process. If it is, per process open file table entry is created pointing to this.- If not, the directory structure is searched for the file. FCB is copied to the system wide OFT. Entry is made in per process OFT and a pointer to the entry is returned.The open file tables saves substantial overhead by serving as a cache for the FCB. Data blocks are *not* kept in memory, instead, when the process is closed, the FCB is entry is removed and the updated data is copied back to the disk."
File Systems.md,File Descriptor,A file descriptor is a non-negative integer which indexes into a per-process file descriptor table which is maintained by the kernel. This in turn indexes into a system-wide open file table. It also indexes into the inode table that describes the actual underlying files. All operations are done on the file descriptor
File Systems.md,Storage allocation,"File-Organisation Module: allocates storage space for files, translates logical block addresses to physical block addresses, and manages free disk space."
File Systems.md,Contiguous,"Each file occupies a set of contiguous blocks on the disk.Advantages:- Simple as only starting location and length is required- Supports random accessDisadvantages:- Finding a hole big enough may result in external fragmentation- File space is constricted by size of the hole, it might need to be moved to a bigger hole in the future- If file space is overestimated there will be internal fragmentation> [! The delete operation]> Deleting a data block stored with contiguous allocation requires shifting of the data blocks.> *e.g. Delete data block 5 in a file with 10 data blocks*:> i.e. Read block 6, write block 5 with data from block 6, read block 7 and write block 6 with block 7 etc."
File Systems.md,Linked,"Each file is a linked list of disk blocks and the blocks may be scattered anywhere on the disk.Advantages:- Simple as only need starting address- No wasted space and no constraint on file sizeDisadvantages:- No random access*Assuming 4 bytes is reserved for the pointer to the next block:*Why displacement need to + 4? #question i think maybe the first 4 bytes is used for the pointer, so displacement needs to +4 to skip the pointer address.> [! The delete operation]> Deleting a data block stored with linked allocation requires an update to the connected pointer.> *e.g. Delete data block 5 in a file with 10 data blocks*:> 6 reads to reach block 5. 1 write to update the pointer of block 4 to block 6"
File Systems.md,Indexed allocation,"Each file has an index block which contains all pointers to the allocated blocks. Directory entry contains the block number of the index block. Similar to a [page table](Notes/Memory%20Organisation.md#^b8969e) for memory allocation.Advantages:- Supports random access- Dynamic storage allocation without external fragmentationDisadvantages:- Overhead in keeping index blocks- Internal fragmentation as the last block that the index is pointing to may not be fully utilised> [! The delete operation]> Deleting a data block stored with indexed allocation requires an update to the indexed pointers.> *e.g. Delete data block 5 in a file with 10 data blocks*:> 1 read for the index block, 4 writes to update pointers"
File Systems.md,inode,"An inode is an index block. For each file and directory there is an inode. The inode contains file attributes, 12 pointers to direct blocks (data blocks) and 3 pointers point to indirect blocks (index blocks) with 3 levels of indirection.Indirection allows the system to support large file sizes:Using the inode:"
File Systems.md,Directories,
File Systems.md,Structure,"A directory can be structured in two ways:1. each entry contains a file name and other attributes2. each entry contains a file name and a pointer to another data structure where file attributes can be found> [!Disk reads when navigating a directory]> Assume that root directory is in memory> Open(“/usr/ast/mbox”) will require 5 disk reads> 1. load inode of “usr”> 2. load data block of “usr” (i.e., directory “usr”)> 3. load inode for “ast”> 4. load data block of “ast” (i.e., directory “ast”)> 5. load inode for “mbox”"
File Systems.md,Tree Structured,"Path Name- Absolute Path Name: begins at the root and follows a path down to the specific file, e.g., /spell/mail/prt/first- Relative Path Name: Defines a path from the current directory, e.g. current directory is: /spell/mail relative path name for the above file is: prt/firstCharacteristics:- Efficient Searching: File can be easily located according to the path name.- Naming: Files can have the same name under different directories.- Grouping: files can be grouped logically according to their properties"
File Systems.md,Acyclic Graph Directories,The tree structure prohibits sharing of files or directories while an acyclic graph allows this. It is a natural generalisation of the tree structure.
File Systems.md,Links as a UNIX implementation,"A link is a directory entry which is a poitner to another file or subdirectory- A hard link points to the data on storage, while a soft link can point to another link which points to information on storage.- Both linking strategies allow a separate file name to be used for the source file name. This source file name will resolve to the target file data by following the link."
File Systems.md,What happens on deletion?,"One possibility is to remove the file whenever anyone deletes it, but this action may leave dangling pointers to the now-nonexistent file. Worse, if the remaining file pointers contain actual disk addresses, and the space is subsequently reused for other files, these dangling pointers may point into the middle of other files.Soft links- Search for liks and remove them: expensive unless a list of links is kept with the file OR- Leave the links and remove them only when trying to access themHard links- Preserve file unless all references are deleted. A count to the number of references is maintained in the file (a new link ++, deleting a link--)."
File Systems.md,Why not just duplicate the file?,Duplicate directory entries make the original and the copy indistinguishable. A major problem with this is maintaining consistency when a file is modified
File Systems.md,Disk Space Management,"Block size affects both data rate and disk space utilisation- Big block size: file fits into few blocks resulting in fast to find & transfer blocks, but wastes space if file does not occupy the entire last block- Small block size: file may consist of many blocks resulting in slow data rate"
File Systems.md,Managing Free Blocks,There is a need to track which blocks are free in order to allocate disk space to files
File Systems.md,Bitmap,"Each block is represented by 1 bit, 1 (free) and 0 (allocated)Advantage:- Simple and efficient to find the first free block via bit manipulation. i.e. Find the first non-0 word, and find the first bit 1 in the word.Disadvantage:- Takes up additional space as each block requires 1 bit- Inefficient to look up this bitmap unless the entire map is kept in memory"
File Systems.md,Linked list,"The pointer to the next block is stored in the block itself, hence to read the entire list, each block must be read sequentially requiring substantial I/O time."
File Systems.md,Practice Problems,"a. False. Owner and the group which owner belongs to is able to read.b. False. The OFT caches the FCB rather than the data block.c. False. Using linked file allocation, any free data block can be used.a. The previous links will now point to the data of the new file. To avoid this, dangling links need to be cleaned up.b.Single copy- Race conditions, mutual exclusionMultiple copy- Storage waste- Inconsistencya. 5 disk accesses1. Load inode of usr2. Load directory for usr3. Load inode for ast4. Load directory for ast5. Load inode for mboxb. Seek: no disk reads neededCurrent position is 5900: Logical block 5, byte 900.read(100): 1 disk read by following direct pointerread(200): 2 disk read by following single indirect pointer3 disk reads totalc.Number of pointers in 1 index block = $1000/2=500$File size supported = $(6+500) \times 1000=506,000B$File data can be stored across different physical storage blocks. A smaller physical block helps to reduce internal fragmentation as the last block occupied by the file can is only 512B compared to 4KB. Using the larger block size would also help to improve throughput."
Fibonacci Sequence.md,Problem Formulation,The Fibonacci Series:$$F_i=F_{i-1}+F_{i-2} $$
Fibonacci Sequence.md,Strategy,
Fibonacci Sequence.md,Pseudocode,
Façade Pattern.md,Problems we want to solve,1. Decoupling of object interactions2. Open-closed principle3. Least Knowledge principle
Façade Pattern.md,Pros,1. Decouples client from complex system logic2. Reduces dependencies on classes: favour composition over inheritance
Façade Pattern.md,Cons,1. Complexity and possible rework
Failure Detectors.md,Failure Models,- Fail-stop: can reliably detect failure (achievable in synchronous model)- Fail-noisy: can eventually detect failure (achievable in partially synchronous model)- Fail-silent: cannot detect between a crash or omission failure (asynchronous model)
Failure Detectors.md,Properties,
Failure Detectors.md,Completeness,No false negatives: all failed processes are suspected*Asynchrony: suspect every node to achieve completeness*
Failure Detectors.md,Strong completeness,
Failure Detectors.md,Weak completeness,
Failure Detectors.md,Accuracy,No false positives: correct processes are not suspected*Asynchrony: suspect 0 nodes to achieve accuracy*
Failure Detectors.md,Strong accuracy,No correct process is ever suspected
Failure Detectors.md,Weak accuracy,There exists a correct process *P* which is never suspected by any process
Failure Detectors.md,Eventual Strong accuracy,"After some time, strong accuracy achieved, prior to this, any behaviour possible.- This does not satisfy weak accuracy, as before achieving strong accuracy, any behaviour allowed"
Failure Detectors.md,Eventual Weak accuracy,"After some time, weak accuracy achieved, prior to this, any behaviour possible."
Failure Detectors.md,Perfect failure detector,"Only implementable in the [Synchronous system](2203%20Distributed%20Systems.md#Synchronous%20system), else there will be some incorrectly suspected processes while figuring out the actual delay."
Failure Detectors.md,Eventually perfect failure detector,"How to achieve strong accuracy?Each time p is inaccurately suspected by a correct q- Timeout T is increased at q- Eventually system becomes synchronous, and T becomes larger than the unknown bound δ (T>γ+δ)- q will receive HB on time, and never suspect p again"
Failure Detectors.md,Leader Election,"We want all processes to detect a single and same correct process. To do so, we need to define the set of failed processes (using a FD)"
Failure Detectors.md,Why local accuracy?,
Failure Detectors.md,Implementation,
Failure Detectors.md,Eventual Leader Election,
Failure Detectors.md,Reductions,
Failure Detectors.md,Strong completeness equivalent to weak completeness,"- If strong accuracy, no one is ever inaccurate, reduction never spreads inaccurate Susp- If weak accuracy, everyone is accurate about at least one process p, no one spreads inaccurate information about p"
Failure Detectors.md,Eventual leader election $\Omega\equiv \diamond S$,"Implement S using $\Omega$:- Strong completeness $\equiv$ weak completeness: if we suspect everyone (except the leader which we know is correct), every process suspects all incorrect processes- Eventual weak accuracy: if we only trust the leader, there exists 1 correct process not suspected"
Failure Detectors.md,Summary,
Factory Pattern.md,Problems we want to solve,1. Decouple class selection and object creation from the place where the object is used.2. Need to instantiate a set of classes but without knowing exactly which one until runtime.3. Do not want to expose object creation logic to the client.The interface and concrete product classes implement an additional [Strategy Pattern](Notes/Strategy%20Pattern.md) design which allows the algorithms to be instantiated and changed during runtime.
Factory Pattern.md,Pros,1. Encapsulation of object creation2. Extensibility of classes3. Can easily change object creation logic without affecting context due to decoupling
Factory Pattern.md,Cons,1. Complexity
Event-B.md,Abstract Machine Notation,
Event-B.md,Syntax,
Event-B.md,Context,
Event-B.md,Machine,
Event-B.md,Events,
Event-B.md,Actions,
Event-B.md,Set,
Event-B.md,Examples,
Event-B.md,University Access,A system for controlling access to a university building- An university has some fixed number of students.- Students can be inside or outside the university building.- The system should allow a new student to be registered in order to get the access to the university building.- To deny the access to the building for a student the system should support deregistration.- The system should allow only registered students to enter the university building.
Event-B.md,Coffee Club,
Event-B.md,Printer Access,- A system should support adding a permission for a student in order to get an access to a particular printer and removing a permission.- A system should support removing a student’s access to all printers at once.- A system should support giving the combined permissions of any two students to both of them.
Event-B.md,Requirements Document,
Event-B.md,Modelling,"- To keep track of changing permissions, it will make use of a variable access whose type is a relation between STUDENTS and PRINTERS."
Event-B.md,New requirement: a student can use no more than 3 printers,
Event-B.md,Seat Booking System,
Ensemble Learning.md,Bootstrap Aggregating (Bagging),"Use replicates of the training set by sampling with replacement to train each model. Combine $B$ such models together, by running the test data on each replicate. The classification which received the most ""votes"" from the replicates is the decided value."
Ensemble Learning.md,Random Forest,For use in [Decision Trees](Notes/Decision%20Trees.md). Bagging + random feature selection (randomly select a feature to split a node) for every node.
Ensemble Learning.md,Estimating accuracy,Out-of-bag error: About 1/3 of the training set is not used in bagging. These data points can be used to test the accuracy of the random forest.
Ensemble Learning.md,Boosting,Use the data to train a set of weak learners/classifiers. Combine them to create a single strong classifier.
Ensemble Learning.md,Adaboost,"Let the sample data $S = \{(x_1,y_1),...(x_m,y_m)\}$1. Each sample in the train data is given the same weight: $w_i = \frac{1}{m}$2. Train a weak classifier (for example a *stump* which is a decision tree with only 1 fork) using S and their weights. Choose the weak classifier $h$ which minimizes the training error.3. Compute the **reliability coefficient** $a=log_e(\frac{1-error}{error})$ for the chosen weak classifier. This can be seen as the *amount of say for this classifier*.4. Use the reliability to scale the weights of each training sample. $w_{t+1}=w_{t}\times e^{-a_tyh_t(x)}$. A misclassification causes the weight to be increased and vice versa.5. Place more emphasis on correctly classifying the higher weighted sample, e.g. by randomly sampling based on a probability distribution using the weights (higher weight means more likely to be chosen)."
DNS.md,Features,
DNS.md,Address Translation,"To identify a host, people prefer the mnemonic hostname identifier such as google.com. However, routers prefer fixed length hierarchically structured IP addresses. DNS's job is to provide the translation between these two references.DNS being employed by HTTP:1. The same user machine runs the client side of the DNS application.2. The browser extracts the hostname, www.someschool.edu, from the URL and passes the hostname to the client side of the DNS application.3. The DNS client sends a query containing the hostname to a DNS server.4. The DNS client eventually receives a reply, which includes the IP address for the hostname.5. Once the browser receives the IP address from DNS, it can initiate a TCP connection to the HTTP server process located at port 80 at that IP address"
DNS.md,Host aliasing,"DNS can be invoked by an application to obtain the **canonical hostname** for a supplied alias hostname. A host with a complicated hostname can have one or more alias names. For example, a hostname such as relay1.west-coast .enterprise.com could have two aliases such as enterprise.com and www.enterprise.com. The hostname relay1 .west-coast.enterprise.com is said to be a canonical hostname."
DNS.md,Load distribution,"Busy sites can have different servers all replicating the same content, each having their own IP address. DNS stores the entire set of addresses, but is able to rotate their order with each reply. Because the client sends its HTTP request message to the first IP address, this performs load distribution."
DNS.md,How it works,"Rather than having 1 central DNS server which does not scale, DNS servers are distributed and organized in a hierarchical structure: Root -> Top Level Domain -> Authoritative.- Root: provides the IP address of TLD servers. There are 400 root name servers scattered across the world- TLD: com, org, net etc. and all country TLD uk, sg etc. maintained by companies and countries.- Authoritative: houses the DNS records of organization host IP addresses e.g. amazon.com. Can be done in house or outsourced to some service provider- Local: close to the host which acts as a proxy, forwarding queries to the DNS server hierarchy> [!Note]> TLD server may not know directly the authoritative server address, but rather some other intermediate server. In this way, there could be 2 more DNS messages requiredQuery 1 is a recursive query, as it asks to obtain the mapping on its behalf. Subsequent queries are iterative as all replies are directly returned to local DNS server. This is the more typical scenario. There are also queries which are all recursive:"
DNS.md,DNS Caching,"In a query chain, when a DNS server receives a DNS reply (containing, for example, a mapping from a hostname to an IP address), it can cache the mapping in its local memory. This DNS server can provide the desired IP address, even if it is not authoritative for the hostname. Because hosts and mappings between hostnames and IP addresses are by no means permanent, DNS servers discard cached information after a period of time (often set to two days)."
DNS.md,DNS Records,
DNS.md,Inserting Records,"A registrar is a commercial entity that verifies the uniqueness of the domain name, enters the domain name into the DNS database and collects a small fee.Example registering domain name `networkutopia.com`, two records are inserted:`(networkutopia.com, dns1.networkutopia.com, NS)`(dns1.networkutopia.com, 212.212.212.1, A)`"
DNS.md,DNS attacks,
DNS.md,Exercises,"a.1. Host sends request to local DNS server2. Local DNS makes a query to the root DNS server3. Root DNS returns the Top level domain DNS server for ""com""4. Local DNS makes query to TLD5. TLD returns the authoritative name server for ""fws.com""6. Local DNS makes query to DNS server for ""fws.com""7. Authoritative DNS returns the IP address for ""punchy.fws.com""8. Local DNS returns this IP address to the hostb. Query 1 is recursive. The rest are iterativec. Yes%%[🖋 Edit in Excalidraw](Pics/Transmission%20Control%20Protocol%202023-02-07%2017.09.11.excalidraw.md), and the [dark exported image](Pics/Transmission%20Control%20Protocol%202023-02-07%2017.09.11.excalidraw.dark.svg)%%"
Distributed Hash Table.md,Hash Table,
Distributed Hash Table.md,Distributed (assigning keys to peers),"Pairs are evenly distributed among peers, with each peer only knowing a small number of other peers. To resolve a query, a small number of messages are exchanged to obtain the value."
Distributed Hash Table.md,Circular DHT,Each peer is only aware of its immediate successor and predecessor.Average of $N/2$ messages needed.
Distributed Hash Table.md,Shortcuts,
Distributed Hash Table.md,Peer churn,
Distributed Shared Memory.md,Registers,"A register represents each memory location. The operations are:- write(r,v): update the value of register $x_r$ to v- read(r): return the current value of register $x_r$"
Distributed Shared Memory.md,Trace,"A trace is a sequence of events- $r-inv_i(r)$, $r-res_i(v)$: read invocation by process pi on $x_r$ register and the corresponding response with value $v$- $w-inv_i(r)$, $w-res_i(v)$: write invocation by process pi on $x_r$ register and the corresponding response with value $v$"
Distributed Shared Memory.md,Properties,"Well-formed- First event of every process is an invocation- Each process alternates between invocations and responsesSequential- 𝑥-inv by i immediately followed by a corresponding 𝑥-res at i- 𝑥-res by i immediately follows a corresponding 𝑥-inv by i, i.e. no concurrency, read x by p1, write y by p5, ...Legal- T is sequential- Each read to Xr returns last value written to register XrComplete- Every operation is complete- Otherwise T is partial- An operation O of a trace T is- complete if both invocation & response occurred in T- pending if O invoked, but no responsePrecedence- op1 precedes op2 in a trace T if (denoted <T)- Response of op1 precedes invocation of op2 in T- op1 and op2 are concurrent if neither precedes the other"
Distributed Shared Memory.md,Regular Register Algorithms,"A regular register is one that meets the following criteria:Termination- Each read/write operation issued by a correct process eventually completes.Validity- Read returns last value written if- Not concurrent with another write, and- Not concurrent with a failed write- Otherwise may return last or concurrent “value”"
Distributed Shared Memory.md,Fail-Stop Read-one Write-All,"Uses [perfect failure detector P](Notes/Failure%20Detectors.md#Perfect%20failure%20detector):write(v)1. Update local value to v2. [Fail Stop Broadcast](Notes/Broadcast%20Abstractions.md#Fail%20Stop) v to all, and each node locally updates to v: 1 RTT needed3. Wait for ACK from all correct processes4. Return: this return means that all processes have updated locally to v, validity is ensured!read1. Return local value: 0 RTT needed[Eventually perfect failure detector](Notes/Failure%20Detectors.md#Eventually%20perfect%20failure%20detector) will not work here as it might falsely suspect some processes as having crashed. During this time, since a write on another process only waits for ACKs from all correct processes, it could return early. A read on the falsely suspected process will incorrectly return the old value."
Distributed Shared Memory.md,Fail silent Majority voting,"Make use of timestamp-value pairs, tvp = (ts, v), where the timestamp can be used to determine which value is more recent. Each process stores the value of register r with max timestamp of each register r.Majority idea is based of [quorums](Notes/Distributed%20Abstractions.md#Quorums).- Read and write operation reads from quorums, this means at least 1 process knows the most recent value- 1 RTT is needed- 1 RTT is needed"
Distributed Shared Memory.md,Sequential Consistency,"Allows executions whose results appear as if the operations of each processes were executed in some sequential order according to ""local time"" (we can reorder operations across processes but not locally):"
Distributed Shared Memory.md,Liveness requirements,"- Wait-free: no deadlocks, no livelocks, no starvation- Lock-free: no deadlock, no livelocks, maybe starvation- Obstruction-free: no deadlock, maybe livelocks and starvation"
Distributed Shared Memory.md,Register Linearizability/Atomicity,"Allows executions whose results appear as if the operations of each processes were executed in some sequential order according to ""global time"" (cannot reorder):"
Distributed Shared Memory.md,Read/Write Majority Problem,
Distributed Shared Memory.md,Read-Impose Write Majority,
Distributed Shared Memory.md,Extending to N readers N writers (Read-impose Write-consult-majority),"Problem:Before writing, read from majority to get the latest timestamp (query phase before update phase): 2 RTT needed"
Distributed Shared Memory.md,Eventual Consistency,"State updates can be issued at any replica/correct process. All updates are disseminated via BEB, RB,...- Each correct process that receives all updates should deterministically converge to the same state.- Eventually every correct process should receive all updates...- Problem: When can a process know it has received all updates??"
Distributed Shared Memory.md,Strong Eventual Consistency,"If state operations are **commutative** and processes exchange information, eventually they converge to an identical view."
Distributed Shared Memory.md,Conflict Free Replicated Data Types (CRDTs),"Data structures which implement strong eventual consistency.The join operation allows there to be a commutative operation relationship between sets. However, operations need to have a strict monotonically increasing effect on the set."
Distributed Shared Memory.md,State Based CRDT (CvRDT),
Distributed Shared Memory.md,Grow-Only Counter,
Distributed Shared Memory.md,Up-Down Counter,
Distributed Shared Memory.md,Or-Set,
Distributed Shared Memory.md,Operation Based CRDTs (CmRDTs),CmRDTs impose stricter assumptions. Causally dependent updates are replaced with [Causal Broadcast](Notes/Broadcast%20Abstractions.md#Causal%20Broadcast) and the join function is replaced with any commutative update function.- Less states and IO required (only the operations are broadcasted)- More restrictions to programming model leading to less flexibility
Distributed Shared Memory.md,Or-Set,
Distributed Data Management.md,Distributed Transactions,[Transaction Management](Notes/Transaction%20Management.md) for distributed systems. All shards should either commit/abort the same transaction.
Distributed Data Management.md,Atomic Commit,"The de-facto protocol for atomic commit is *two-phase commit (2PC)*Approach: use 1 process as the *coordinator* (leader). Given a proposed transaction T, commit if all followers agree to commit. Abort if at least one follower aborts/fails.Problem: if the process was to fail after the decision was made by the coordinator it will be unable to apply the changes locally in the shard.- Build a more reliable system by building a replicated state machine within the shard. Replicated log will allow fault toleranceProblem: replicated cluster failure will cause loss of entire log- Perform replication across different clusters. With a replica of each shard in each data centreProblem:  coordinator was a single point of failure- Replicate coordinator in the same way for fault tolerance. Second phase of 2PC is no longer needed as each shard can access the local log for decision (abort/commit) without additional broadcast."
Distributed Data Management.md,Distributed Snapshotting,Capturing the global state of a distributed system.
Distributed Data Management.md,Consistent Cuts,Properties:1. Termination: eventually every process records its state2. Validity: all recorded states correspond to a consistent cut
Distributed Data Management.md,Chandy Lamport Algorithm,Approach: disseminate a special marker to mark events during the cut.- Channels and processes record state (add to snapshot) until the marker has been received. E.g. channel incoming to p2 continuously records messages until a marker is passed through it- Snapshot is complete once everyone has seen the marker.
Distributed Data Management.md,Epoch-based Snapshotting,"For continuous data stream processing, it is difficult to log individual task executions.Approach: divide computations into epochs, such as stages, and treat them as 1 transaction.The Chandy Lamport algorithm is not enough, as it will capture a lot of in-flight messages. We want to capture just the states which would in itself reflect the effect of these messages.This is done by *epoch alignment*:1. Allow all messages to go through until an epoch change marker is introduced2. On receiving the marker, log the state3. When a process receiving the marker has multiple channels, prioritise inputs from channels which have not seen the marker until they all see the marker.4. Terminate once all processes seen the marker."
Distributed Abstractions.md,Event based Component Model,"The distributed computing model consists of a set of processes and a network. Events can be used as messages between components of the same process, which trigger event handlers.Types of events- Requests: incoming to component- Indications: outgoing from component"
Distributed Abstractions.md,Specifications,"A distributed system specification includes the interface, correctness properties and model/assumptions."
Distributed Abstractions.md,Interface,"This specifies the API, importantly, the requests and events of the service"
Distributed Abstractions.md,Correctness Properties,Any trace property can be expressed as a *conjunction* of safety and liveliness properties.
Distributed Abstractions.md,Safety,"Properties that state that nothing bad ever happens. It can only be:- satisfied in infinite time (you cannot be sure you are safe)- violated in finite time (when bad happens)The **prefix** of a trace T is the first k (for k ≥ 0) events of T- cut off the tail of T- finite beginning of TAn **extension** of a prefix P is any trace that has P as a prefix>[!Formal definition]> A property P is a safety property if given any execution E such that P(trace(E)) = false, there exists a prefix of E, s.t. every extension of that prefix gives an execution F s.t. P(trace(F))=false"
Distributed Abstractions.md,Liveliness,"Properties that state that something good eventually happens. It can only be:- satisfied in finite time (when good happens)- violated in infinite time (there is always hope)>[!Formal definition]>A property P is a liveness property if given any prefix F of an execution E, there exists an extension of trace(F) for which P is true"
Distributed Abstractions.md,Model/Assumptions,
Distributed Abstractions.md,Failure assumptions,Processes that do not fail in an execution are **correct**.
Distributed Abstractions.md,Crash stop failure,Process is not correct if it stops taking steps like sending and receiving messages.
Distributed Abstractions.md,Omission failure,Process is not correct if it omits sending or receiving messages- Send omission: not sending messages according to algorithm- Receive omission: not receiving messages that have been sent to the process
Distributed Abstractions.md,Crash recovery,"Process is not correct if it crashes and- never recover, or- recovers an infinite number of timesMay recover after crashing with a special recovery event automatically generated"
Distributed Abstractions.md,Byzantine,"Process behaves arbitrarily such as sending messages not in its algorithm, or behave maliciously attacking the system.Model B is a special case of model A if a process that works correctly under A, also works correctly under B.- Crash is a special case of omission where all messages are omitted.- Omission is a special case of crash-recovery, as it recovers but does not restore state- Omission == Crash-recovery: where access to volatile memory means some messages after a crash are omitted as it cannot be restored"
Distributed Abstractions.md,Quorums,A quorum is any set of majority processes (i.e. $\lfloor N/2\rfloor+1$)- Two quorums always intersect in at least 1 process- There is at least 1 quorum with only correct processes- There is at least 1 correct process in each quorum
Distributed Abstractions.md,Channel Failure Modes,
Distributed Abstractions.md,Fair loss links,Channels delivers any message sent with non-zero probability (no network partitions)
Distributed Abstractions.md,Stubborn links,Channels delivers any message sent infinitely many timesWe can implement stubborn links using fair loss links- sender stores every message it sends in *sent*- periodically resends all messages in *sent*
Distributed Abstractions.md,Perfect Links,Channels that deliver any message sent exactly once.
Distributed Abstractions.md,Timing assumptions,Processes: bounds on time to make a computation stepNetwork: Bounds on time to transmit a message between a sender and a receiverClocks: Lower and upper bounds on clock rate-drift and clock skew w.r.t. real timeAsynchronous systems: no timing assumption on process and channelsPartially synchronous systems: eventually every execution will exhibit synchronySynchronous systems: build on solid timed operations and clocks
Distributed Abstractions.md,Causality,"In the asynchronous model, we can only reason about the order of events by observing which events may cause other events."
Distributed Abstractions.md,Computation Theorem and equivalence,A permutation of the same collection events whilst preserving causal order are said to be equivalent.
Distributed Abstractions.md,Logical Clocks,"A logical clock is an algorithm that assigns a timestamp to each event occurring in adistributed system.$$if  \ a\rightarrow b, t(a)<t(b)$$"
Distributed Abstractions.md,Lamport clocks:,- Note that this does not mean that $t(a)<t(b) \implies a\rightarrow b$. Lesser timestamps does not necessarily mean they are causally relatedWe need to distinguish the total order of events for same timestamps across different processes.
Distributed Abstractions.md,Vector clocks,"We want to tell the causal relation using the timestamps.$$\begin{align}v(a) < v(b), then\ a\rightarrow_\beta b \\if\ a\rightarrow_\beta b,v(a)<v(b)\end{align}$$Limitations- Vectors need to be defined of size n- cannot provide total event ordering"
Distributed Abstractions.md,Similarity,"If two executions F and E have the same collection of events, and their causal order is preserved, F and E are said to be similar executions, written `F~E`"
Distributed Abstractions.md,[[Failure Detectors]],
Disk.md,Disk Mechanics,A disk is made up of multiple cylinders (platters) each with a set of tracks> [!NOTE]> Each platter consists of 2 surfaces which data can be read/writtenDisk capacity calculation:
Disk.md,Disk Access,> [!IMPORTANT]> Data can only be accessed in units of blocks. Each block must be loaded from the disk into main memory. Only in main memory can we individually address each word.
Disk.md,Seek time,"Seek time depends on the total number of cylinders. However, it is not linear as the time taken is also dependent on the acceleration of the head."
Disk.md,Rotational Delay,$$t = \frac{Angle}{Rotation \ Speed}$$On average the rotational delay is 0.5 * t
Disk.md,Transfer Time,$$t = \frac{block\ size}{transfer\ rate}$$
Disk.md,Random Disk Access,"Average seek time: let i be the cylinder of the block just accessed and j be the cylinder of the block to be accessed, N be the total number of cylinders$$t = \frac{\sum_{i=1}^{N}\sum_{j=1}^{N}seektime(i-j)}{N^2}$$"
Disk.md,Sequential Disk Access,Average seek time is approximately 0 as the block to be accessed is likely to be in same cylinderAverage rotational delay is approximately 0 as the head points to the next block after current access
Disk.md,Disk Scheduling,
Disk.md,First Come First Serve,
Disk.md,Shortest Seek Time First,Similar to [](Notes/Process%20scheduling.md#Shortest%20Job%20First%20(SJF)%7Cshortest%20job%20first). Selects the request with the minimum seek timefrom the current head position. It is susceptible to starvation.
Disk.md,Elevator / Scan,"Disk arm starts at one end of the disk, and moves toward the other end, servicing requests until it gets to the other end of the disk, where the head movement is reversed:"
Disk.md,C-Scan,"Variant of elevator: after reversing direction, may not need to service requests immediately as more requests would be on the other end (uniform distribution)"
Disk.md,C-Look,"Rather than reversing only when reaching one end of the disk, reverse after servicing the last request in the current direction."
Disk.md,Comparison,- SSTF is common and has a natural appeal- SCAN and C-SCAN (or LOOK and C-LOOK) perform better for systems that place a heavy load on the disk (since starvation is unlikely)- Performance depends on the number and types of requests- [](Notes/File%20Systems.md#Storage%20allocation%7CFile%20allocation%20methods) also affect the effectiveness of the algorithm. A linked or indexed file may generate requests wide apart.- All the discussed algorithms (except for FCFS) do not solve the underlying issue of starvation. e.g. SCAN can be prevented from servicing the requests on the other end if new requests keep arriving at the same place.
Disk.md,Disk Management,Formatting- Divide the disk into sectors which the controller can read and writePartitioning- separating the disk into 1 or more groups of cylindersLogical formatting- Making a new file system by creating data structures to support file access across different partitions
Disk.md,Disk Reliability,
Disk.md,Striping,"Uses a group of disks as one storage unit- Each block is broken into several sub-blocks, with one sub-block stored on each disk- Time to transfer a block into memory is faster because all sub-blocks are transferred in parallel"
Disk.md,Mirroring,Keeps a duplicate of each disk by using 2 physical disks in 1 logical disk. If one fails data can still be read by the other.
Disk.md,Redundant Array of Independent Disks (RAID),"Raid 0: StripingRaid 1: MirroringRaid 0 + 1: Mirror of StripesRaid 1 + 0: Strip of Mirror- A difference occurs if there are at least 6 disks involved.- Raid 10 has better fault tolerance: allows for disk 1,3,5 to be down and still functional- Raid 01 degrades to Raid 0 when any disk fails. i.e. Will only be able to read a file from one group."
Disk.md,Storing relational data,
Disk.md,Fields to Record,
Disk.md,Record to Block,There a a few considerations when storing a record into a block
Disk.md,Supporting record separation,
Disk.md,Order of records,We can store records in the order of the primary key. Order can be maintained either physically (in memory) or logically (through a pointer)
Disk.md,Practice Problems,"a. 10 + 35 + 20 + 18 + 25 + 3 = 111b. Order: 11->12->9->16->1->34->361+3+7+15+33+2 = 61c. Order: 11->12->16->34->36->9->11+4+18+2+27+8 = 60Total bytes per tuple = 8+17+1+4+4+4+1 = 39Block contains meta data of 40bytesa.Total byte without block meta data: $8\times 1024-40=8152$Records: $8152\div 39=209.03$209 records can be storedb.Total bytes per tuple:17 byte character string needs to pad additional 3 bytes: 20 byte1 byte needs to pad additional 3 bytes: 4 byte$8+20+4+4+4+4+4=48$ bytesRecords: $8152\div 48 = 169$169 Recordsc.Total bytes for block header:$10\times 8 = 80$Total bytes per tuple:17 byte character string needs to pad additional 7 bytes: 24 byte1 byte needs to pad additional 7 bytes: 8 byteRecord header: $2\times 8 + 8 = 24$$24+8+24+8+8=72$ bytesRecords: $8192-80\div 72 = 112$112 Recordsa. A “sector” is a physical unit of the disk and a “block” is a logical unit, a creation of whatever software system – operation systems or database systems, for example – is using the disk. As we mentioned, it is typical today for blocks to be at least as large as sectors and to consist of one or more sectors. However, there is no reason why a block cannot be a fraction of a sector, with several blocks packed into one sector. In fact, some older systems did use this strategy.With the block size increases, the # of blocks to be accessed for a relational table decreases but the transfer time then increases.b. One block consists of multiple sectors. If these sectors are not sequential, the transfer time will be directly proportional to the RPM which the seek head is able to reach each sector.a.Capacity: $8\times2^{13}\times2^8\times2^9=2^{33}$bytes = 8GBb.1 round around the track is 256 sectors and 256 gaps, can be completed in $\frac{1}{3840}min$ or 1/64 secondsTo navigate 1 sector and 1 gap: $\frac{1}{64\times256}=0.061ms$Min time to 1 sector and 0 gap: $0.061\times0.9=0.0549ms$Min time for 8 sector and 7 gaps: 0.482msMax rotational delay occurs when we need to traverse 256-8 sectors and gaps to find the blockMax cylinder access occurs when we need to traverse all the tracksMax time: $0.061\times248+0.482+17.4=33.01ms$c.There are 8192 cylinders.If access is on cylinder 1000: block access time = average rotational delayIf access is on cylinder 1001: block access time = 1 track seek time + avg rotational delayAverage cylinder access: (1000+999...+0+1+...+7192)/8192 = 3218Average cylinder access time: $3218/500+1=7.44ms$Average  rotational delay: $\frac{1}{64\times2}=7.8ms$Average total block access time: 15.24ms"
Direct Memory Access.md,Modes,
Direct Memory Access.md,Burst,DMA controller transfers multiple units of data before returning control.- Fast data transfer rate- CPU inactivity for longer periods of time as it needs to wait for a long time for control of the data bus
Direct Memory Access.md,Cycle stealing,"Release data bus after transferring 1 unit of data. Executes between CPU instructions and pipeline stages.- Slow transfer rate- CPU inactive time is very short, making it favourable for applications which need to be responsive"
Direct Memory Access.md,Transparent,Transfer data only when CPU is not using the data bus- Potentially slowest transfer rate as CPU could always be using the data bus- CPU basically has no inactive time as transfer only done when it is not using data bus- Complex to detect when CPU is not using the data bus
Dimensionality Reduction.md,Principle Component Analysis,"A method for dimensionality reduction, by finding the variables which most account for the variance in the data."
Dimensionality Reduction.md,A 2D example,"Plot the data, with each axis being one of the variables.Center the plot around the origin. Find the best fitting line which passes through the data. The best fit is that which minimizes the sum of the distances from data to line. By the Pythagoras theorem, it is also the one which maximises the distance from origin to projected data.The best fit line is Principle Component 1 (PC1).Since this is a 2d example, PC2 is now simply the line perpendicular to PC1. Why? Idk...We can then remove variables which account for less of the variation in the data."
Dimensionality Reduction.md,Linear Discriminant Analysis,"LDA is a method of dimensionality reduction, by finding a new axis (or set of axes) which maximizes the separation amongst the categories in data.When we have n-dimensions of data, LDA allows us to find the axes which can separate the categories the best."
Dimensionality Reduction.md,Subspace Methods,"Samples in the same class are similar to each other. We can think of them as localized in a subspace spanned by a set of basis vectors. If we project the new test data onto this subspace, we can find the similarity of it to the class.One method to find similarity is to choose the subspace which maximizes the projection length:"
Dijkstra's Algorithm.md,Data Structures Needed,
Dijkstra's Algorithm.md,Pseudocode,We can also obtain the number of distinct shortest paths by using an additional n-size array to store the counts of paths which have the same shortest distance:
Dijkstra's Algorithm.md,Proof of Correctness,Why the greedy choice is optimal:> [!important]> This step is the reason for why graphs with negative weights do not ensure correctness of Dijkstra's .
Dijkstra's Algorithm.md,Examples,Manually computing shortest path:
Depth First Search.md,Graph Traversal,_Assuming ties are handled in alphabetical order_Expansion Order:A > B > C > E > F > GFinal Path:A > B > C > E > F > G
Depth First Search.md,Pseudocode,"A recursive implementation of DFS:**procedure** DFS(_G_, _v_) **is**label _v_ as discovered**for all** directed edges from _v_ to _w that are_ **in** _G_.adjacentEdges(_v_) **do****if** vertex _w_ is not labeled as discovered **then**recursively call DFS(_G_, _w_)A non-recursive implementation of DFS with worst-case space complexity O(E)**procedure** DFS_iterative(_G_, _v_) **is**let _S_ be a stack_S_.push(_v_)**while** _S_ is not empty **do**_v_ = _S_.pop()**if** _v_ is not labeled as discovered **then**label _v_ as discovered**for all** edges from _v_ to _w_ **in** _G_.adjacentEdges(_v_) **do**_S_.push(_w_)"
Dependency Injection.md,Problems we want to solve,"1. How can a class be independent from the creation of the objects it depends on?2. How can an application, and the objects it uses support different configurations?3. How can the behaviour of a piece of code be changed without editing it directly?"
Dependency Injection.md,General Idea,"An object receives other objects that it depends on. A form of inversion of control, dependency injection aims to separate the concerns of constructing objects and using them, leading to loosely coupled programs."
Dependency Injection.md,Constructor injection,"The most common form of dependency injection is for a class to request its dependencies through its constructor. This ensures the client is always in a valid state, since it cannot be instantiated without its necessary dependencies.```java// This class accepts a service in its constructor.Client(Service service) {// The client can verify its dependencies are valid before allowing construction.if (service == null) {throw new InvalidParameterException(""service must not be null"");}// Clients typically save a reference so other methods in the class can access it.this.service = service;}```"
Dependency Injection.md,Pros,1.
Dependency Injection.md,Cons,1.
Default Logic.md,Definitions,
Default Logic.md,Reiter Extension,
Default Logic.md,Makinson Approach,
Default Logic.md,Process Tree Algorithm,"A __closed__ default is one that has been instantiatedThe In-set contains all the consequences from applying a defaultThe Out-set contains all the negations of the justifications from applying a default: these are the predicates which cannot be proven true by the In-Set for the extension to be consistent1. Start with the root node: Out is initialized to $\emptyset$ while In is set to the current knowledge base2. For every node, check for direct applicability of defaults (If no defaults are directly applicable: __we arrived at a closed process)__ direct applicability must satisfy 2 conditions:1. Default must be triggered: In-set contains the prerequisite2. Default must be justified: negation of justifications cannot be proven True from the current In-set4. If the new In-set becomes inconsistent ($In \cap Out \neq \emptyset$ or $In\cup \delta .consq \ \vdash Out$) : __process is unsuccessful____We arrive at an extension for every closed and successful process__."
Decision Trees.md,Growing the Tree,1. Choose the best question (measured base on information gain) and split the input data into subsets2. Terminate when a unique class label is formed (no need for further questions)3. Grow by recursively extending other branches
Decision Trees.md,Entropy (measuring information gain),- [Entropy](Notes/Information%20Theory.md#Entropy)- [Gini Impurity](Notes/Information%20Theory.md#Gini%20Impurity)
Decision Trees.md,Choosing attributes,
Decision Trees.md,Avoid overfitting,"- Stop growing when data split not statistically significant- Grow full tree, then post-prune (e.g. Reduced error pruning)"
Deadlocks.md,Modelling Deadlocks,
Deadlocks.md,Cyclic Properties of Deadlocks,"> [!Having a cycle in the graph is only a necessary condition but *not a sufficient condition* for a deadlock.]If each resource only has 1 instance, a cycle implies a deadlock."
Deadlocks.md,Deadlock Conditions,"A deadlock **may** occur if these conditions hold at the same time:1. Mutual exclusion: Only one process at a time can use a resource instance2. Hold and wait: A process holding at least one resource is waiting to acquire additional resources held by other processes3. No preemption: A resource can be released only voluntarily by the process holding it, after that process has completed its task4. Circular wait: There exists a set {P0, P1, …, Pn} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for P2, …, Pn–1 is waiting for Pn, and Pn is waiting for P0"
Deadlocks.md,Deadlock Prevention,"As long as we are able to ensure at least one of the following conditions do not hold, we can prevent a deadlock from occurring.Example using [](Notes/Process%20Synchronization.md#Dining%20Philosophers%7CDining%20Philosophers%20Problem):Each process must request for the lower numbered resource first before able to request for the higher numbered resource. This breaks the circular wait as process requests are increasing in their order (no cycle):"
Deadlocks.md,Deadlock Avoidance,"Rather than prevent deadlocks as they are about to occur, we can avoid entering into a state where deadlocks are possible. This state is called the *unsafe state*.> [! Safe state]>  if there exists a safe completion sequence of all processes without deadlockA process completion sequence is safe, if for each $P_i$ , the resources that it requests can be satisfied by currently available resources + resources held by all the $Pj , j< i$Algorithm:1. When a process request for resource, determine if allocation leaves the system in a safe state2. If safe: grant the request3. Else: wait"
Deadlocks.md,Banker's Algorithm,Checking whether the satisfaction of a request will lead to an unsafe stateNecessary assumptions:1. Each process must declare the maximum instances of each resource type that it needs2. When a process gets all its resources it must return them in a finite amount of timeWe need to keep track of some information:Let m be the number of resource types and n be number of processes- Available: `Available = [m]int` the number of instances of each resource currently available to be allocated- Max: `Max[n][m]` is number of resource a process can request. *Note*: the process completes once it reaches its max- Allocation: current allocated resources- Need: number of instances required to completeEach process can also make a request for resources:
Deadlocks.md,Deadlock Detection,"Allow the system to enter deadlock state, invoke detection and recovery algorithms."
Deadlocks.md,Practice Problems,"a. False. If there are only 4 people, the circular wait condition is brokenb. True. A single process will not be in a deadlock as there are no other processes which it is sharing resources with. *Hold and wait condition can never be satisfied.*c. False. Not all cycles indicate a deadlock. Depends on the number of resource instancesa. Available -> 1. P4 allocation = 5. No process can be satisfied with available 1. Unsafe stateb. Safe state. Completion order: P3, P4, P2, P1x = 0| Process | Allocation | Need  | Available | Completed    || ------- | ---------- | ----- | --------- | ------------ || P0      | 2 1 1      | 0 1 0 | 0 1 0     | P0 Completed || P1      | 1 1 0      | 2 1 2 |  2 2 1    | P2 Completed || P2      | 1 1 1      | 2 0 1 |  3 3 2    | P1 Completed || P3      | 1 1 1      | 4 1 0 | 4 4 2     | P3 Completed             |"
Data Level Parallelism.md,Processors,Different types of hardware can support different levels of data parallelism.
Data Level Parallelism.md,Flynn's Processor Taxonomy,Advantages of SIMD > MIMD- Allow sequential thinking yet achieves parallel speedup- Reduced energy usage- More efficient parallel efficiency
Data Level Parallelism.md,Single Instruction Multiple Data (SIMD),
Data Level Parallelism.md,Vector processor,"One vector instruction can perform N computations, where N is the vector length.- Reduces the number of instructions: less branching- Less execution time with lower instruction count- Simpler design as there is no requirement for data dependency check since each execution is independent"
Data Level Parallelism.md,Array processor,"- Array processor works more like a true parallel system, with each processor able to run same instruction on different data- Vector processor works in more of a pipelined fashion."
Data Level Parallelism.md,Multimedia Extensions (MMX),
Custom Computing.md,General Purpose Processors (GPP),
Custom Computing.md,Application Specific Instruction Set Processor (ASIP),Flexibility should just be sufficient instead of unlimited in the case of a GPP. Want to achieve highest performance with minimum power consumption.
Custom Computing.md,Digital Signal Processor,Architecture designed for repetitive multiply-accumulate operations and bit-reversal addressing
Custom Computing.md,GPU,
Custom Computing.md,Field Programmable Gate Array (FGPA),"The FGPA has an edge over the DSP by supporting parallel designs and greater performance. However, it is more expensive and takes longer time to manufacture."
Custom Computing.md,Application Specific Integrated Circuits (ASIC),"With more customization, the ASIC can achieve lower power consumption but results in inflexibility."
Custom Computing.md,Heterogenous Computing Systems,
Custom Computing.md,Examples,
Conventional Indexes.md,Updating Indexes,1. Locate the targeted record or the place to hold new record2. Update data file3. Update index
Conventional Indexes.md,Clustered and Non-Clustered Indexes,Clustering index: indexes on an attribute is such that all the tuples with a fixed value for the search key of this index appear on as few blocks as can hold them.If a relation is clustered (it must be sorted and packed together according to some attribute a) another index on another attribute _b_ would likely be non-clustered unless _a_ and _b_ are highly correlated.
Conventional Indexes.md,Comparisons,
Conventional Indexes.md,Read,A range read of keys that are close together will result in high number of I/O:
Conventional Indexes.md,Update,Clustered indexes will not be as good if the database goes through many update operations.
Conventional Indexes.md,Multi-layer Index,
Conventional Indexes.md,Practice Problems,"a.Dense: We need 300 key pointer pairs. Each block can hold 10 pairs. Total blocks = 300 / 10 = 30Sparse: 1 index pointer can point to a block of 3 records. Each block can hold 10 pointers. 1 index block represents 30 records. $300/30=10$ blocks neededb.Worst case: retrieve the last record -> 10 I/Oc.Another sparse index to point to a block of sparse indexSince the initial sparse index needs 10 blocks to represent, the second level index can use 1 block (10 pointers) to fully represent it.I/O for 2nd level: 1I/O for 1st level: 1I/O to read record: 1Total 3 I/Oa.Best case when inserting a record in the not full block with record 9. Insert 101 I/O to read the index block, 1 I/O to load the block with record 9. Total 2 I/Ob.Worst case when inserting into first data block. Insert 0.1 I/O to read index block. Need to load every data block to shift records down. Total 1+4=5 I/O."
Constraint Satisfaction Problem.md,Constraint Propagation,"Propagate the implications of constraints from assigning 1 variable to the other variables.Useful to optimize the __order of variable assignments__. This has the effect of making inconsistent assignments to **fail earlier in the search**, which enables more efficient pruning. This means that it may lead to more dead ends than:Useful to optimize the __order of value assignments__. This prevents deadlocks and reduces backtracking by choosing the values which are most likely to work."
Constraint Satisfaction Problem.md,Example Problems,Cryptarithmetic Puzzle
Consensus.md,Paxos Algorithm,An [Eventual Leader Election](Notes/Failure%20Detectors.md#Eventual%20Leader%20Election) (weakest leader elector we can use) can be used to eventually elect 1 single proposer *(providing termination)*.- Proposers: attempt to impose their proposal to acceptors- Acceptors: may accept values issued by proposers (cannot talk to each other)- Learners: decide depending on what is acceptedContention problem: several processes might initially be proposers
Consensus.md,Abortable Consensus,Algorithm aborts if there is contention of multiple proposers.
Consensus.md,Version 1 (Centralised),"Proposer sends value to a central acceptor. Acceptor decides on the first value which it gets.Problem 1: if this acceptor fails, we will never know of the decision"
Consensus.md,Version 2 (Decentralised),"Proposers talk to a set of acceptors, use a majority [quorum](Notes/Distributed%20Abstractions.md#Quorums) to choose a value and enable fault tolerance.Problem 2: acceptor accepts the first proposal, if messages arrive out of order, possible to have no majority"
Consensus.md,Version 3 (Enable restarts),"If no majority value, we need to restart until there is one.Since proposers can propose again, we need a way to differentiate between them.- Use a ballot number: sequence number in the form $i, n+i, 2n+i$ for a process $i$ and $n$ processesProblem 3: restarts lead to different majority accepted values across time, learners cannot make a single decision"
Consensus.md,Version 4 (Prepare and Accept),"We need a way to ensure that every higher number proposal results in the same chosen value- Satisfied by ensuring acceptors only accept this value- Satisfied by ensuring proposers only propose this value- Proposers need to learn this value from the highest sequence number of those accepted. Proposers need to ensure that this ""highest value"" does not change.Proposers query acceptors so that if a value is accepted, every higher proposal issued has the same value previously accepted1. Proposer $prepare(n)$:- Gets a promise from acceptors not to accept a proposal with lower ballot number n- Acceptor also responds with the value corresponding to the highest ballot number proposal2. Proposer $accept(n,v)$:- Pick the value from the maximum proposal number returned. If none of the processes return a value, proposer can pick freely.- Acceptor $accept(n,v)$ if not accepted any $prepare(m)$ such that $m>n$; else $reject$1. Proposer $decide(v)$ if majority acks; else $abort$"
Consensus.md,Optimisations,"- Reject `prepare(n)` if accepted `prepare(m); m > n`: Reject a lower prepare- Reject `accept(n,v)` if answered `accept(m,u); m > n`: Reject a lower accept- Reject `prepare(n)` if answered `accept(m,u); m > n` : Reject a lower accept- Ignore messages if majority obtained"
Consensus.md,Multi Paxos,"The motivation: replicated state machines need to agree on a sequence of commands to execute.Approach: organise the algorithm into rounds. In each round, each server starts a new instance of Paxos. They propose (2 RTT), accept (2 RTT) and decide on 1 command, add that to the log and restart.Initial states- $ProCmds = \emptyset$: stores the list of commands proposed- Log = <>: a log of decided commandsA process which wants to execute a command C triggers $rb-broadcast<C, Pid>$. On delivery, the command pair is added to `ProCmds` unless it is already in Log.Problem: the same command across multiple processes might be decided in different slots in time."
Consensus.md,Sequence Consensus,"Rather than agreeing on a single command and storing that in a Log, we can directly try to agree on the sequence of commands.- Validity: if process p decides on a value, the value is a sequence of commands- Uniform Agreement: if process p decides u and another decides v, then *one is a prefix of the other*- Integrity: process can later decide another value, but the *previous value is a strict prefix of the newly decided value*- TerminationAfter adopting a value with highest proposal number, the proposer is allowed to extend the sequence with the new command.Problem: in the prepare phase, processes send a lot of redundant state as the full log is transferred between the proposer and acceptor leading to high IO. No pipelining as well since each round must begin with the prepare phase."
Consensus.md,Log Synchronisation,"Modify the prepare phase and shared states such that we can work on a single synchronised log $v_a$. *To do this, let 1 process act as the sole leader (proposer) until it is aborted by an election of higher ballot number*"
Consensus.md,Prepare Phase,"The leader sends `Prepare`:- current round: $n$- accepted round: $n_a$- log length: $|v_a|$- decided index $l_d$, where the decided sequence is $prefix(v_a,l_d)$The followers reply with `Promise`:- their accepted round- the log entries which the leader is missing and the leader appends those to the log. `AcceptSync` is used to synchronise the new log.*Promised followers and leader now have the same common log prefix*"
Consensus.md,Accept Phase,"The leader sends `Accept` command with highest $n$ to all promised followersThe followers reply with `Accepted`When majority accepted, `Decide` is sent.Any late `Promise` is replied with an `AcceptSync`"
Consensus.md,Partial Connectivity (enabling quorum connectedness),"Chained scenario:When one server loses connectivity to the leader, it will try to elect itself as a leader. Livelock situation as servers compete to become the leader. Can be solved if A becomes the leader but can't because it is already connected to a leader.Quorum Loss:When the leader loses quorum connectivity, deadlock situation as a majority cannot be obtained to make progress. B, D, E cannot elect a leader without a quorum. A is quorum connected but cannot elect a new leader since it is still connected to the alive leader C.Constrained Election:Leader is fully disconnected. A can become the new leader but will not be elected as it does not have the most updated log (log length)."
Consensus.md,Failure recovery,"1. Recover state from persistent storage2. Send a `PrepareReq` to all peers- If elected as leader, synchronise through a `Prepare` phase- `Prepare` phase from another leader will synchronise"
Consensus.md,Reconfiguration,"Supporting a way to add/replace any process part of the replicated state machine.A configuration $c_i$ is defined by a set of process ids $\{p1, p2, p3\}$ and the new configuration can be any new set of processes e.g. $\{p1,p2,p4\}$"
Consensus.md,Stop Sign,"To safely stop the current configuration, we must prevent new decisions in the old configuration (""split-brain"" problem) using a stop sign:The stop sign contains information about the new configuration to help processes reconfigure:- the new set of processes in $c_{i+1}$- the new configuration id number- the identifiers for each replica in the new configurationEach process on viewing the stop sign, can safely shut down and restart in the new configurationA new process not previously part of $c_i$ must perform log migration to catch up with the new instance. Log migration can be done with snapshots of the latest state."
Consensus.md,Raft,A state of the art consensus algorithm.
Consensus.md,State of servers,"Rather than using process ids to break ties for leader election in omnipaxos, Raft uses a form of random retrying when there are split votes."
Consensus.md,Log Reconciliation,"A server must have the most up to date log in order to become a leader, compared to omni-paxos where any server can be the leader and be synced up during the Prepare phase."
Concurrency Control.md,Scheduler,A schedule is a sequence of interleaved actions from all transactions.
Concurrency Control.md,Serial schedule,"A schedule is serial if its actions consists of all the actions of one transaction, then all the actions of another transaction, and so on."
Concurrency Control.md,Serializable schedule,"Result is equivalent to a serial schedule, but actions of one transaction do not have to occur before the actions of another."
Concurrency Control.md,Conflict Serializable Schedule,"If a pair of actions conflict, if their order is interchanged, the behavior of at least one of the transactions can change.From this we can draw 2 conclusions about when a pair can be swapped.1. Actions involve the same element2. At least one is a write"
Concurrency Control.md,Precedence Graph,"We can use a precedence graph to determine if a set of transactions are conflict serializable. An edge from one node to another represents a constraint on the order of the transactions. i.e. Actions in a transaction t1 cannot be swapped with another transaction t2. If a cycle occurs, the order of transactions become contradictory and no serial schedule can exist."
Concurrency Control.md,Recoverable Schedule,"A schedule is recoverable if transactions commit only after all transactions whose changes they read have committed.Else, the DBMS is unable to guarantee that transactions read data that will be the same as before the crash and after the crash."
Concurrency Control.md,Locks,Ensure that data items that are shared by conflicting operations are accessed one operation at a time. Same as with [Process Synchronization](Notes/Process%20Synchronization.md).
Concurrency Control.md,Two Phase Locking (2PL),Arbitrary assignment of locks do not lead to a serializable schedule. Two transactions can operate on elements in a different order resulting in different results. We can solve this by ensuring that transactions take up all lock actions before all unlock actions.
Concurrency Control.md,Why 2PL works?,"Intuitively, each two-phase-locked transaction may be thought to execute in  its entirety at the instant it issues its first unlock request. i.e. a legal schedule can only be such that the transaction completes fully, because otherwise, this means that another transaction is attempting to take a held lock, causing a deadlock. Hence, there is at least one conflict-serialisable schedule: the one in which the transactions appear in the same order as first unlocks.Suppose the schedule starts with T1 locking and reading _A_. If T2 locks _B_ before T1 reaches its unlocking phase, then there is a deadlock, and the schedule cannot complete. Thus, if T1 performs an action first, it must perform _all_ its actions before T2 performs any. Likewise, if T2 starts first, it must complete before T1 starts, or there is a deadlock. Thus, only the two serial schedules of these transactions are legal."
Concurrency Control.md,Lock mechanisms,
Concurrency Control.md,Shared and Exclusive locks,"- Shared lock: to allow for multiple transactions to perform `READ`- Exclusive lock: for `WRITE`A transaction should only ask for an exclusive lock when it is ready to write, so that any read operations can still continue. Upgrade the lock when needed:"
Concurrency Control.md,Update locks,"Deadlocks can occur when transactions are unable to upgrade their shared locks to exclusive ones, since there are already shared locks taken. A separate lock type that may be later upgraded to an exclusive lock is needed."
Concurrency Control.md,Compatibility matrix,
Concurrency Control.md,Workings of Scheduler,1. Part 1: Inserts appropriate lock actions ahead of all DB access operations and release the locks held by the Transaction when it aborts/commits2. Part 2: maintains a waiting list of transactions that need to acquire locks
Concurrency Control.md,Deadlock Detection & Prevention,
Concurrency Control.md,Timeout,"Place a limit on how long a transaction may be active, if it exceeds this time, it is forced to release its locks and other resources and roll back."
Concurrency Control.md,Waits-For Graph,Utilises the [cyclic properties of deadlocks](Notes/Deadlocks.md#Cyclic%20Properties%20of%20Deadlocks) to detect them.- Each transaction holding a lock or waiting for one is a node- An edge exists from T1 to T2 if there is some element A where:- T2 holds a lock on A- T1 is waiting for lock on A- T1 cannot get the lock on A unless T2 releases itThis graph can become very large ad analysing this graph for every action can take a long time.
Concurrency Control.md,Timestamps,Assign each transaction with a timestamp. This timestamp never changes for the transaction even if it is rolled back.
Concurrency Control.md,Wait-Die,
Concurrency Control.md,Wound-Wait,
Concurrency Control.md,Comparison,
Concurrency Control.md,Timestamp Ordering,"An optimistic approach. Use the timestamps of transactions to determine the serialisability of transactions.- Each transaction receives a unique timestamp TS(T).- If TS(Ti ) < TS(Tj ), then the DBMS must ensure that the execution schedule is equivalent to a serial schedule where Ti appears before Tj ."
Concurrency Control.md,Rules,"- A transaction wants to read, but the element has already been written to by another transaction- A transaction wants to write, but the element has already been read by another transaction or written by another transaction"
Concurrency Control.md,Thomas Write Rule,"When a transaction wants to write, and TS(T) < WT(X), ignore the write and allow the transaction to continue without aborting.- Timestamp ordering creates conflict serialisable schedules when this rule is not used- Schedules are not recoverable as TO does not have any checks"
Concurrency Control.md,Comparisons,
Concurrency Control.md,Multi Version Concurrency Control,"A misnomer as it is not actually a concurrency control protocol.DBMS maintains multiple physical versions of a single object in the database:- When transaction writes to an object, the DBMS creates a new version- When transaction reads an object, it reads the newest version that exists when the transaction startedEach user connected to the database sees a _snapshot_ of the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed, providing isolation"
Concurrency Control.md,Concurrency Protocol,Able to use any concurrency protocol to underly its implementation. MVCC is simply another layer on top of these protocols to provide transactional memory.
Concurrency Control.md,Version Storage,"DBMS creates a version chain per tuple, which allows it to find version that is visible to a particular transaction at runtime. There are indexes which point to the head of the chain.Version ordering:- Oldest to Newest: append new version at the end of chain -> the entire chain has to be traversed for look ups- Newest to Oldest: append new version at the head of the chain, update index pointers to the head for every new version"
Concurrency Control.md,Append only,
Concurrency Control.md,Time travel storage,
Concurrency Control.md,Delta storage,
Concurrency Control.md,Garbage Collection,
Concurrency Control.md,Practice Problems,"```mermaidgraph LR;T3 --> T2;T1 --> T2;T3 --> T1;```| Time | T1       | T2       | T3      || ---- | -------- | -------- | ------- || t1   |          |          | Read(A) || t2   |          |          | Read(B) || t3   | Read(A)  |          |         || t4   | Read(C)  |          |         || t5   | Write(A) |          |         || t6   |          | Read(C)  |         || t7   |          | Read(B)  |         || t8   |          | Write(C) |         || t9   |          | Write(B)         |         |In 2PL, all locks must be acquired by the transaction, operations are done and then all locks are released at once.This is schedule is not consistent with 2PL.T1 takes ul(B), xl(B), ul(D), xl(D). T2 reads and writes item B at step 6, this is not possible if T1 still has the exclusive lock on B. Hence, T1 must have released all locks by step 6. However, T1 still takes read and write actions on D at step 13,14.The minimal set of actions to remove:- 5- 6| Time | T1              | T2             || ---- | --------------- | -------------- || 1    | Read(Savings)   |                || 2    |                 | Read(Checking) || 3    | Write(Checking) |                || 4    |                 | Write(Savings)                || Time | T1    | T2   | R(X) | W(X) | R(Y) | W(Y) || ---- | ----- | ---- | ---- | ---- | ---- | ---- || 1    | r1(x) |      | 1    | 0    | 0    | 0    || 2    |       | r(x) | 2    | 0    | 0    | 0    || 3    |       | w(x) | 2    | 2    | 0    | 0    || 4    | r(y)  |      | 2    | 2    | 1    | 0    || 5    |       | r(y) | 2    | 2    | 2    | 0     |"
Computer Power.md,Power dissipation,
Computer Power.md,Dynamic Power,Dissipated only when computation is performed
Computer Power.md,Static Power,"Due to leakage current and dissipated whenever the system is powered onThus, it is possible that heat reducing solutions like a heat sink can help to reduce powe consumption."
Computer Power.md,Total Power,"When voltage is reduced, the threshold that is used to differentiate between a logic 1 and logic 0 output will be reduced. If this threshold is small, a high frequency will be more prone to noise that could alter the output."
Computer Power.md,Reducing power consumption,"1. Component design2. Power gating: shutting down unused components3. Clock gating: reduce unnecessary switching4. Reduce data movement, number of memory access and register transfer"
Computer Power.md,The problem between power and energy,
Computer Power.md,Practice Problems,__Case 1__:Change in voltage = 3.3 - 3 / 3 = 10%i.New frequency = $1.1 * 300 = 330 MHz$Change in dynamic power = $\frac{3.3^2 \times 330 }{3^2*300} - 1 = 33.1\%$ii. Change in static power = 10%iii. Perf is directly proportional to freq. Change in perf = 10%iv.$$\begin{aligned}&\text{Perf is = 1/Time} \\&\text{Dynamic energy} \\&\text{Increase in performance by 10\% means that the change in time is 1/1.1}\\&\text{Consumption change} = 1.331 P * 1/1.1T - P*T = 21\%increase\\&\text{Static energy:}\\&1.1P * 1/1.1T - 1 = 0\%\end{aligned}$$__Case 2__:i. $\frac{3.3^2 - 3^2}{3^2} = 21\%$ii. 10% increaseiii. No change in frequency so no change in performanceiv.Dynamic energy consumption: 21% increaseStatic energy consumption: 10% increase
Computer Performance.md,Execution time,One indicator of performance is __execution time__ of a program$$\text{Performance} = \frac{1}{\text{Execution Time}} = \frac{1}{time_{end} - time_{start}}$$$$\text{Execution Time} = \text{Instruction Count} \times \text{Clocks Per Instruction} \times \text{Clock Period}$$
Computer Performance.md,Instruction Count,
Computer Performance.md,Clocks Per Instruction (CPI),$$\text{Average CPI}=\text{Cycle Count}/\text{Instruction Count}$$
Computer Performance.md,Clock Period,Clock period is the inverse of clock frequency__Memory wall problem__: a higher clock frequency may not result in better performance if the the time needed for memory access operations is slower than the CPU
Computer Performance.md,Speed-up,Speed-up is the factor over the original machine of improved performance$$\text{Speedup} = \frac{Perf_a}{Perf_b}$$We can define the execution time of an enhanced machine by the proportion of the program _E_ that is improved and _T_ the original time taken and _S_ the enhancement factor$$T' = (T\times (1-E))+\frac{T\times E}{S}$$
Computer Performance.md,Example,
Computer Performance.md,Amdahl's Law,"__If the program is of a fixed workload:__Let _E_ be the fraction of program that is enhanced via _parallelism_, with maximum enhancement factor $S = \infty$, the maximum speedup is $$\text{Max Speedup} =lim_{s\rightarrow \infty}\frac{1}{1-E+\frac{E}{S}}=\frac{1}{1-E}$$Let *E* be the fraction of program enhanced by Speedup $S_1$ and $1-E$ enhanced by Speedup $S_2$.$$Speedup=\frac{1}{\frac{1-E}{S_2}+\frac{E}{S_1}}$$"
Computer Performance.md,Gustafson's Law,__If the program is set to a fixed time period instead:__ do more parallel work in the same amount of time
Computer Performance.md,Other performance metrics,
Computer Performance.md,Practice Problems,$$\begin{align}&IC=200+500+300=1000\\&T_c=100ns\\&CPI_{average}=(200\times1+500\times2+300\times3)/1000=2.1\\&T_{execution}=1000\times100\times2.1=210ms\end{align}$$a.$$\begin{align}&10=200\times10^6\times\frac{1}{200\times10^6}\times CPI_{avg}\\&CPI_{avg}=10\\&5=160\times10^6\times\frac{1}{300\times10^6}\times CPI_{avg}\\&CPI_{avg}=9.375\end{align}$$b.$$\begin{align}&IC_a=4\div(\frac{10}{200\times10^6})=80\times10^6\\&IC_b=3\div(\frac{9.375}{300\times10^6})=96\times10^6\end{align}$$a.$$\begin{align}&T_{M2}=(1+2+3+4)\times\frac{1}{500\times10^6}\times4\\&T_{M3}=(2+2+4+4)\times\frac{1}{750\times10^6}\times4\\&\text{Speedup}=1.25\end{align}$$b.$$\begin{align}&T_{M1}=2\times\frac{1}{500\times10^6}\times1\\&T_{M2}=1\times\frac{1}{500\times10^6}\times1\\&T_{M3}=2\times\frac{1}{750\times10^6}\times1\\&\text{Speedup M2 over M1}=2&\text{Speedup M3 over M1}=1.5\end{align}$$
Complexity Analysis.md,Asymptotic Notations,Notations used to describe the order of growth of a given function
Complexity Analysis.md,Big-O $O(f(x))$,The limits when taking the 2 functions to infinity produces a constant C that$$\begin{align}\lim_{n\to \infty}\frac{f(n)}{g(n)}=C \\ C=0 \ or\ 0<C<\infty \end{align}$$
Complexity Analysis.md,Big-Omega $\Omega(f(x))$,The limits when taking the 2 functions to infinity produces a constant C that$$\begin{align}\lim_{n\to \infty}\frac{f(n)}{g(n)}=C \\ C=\infty \ or\ 0<C<\infty \end{align}$$
Complexity Analysis.md,Big-Theta $\theta(f(x))$,The limits when taking the 2 functions to infinity produces a constant C that$$\begin{align}\lim_{n\to \infty}\frac{f(n)}{g(n)}=C \\ 0<C<\infty \end{align}$$
Complexity Analysis.md,Properties,"$$f(n)\in O(h(n)),g(n)\in O(h(n)) \implies f(n)+g(n)\in O(h(n))$$"
Complexity Analysis.md,Example function comparisons,
Complexity Analysis.md,Order of Common Functions,
Complexity Analysis.md,Specific Complexities,- [Polynomial Time Complexity](Notes/Polynomial%20Time%20Complexity.md)- [Pseudo-Polynomial Time Complexity](Notes/Pseudo-Polynomial%20Time%20Complexity.md)
Combinational Circuits.md,Multiplexer,A multiplexer is used to select 1 out of n inputs.
Combinational Circuits.md,Decoder,A decoder is used to select a 1-hot output based on an n bit input.
Clustering.md,K-means clustering,- Using the Euclidean distance as the minimising function causes it to favour spherical clusters which may not be the case in linear separated data points:
Clustering.md,Expectation Maximisation,"Rather than minimizing the Euclidean distances of the data points to the cluster, try to maximize the probability of the data point being generated from the cluster."
Clustering.md,Step 1,Introduce a hidden variable $h$ for each desired cluster and initialize the starting parameters for each cluster distribution
Clustering.md,Step 2,
Clustering.md,Step 3,Update the values of the model parameters iteratively:
Clustering.md,Comparison to K-means,
Clock (or Second Chance) Policy.md,Idea,"1. Keep a circular list of items in memory2. A ""clock hand"" is used to suggest the next item for eviction3. It initially points to the oldest page4. Maintain a _use-bit_ for each item: this will tell us if the item has been accessed recently. Initially, all items have use-bit = 0.5. Each time an item is accessed, change the use-bit to 16. When choosing item to be evicted:1. If the item has use-bit = 1, we reset it back to 0 (this item has used its second chance)2. Else we evict it (no second chance, replace it)7. When bringing in the item (for our case):1. We do not set the use bit for the incoming page2. **We advance the clock hand to the next item after bringing in the page**At the worst case, the algorithm behaves the same as FIFO. E.g. when all the pages have their used bit set to 1, eventually the clock hand will return back to the oldest page and replace."
Clean Code.md,Comments,
Clean Code.md,Explain yourself in code,```go//check to see if employee is elgiible for full benefitsif (employee.flags & HOURLY_FLAG) && employee.age > 65```Create a function to describe the comment:```goif (employee.isEligibleForFullBenefits())```
Class Diagrams.md,Basic Notation,
Class Diagrams.md,Visibility Modifiers,\+ : public\- : private\# : protected~ : package private
Class Diagrams.md,Associations,
Class Diagrams.md,Stereotypes,"> [!NOTE] Heuristics for identifying entity objects> - Terms that developers or users need to clarify in order to understand the use case • Recurring nouns in the use cases (e.g., Incident)> - Real-world entities that the system needs to track (e.g., FieldOfficer, Dispatcher, Resource)> - Real-world activities that the system needs to track (e.g., EmergencyOperationsPlan)> - Data sources or sinks (e.g., Printer).> [!NOTE] Heuristics for identifying boundary objects> - Identify user interface controls that the user needs to initiate the use case (e.g., ReportEmergencyButton).> - Identify forms the users needs to enter data into the system (e.g., EmergencyReportForm).> - Identify notices and messages the system uses to respond to the user (e.g., AcknowledgmentNotice).> - When multiple actors are involved in a use case, identify actor terminals (e.g., DispatcherStation) to refer to the user interface under consideration.> [!NOTE] Heuristics for identifying control objects>- Identify one control object per use case.>- Identify one control object per actor in the use case.>- The life span of a control object should cover the extent of the use case or the extent of a user session. If it is difficult to identify the beginning and the end of a control object activation, the corresponding use case probably does not have well-defined entry and exit conditions."
Chain Matrix Multiplication.md,Problem Formulation,"Satisfaction of the Principle of Optimality:Let $A_i$ represent the $i^{th}$ matrix with dimensions $(d_{i-1}\times d_i)$.The optimal way to multiply matrices $A_i \ to\ A_j$  can be broken down into the optimal way to multiply the matrices $A_i \ to\ A_k + A_{k+1} \ to\ A_j$ (for some k) + the cost to multiply the final 2 matrices = $d_i \times d_k \times d_j$Define $OptCost(i,j)$ to be the optimal cost of multiplying matrices with dimensions $d_i, d_{i+1},...d_j$Base case:$$OptCost(i,j) = 0 \qquad j-i=1$$Recursive equation can be formed as follows: loop through possible k to find min$$OptCost(i,j) = \min_{i+1\le k\le j-1}(OptCost(i,k)+OptCost(k,j)+d_i\times d_k\times d_j) $$"
Chain Matrix Multiplication.md,Strategy,Store the solutions to subproblems in _cost 2d array_.$Cost[i][j]$ represents the optimal cost of multiplying matrices $A_{i+1} \to A_j$Store the optimal values of k (index to split the matrix multiplication) in _last 2d array_.
Chain Matrix Multiplication.md,Pseudocode,"``` java// Matrix A[i] has dimension dims[i-1] x dims[i] for i = 1..nMatrixChainOrder(int dims[]){// length[dims] = n + 1n = dims.length - 1;// m[i,j] = Minimum number of scalar multiplications (i.e., cost)// needed to compute the matrix A[i]A[i+1]...A[j] = A[i..j]// The cost is zero when multiplying one matrixfor (i = 1; i <= n; i++)m[i, i] = 0;for (len = 2; len <= n; len++) { // Subsequence lengthsfor (i = 1; i <= n - len + 1; i++) {j = i + len - 1;m[i, j] = MAXINT;for (k = i; k <= j - 1; k++) {cost = m[i, k] + m[k+1, j] + dims[i-1]*dims[k]*dims[j];if (cost < m[i, j]) {m[i, j] = cost;s[i, j] = k; // Index of the subsequence split that achieved minimal cost}}}}}```"
Chain Matrix Multiplication.md,Exercises,"Suppose the dimensions of the matrices A, B, C, and D are 20x2, 2x15, 15x40, and 40x4, respectively, and we want to know how best to compute AxBxCxD. Show the arrays cost, last, and multOrder computed by Algorithms matrixOrder() in the lecture notes.|     | 0   | 1   | 2   | 3                                                                                            | 4                                                                                                                                                                                                                 || --- | --- | --- | --- | -------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || 0   | -   | 0   | 600 | min{$(0,1)+(1,3)+d_0*d_1*d_3$ <br>                         $(0,2)+(2,3)+d_0*d_2*d_3$} = 2800 | min{$(0,1)+(1,4)+d_0*d_1*d_4$ <br>                                                        $(0,2)+(2,4)+d_0*d_2*d_4$ <br>                                                        $(0,3)+(3,4)+d_0*d_3*d_4$} = 1680 || 1   |     |     | 0   | 1200                                                                                         | min{$(1,2)+(2,4)+d_1*d_2*d_4$ <br>                                                                                                                         $(1,3)+(3,4)+d_1*d_3*d_4$} = 1520                      || 2   |     |     |     | 0                                                                                            | 2400                                                                                                                                                                                                              || 3   |     |     |     |                                                                                              | 0                                                                                                                                                                                                                 || 4   |     |     |     |                                                                                              |                                                                                                                                                                                                                   |Last Table|     | 0   | 1   | 2   | 3   | 4   || --- | --- | --- | --- | --- | --- || 0   | -   | -   | 1   | 1   | 1   || 1   |     |     |     | 2   | 3   || 2   |     |     |     |     | 3   || 3   |     |     |     |     |     || 4   |     |     |     |     |     |Final order: $A_1*((A_2*A_3)*A_4)$"
Chain Matrix Multiplication.md,Greedy heuristic,
Capital Budgeting.md,NPV,Net Present Value: calculate the present value of future cash flows in order to whether a project is worth up taking $NPV_1\ge NPV_2$.
Capital Budgeting.md,Calculating NPV of projects,"NOWC: This is the net change in accounts receivable, accounts payable, and inventory during the measurement period. __An increase in working capital uses cash, while a decrease produces cash."
Capital Budgeting.md,Unequal Life Projects (Fixed term),
Capital Budgeting.md,IRR,"Rate of return which makes the NPV of a project = 0.A higher IRR is better. _Intuitively, if the IRR is higher, this means that the cashflows are equivalent to returns at that level of interest rate_.Dependent only on the cashflows and not the required return.Cons:- Non conventional cashflows makes IRR method unreliable as there may be more than 1 IRR- The IRR can also be 0- Assumes cash flows are reinvested at the IRR and not the WACC (unrealistic)Relationship to NPV:- IRR and NPV gives the same decision for independent projects.- NPV should be used for mutually exclusive projects"
Capital Budgeting.md,Choosing between projects (Crossover rate),Crossover rate is the rate which we are indifferent between 2 projects. Both projects have NPV = 0.
Capital Budgeting.md,MIRR,The internal rate of return which makes the present value of cash outflows = to the present value of the terminal value (FV of cash inflows) of the project.Handles the IRR problem by combining cash flows into (-) sign cash flow at time 0 and 1 (+) sign cash flow at terminal year.
Cache.md,Memory Organisation,Memory in cache is stored as cache lines.A CPU will try to access cache through a memory address:
Cache.md,Instruction cache and Data cache,Storing these separately will allow in better parallelism. CPU is able to fetch instructions from instruction cache while writing to data cache for STUR instructions.
Cache.md,[Cache Placement Policies](Notes/Cache%20Placement%20Policies.md),
Cache.md,[Cache Replacement Policies](Notes/Cache%20Replacement%20Policies.md),
Cache.md,[Cache Write Policies](Notes/Cache%20Write%20Policies.md),
Cache.md,Performance,"The key factor affecting cache performance is the effects of cache misses. When a cache miss occurs, compute cycles are needed to find a victim, request the appropriate data from memory, fill the cache line with this new block and resume execution."
Cache.md,Types of Misses,
Cache.md,Compulsory miss,First reference to a given block of memory. This is an inevitable miss as data has not been accessed before.
Cache.md,Capacity miss,This occurs when the current working set exceeds the cache capacity. Current useful blocks have to be replaced.
Cache.md,Conflict miss,When useful blocks are displaced due to placement policies. E.g. fully associative cache mapping
Cache.md,Design considerations,- Number of blocks- More blocks means that we will have larger capacity and result in lesser capacity misses- Associativity- Reduce conflict misses- Increases access time- Block size- Larger blocks exploit spatial locality. More data in the same area is loaded together when requested.- Reduces compulsory misses since more data is loaded at once.- Reduces number of cache blocks for a fixed block size. This leads to increase in conflict miss as higher chance for different data map to the same blocks.- Increases miss penalty as more data needs to be replaced on a miss.- Levels of cache- Using multi-level cache will reduce the miss penalty
Cache.md,Measuring Impact with CPI,$$\begin{align}&CPU_{time}=(CPU_{\text{execution cycles}}+\text{Memory stall cycles})\times\text{Cycle Time}\\&CPU_{time}=((IC\times CPI)+(IC\times\%\text{Memory Access}\times\text{Miss Rate}\times\text{Miss Penalty}))\times \text{Cycle Time}\end{align}$$L1 cache hit can be considered to be part of CPI ideal as it is often possible to complete the data access within the ideal clock cycles.
Cache.md,Example,
Cache.md,Multi-level cache example,
Cache.md,Measuring Impact with Average Memory Access Time (AMAT),"We need a way to measure the performance of cache, standalone from the performance of the CPU.AMAT is the average time to access memory considering both hits and misses$$\begin{aligned}AMAT&=\text{Hit Time}\times(1-\text{Miss Rate})+\text{Miss Rate}\times(\text{Hit Time}+\text{Miss Penalty})\\&=\text{Time for hit}+\text{Miss Rate}\times\text{Miss Penalty}\\\end{aligned}$$Note here that *Miss Penalty* is the loss in cycles for a miss, and not just the cost of main memory access. e.g If time to hit cache = 1, time to hit main memory = 100, miss penalty is $100-1=99$.One example to show that AMAT is superior would be to consider two different caches with similar miss rates, but drastically different hit times. Using the miss rate metric, we would rate both caches the same. Using the AMAT metric, a cache with a lower hit time or lower miss penalty will outperform a cache with a higher respective time, assuming all other variables are the same."
Cache.md,Practice Problems,"3 bit index, 2bit tag with 0 bit offset.| Addr  | Index | Tag | H/M | State  || ----- | ----- | --- | --- | ------ || 10011 | 011   | 10  | M   | 001,10 || 00001 | 001   | 00  | H   |        || 00110 | 110   | 00  | H   |        || 01010 | 010   | 01  | M   | 010,01 || 01110 | 110   | 01  | M   | 110,01 || 11001 | 001   | 11  | M   | 001,11 || 00001 | 001   | 00  | M   | 001,00 || 11100 | 100   | 11  | M   | 100,11 || 10100 | 100   | 10  | M   |   100,10     |Bits for offset = $log_24=2$2 way set associative cache: Each set contains 2 cache lines -> 2 blocks per set -> 8 bytes per set -> 2 setsBits for index -> 1| Access | 10001101 | 10110010 | 10111111 | 100001100 | 10011100 | 11101001 | 11111110 | 11101001 || ------ | -------- | -------- | -------- | --------- | -------- | -------- | -------- | -------- || Tag    | 10001    | 10110    | 10111    | 10001     | 10011    | 11101    | 11111    | 11101    || Offset | 01       | 10       | 11       | 00        | 00       | 01       | 10       | 01       || Index  | 1        | 0        | 1        | 1         | 1        | 0        | 1        | 0        || H/M    | M        | M        | M        | H         | M        | M        | M        | H        || LRU1   | 10001    | 10110    | 10111    | 10001     | 10011    | 11101    | 11111    | 11101         || LRU2   | NA       | NA       | 10001    | 10111     | 10001    | 10110    | 10001    | 10110         |Hit rate: 2/8 = 25%a.$$\begin{align}&T=IC\times CPI\times Period\\&CPI_{ideal}=1.25\\&\text{L1 stall cycles}=0.2\times0.2\times8=0.32\\&\text{L2 stall cycles}=0.2\times0.2\times0.1\times30=0.12\\&\text{CPI stall}=1.25+0.32+0.12=1.69\\\end{align}$$b.$$\begin{align}&\text{L1 stall cycles}=0.2\times0.2\times30=1.2\\&\text{CPI stall}=1.25+1.2=2.45\end{align}$$"
Cache Write Policies.md,Write-through,"Every write to the cache will lead to subsequent writes to the rest of the memory hierarchy, L1 -> L2 -> Main Memory -> Disk."
Cache Write Policies.md,Advantages,- Memory coherency
Cache Write Policies.md,Disadvantages,"- High bandwidth requirement, every cache write results in high latency- Memory becomes slow as size increases"
Cache Write Policies.md,Write-back,Only write memory to rest of the memory hierarchy on cache replacement.Maintain the state of each cache line as the following- Invalid: not present- Clean: present and unmodified- Dirty: present and modifiedUpdate the state bit on modification.
Cache Write Policies.md,Disadvantages,"- [Cache Coherence](Notes/Thread%20Level%20Parallelism.md#Cache%20Coherence): in multi-processors where each core maintains its own level of cache, if one core needs to access the data that has been modified by another core, they will get the stale data from memory as updated data is still in that core's own cache and has not been propagated.- Coherent I/O: I/O devices are able to use [DMA](Notes/Direct%20Memory%20Access.md) and access stale copies of data in main memory."
Cache Placement Policies.md,Direct Mapped Cache,"Each memory block can only be mapped to a single fixed cache line.This means that there are no sets of cache memory, rather each line is a set itself. If another block maps to this set, the previous data block is replaced. E.g. Block 0, Block 64, Block 0, Block 64 etc."
Cache Placement Policies.md,Advantages and Disadvantages,"- This placement policy is power efficient as it avoids the search through all the cache lines.- It has lower cache hit rate, as there is only one cache line available in a set. Every time a new memory is referenced to the same set, the cache line is replaced, which causes conflict miss"
Cache Placement Policies.md,Fully Associative Cache,"To increase flexibility, one way is to allow memory block to be placed anywhere in the cache. This can be framed as a single cache set holding all the cache lines. Index bits are no longer required since there is no distinguishing between sets:"
Cache Placement Policies.md,Advantages and Disadvantages,- Fully associative cache structure provides us the flexibility of placing memory block in any of the cache lines and hence full utilization of the cache.- The placement policy provides better cache hit rate.- Offers the use of a wider variety of replacement algorithms on miss- The placement policy is slow as it takes time to iterate through all the lines.
Cache Placement Policies.md,Set-Associative Cache,Trade-off between direct and fully associative cache.
C.md,Special keywords,
C.md,Volatile,```cint volatile foo;```Volatile is a qualifier that is applied to a variable when it is declared. It tells the compiler that the value of the variable may change at any time-without any action being taken by the code the compiler finds nearby.
C.md,Use in peripheral registers,"These registers may have their values changed asynchronously during program flow. Code without this keyword can be *optimised* by the compiler into an infinite loop.```cUINT1 * ptr = (UINT1 *) 0x1234;// Wait for register to become non-zero.while (*ptr == 0);// Do something else.```Compiler interprets the ptr value is being always 0, as it has already loaded the value in the second line, resulting in an infinite loop:```assemblymov ptr, #0x1234mov a, @ptrloop bz loop```Same situations can occur for variables that may be modified in [ISRs](Notes/Interrupts.md) or by [multi-threaded applications](Notes/Thread%20Level%20Parallelism.md)."
Building Blocks of the Internet.md,Internet socket interface,"How does one program running on one end system instruct the Internet to deliver data to another program running on another end system?End systems attached to the Internet provide a socket interface that specifies how a program running on one end system asks the Internet infrastructure to deliver data to a specific destination program running on another end system. This Internet socket interface is a set of rules that the sending program must follow so that the Internet can deliver the data to the destination program:> Suppose Alice wants to send a letter to Bob using the postal service. Alice, of course, can’t just write the letter (the data) and drop the letter out her window. Instead, the postal service requires that Alice put the letter in an envelope; write Bob’s full name, address, and zip code in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of the envelope; and finally, drop the envelope into an official postal service mailbox. Thus, the postal service has its own “postal service interface,” or set of rules, that Alice must follow to have the postal service deliver her letter to Bob. In a similar manner, the Internet has a socket interface that the program sending data must follow to have the Internet deliver the data to the program that will receive the data."
Building Blocks of the Internet.md,Protocols,"Protocols define format and order of messages sent and received among network entities, and actions taken on message transmission and reception"
Building Blocks of the Internet.md,Protocol Layering,"Explicit structure allows identification and relationship of the different piecesModularization eases maintenance and updating of system- change of implementation of layer’s service transparent to rest of system- For example, a change in gate procedure doesn’t affect rest of systemTaken together, protocols of the various layers form the protocol stack:"
Building Blocks of the Internet.md,Encapsulation,Each layer encapsulates the payload with additional header information for the next layer to continue the exchange of data with the next layer.
Building Blocks of the Internet.md,Network edge,Computers and other devices connected to the Internet are called *end systems* because they sit at the edge of the Internet. End systems include both **clients and servers**.The access network is the network that physically connects the end systems to the first router.
Building Blocks of the Internet.md,ISP Access,ISP access is how we connect to the ISPs:- Digital Subscriber Line: uses existing telephone line (twisted copper wire) to exchange data with the telcos central office- Fiber To The Home (FTTH): 10 Mbps - 100 Gbps
Building Blocks of the Internet.md,Local Access,- Ethernet: uses twister copper wire to connect to an ethernet switch which in turn connect to the larger internet.- WiFi 802.11
Building Blocks of the Internet.md,Network Core,"The network core represents the mesh of interconnected routers that make up the ISPs.- Tier 1 ISPs span across the globe. But for customers using different ISPs to exchange data, the ISPs themselves must be connected through Internet Exchange Points (IXP)- Regional ISPs compete with each other and pay T1 ISPs for their traffic- Access ISPs connect to any lower tier ISPs for their traffic- Content provider networks create their own private networks which connects its data centres to the internet, bypassing lower tiered ISPs to bring content close to their customers."
Building Blocks of the Internet.md,Packet switching,"Data is broken into smaller chunks called packets, which are transmitted through communication links and packet switches at the **full transmission rate** of the link.Store and forward transmission: the packet switch must receive the entire packet before it can transfer the first bit of the packet."
Building Blocks of the Internet.md,Forwarding Tables and Routing Protocols,"*How does the router determine which link it should forward the packet onto?*For the Internet, the [Internet Protocol](Notes/Internet%20Protocol.md) dictates a destination IP address in each packet. Each router has a forwarding table that maps destination addresses (or portions of the destination addresses) to that router’s outbound links. When a packet arrives at a router, the router examines the address and searches its forwarding table, using this destination address, to find the appropriate outbound link.*How does the forwarding table get set?*Internet has a number of special routing protocols that are used to automatically set the forwarding tables. A routing protocol may, for example, determine the shortest path from each router to each destination and use the shortest path results to configure the forwarding tables in the routers."
Building Blocks of the Internet.md,Queueing Delay,
Building Blocks of the Internet.md,Exercises,"a) What is a communication protocol ?A protocol defines the format and order of messages and the set of procedures performed on a message when it is sent or received.b) Name the different layers in the Internet protocol stack, and place the followingprotocols/functions/concepts at the correct layer : IP, TCP, Ethernet, HTTP, bit coding , FTP, IEEE 802.11 WLAN, TP Category 6, Routing , UDP.| Application | Transport | Network | Link        | Physical   || ----------- | --------- | ------- | ----------- | ---------- || HTTP        | TCP       | IP      | IEEE 802.11 | TP Cat 6   || FTP         | UDP       | Routing | Ethernet    | bit coding |c) What layer in the Internet protoco l stack is responsible for the transfer of a data packet over a single link, between two directly connected devices?Link layer.*d) A router has two main functions, which can be described by the two terms “routing” and “forwarding”. What is the difference between routing and forwarding?*Routing refers to the address translation performed in order to determine the correct destination address for the packet. Forwarding is the actual relay of the data packet to the destination address.*e) What service does the transport layer describe? Give a short answer*Service of the transfer of data from one host to another.$$\begin{align}&\text{First packet delay} = N*(L/R)\\&\text{The 2nd packet must wait for the first packet to reach the 2nd router before it is sent}:\\&\text{2nd packet delay} = N*(L/R)+(L/R)\\&...\\&\text{Pth packet delay} = N*(L/R)+(P-1)*(L/R)\\&\text{All packets sent after Pth delay}:\\&d_{end-to-end} = (N+P-1)*(L/R)\end{align}$$a. $d_{prop}=m/s$b. $d_{trans}=L/R$c. $d_{end-to-end}=d_{prop}+d_{trans}$d. At the start of the linke. In the linkf. At the destinationg.$d_{trans}=\frac{120}{56\times10^3}=0.00214s$$0.00214=m/2.5\times10^8$$m=535.7km$$$\begin{align}&\text{All bits must be generated before it can be grouped into packets}:\\d_{generate}&=56\times8/(64\times10^3)=7ms\\d_{trans}&=56\times8/(2\times10^6)=0.000224s\\d_{prop}&=0.01s\\Total&=17.224ms\end{align}$$The first bit of host 1 reaches Router A after 0.002sThe last bit of host 1 reaches Router A after $0.002+(1500\times8)/(4\times10^6)=0.005s$The first bit of host 2 reaches Router A after 0.006s. Hence, no queueing delay is incurred as the last bit of host 1 is propagated before host 2 first bit arrives.No buffering when:$$d_1+\frac{L}{R_1}< d_2$$a. The first packet to be propagated has no queueing delayThe 2nd packet will have $d=L/R$3rd packet: $d=2\times L/R$Nth packet: $d = (N-1)\times L/R$Avg delay:$$\begin{align}\frac{1}N\sum_{i=0}^{N-1}i\times\frac{L}{R}\\=\frac{L}{NR}\times\frac{N(N-1)}2\\=L(N-1)/2R\end{align}$$b. The Nth packet is propagated after $(N-1)L/R$, which is $<NL/R$. The first packet of the  next stream will not have to wait. Average queueing delay of such a packet will be the same as in (a)."
Buffer Pools.md,Page table,A page table is used to keep track of the pages loaded in the buffer pool. This helps the system to determine if the page is already in buffer without having to go to the disk.
Buffer Pools.md,Buffer Manager,Used to control the memory in the buffer pool and provide the following features:
Buffer Pools.md,Prefetching,"While the current page is being processed, we can prefetch the next required pages to be accessed based on a query plan. This reduces total I/O time as operations are done in parallel."
Buffer Pools.md,Scan sharing,"If a query starts a scan and if there is one already doing this, it would attach to that query’s cursor. Once that current query is complete, the new query can continue to scan those pages that were initially skipped."
Buffer Pools.md,Buffer Replacement Policies,Similar to [Cache Replacement Policies](Notes/Cache%20Replacement%20Policies.md).[Page Replacement Policies](Notes/Page%20Replacement%20Policies.md)
Buffer Pools.md,Practice Problems,Process flow:1. Fetch block 12. Process block 1 and Fetch block 23. Process block 2 and Fetch block 34. Process block 3a. Total time is 4P; only 4 cycles neededb. R + P + 2R = 3R + Pc. R + P + 2P = 3P + RIf no pre-fetching:3(R+P) = 3R + 3P| Reference | LRU     | Optimal || --------- | ------- | ------- || 5         | ABCDE   | ABCDE   || 6         | ABCED   | ABCDE   || 7         | BCEDF/A | ABCDF/E || 8         | BCEFD   | ABCDF   || 9         | CEFDG/B | ABCDG/F || 10        | CEFGD   | ABCDG   || 11        | EFGDH/C | ABCDH/G || 12        | EFGHD   | ABCDH   || 13        | FGHDC/E   | ABCDH   |The LRU is suboptimal in this case because it chooses to replace useful pages like B/C which are needed later. A more optimal strategy is to choose pages for replacement based on the corresponding level of the page in the B-Tree.
Broadcast Abstractions.md,Unreliable Broadcast,Does not guarantee anything. Such events are allowed:
Broadcast Abstractions.md,Best Effort Broadcast,"Guarantees reliability only if sender is correct- BEB1. Best-effort-Validity: If pi and pj are correct, then any broadcast by pi is eventually delivered by pj- BEB2. No duplication: No message delivered more than once- BEB3. No creation: No message delivered unless broadcast"
Broadcast Abstractions.md,Implementation,"We can use perfect links:Upon <beb Broadcast | m>send message m to all processes (for-loop)Correctness- If sender doesn’t crash, every other correct process receives message by perfect channels (Validity)- No creation & No duplication already guaranteed by perfect channels"
Broadcast Abstractions.md,Reliable Broadcast,"BEB gives no guarantees if sender crashes. Reliable strengthens this by giving guarantees even if sender crashes. Guarantee: either all correct processes deliver m or none of them.- RB1 = BEB1. Validity- RB2 = BEB2. No duplication- RB3 = BEB3. No creation- RB4. Agreement. If a correct process delivers m, then every correct process delivers m"
Broadcast Abstractions.md,Fail Stop (Lazy) Implementation,"- Perfect failure detector (P): use this to detect when process crash- If P is replaced with [Eventually perfect failure detector](Notes/Failure%20Detectors.md#Eventually%20perfect%20failure%20detector): eventual strong accuracy means that some correct processes may be suspected as crashed. However, since we redistribute messages on crash, no property is violated.- BEB: use this to redistribute messages when detect a crash from a processCase 1: detect crash and redistributeCase 2: delivered message, detect crash and redistribute"
Broadcast Abstractions.md,Performance,"Message complexity: best case O(N), worst case O(N^2)Time complexity: best case 1 round. worst case 2 rounds"
Broadcast Abstractions.md,Fail Silent (Eager),No failure detector necessary. A pessimistic approach that just redistributes any message by assuming that the process has failed.
Broadcast Abstractions.md,Uniform Reliable Broadcast,"Reliable broadcast creates a problem. If a failed process delivers a message that has a side effect (such as withdrawing some money from an account), the correct processes need not deliver (know of) this side effect.- URB1 = RB1.- URB2 = RB2.- URB3 = RB3.- URB4. Uniform Agreement: For any message m, if a process delivers m, then every correct process delivers m"
Broadcast Abstractions.md,Eager (Fail-stop),"Intuition: deliver the message only when we know that every other correct process can deliver the message. If we do not wait for all correct processes (or we do not have the complete set of failed processes using a weaker FD), we might deliver m even though some correct processes did not receive the message, violating agreement."
Broadcast Abstractions.md,Majority-ACK (Fail Silent),Correctness assumption: a majority of processes are always correct. Resilience is N/2 machines can fail
Broadcast Abstractions.md,Causal Broadcast,"[Causality](Notes/Distributed%20Abstractions.md#Causality) between broadcast events is preserved by the corresponding delivery events- If broadcast(m1) happens-before broadcast(m2), any delivery(m2) cannot happen-before a delivery(m1)- However, delivering m2 by itself still preserves causal order."
Broadcast Abstractions.md,Examples,"- 3 caused the broadcast of 2, causal order is preserved for {3,2,1} or {3,1,2}%%[🖋 Edit in Excalidraw](Pics/Broadcast%20Abstractions%202023-03-07%2021.54.54.excalidraw.md), and the [dark exported image](Pics/Broadcast%20Abstractions%202023-03-07%2021.54.54.excalidraw.dark.svg)%%"
Broadcast Abstractions.md,Reliable (Fail Silent),Each broadcasted messages carries a history which can be used to ensure causality before delivery. The history is an ordered list of casually preceding messages in the past.
Broadcast Abstractions.md,Fail Silent waiting,
Broadcast Abstractions.md,Orderings,- Single source FIFO order: delivery is ordered in FIFO order for deliveries of its own broadcasts- Total order: order of delivery is the same across all processes- Causal order: [Causality](Notes/Distributed%20Abstractions.md#Causality)
Breadth First Search.md,Graph Traversal,_Assuming ties are handled in alphabetical order_Expansion Order:A > B > C > D > E > GFinal Path:A > C > G
Black Box Testing.md,Equivalence Class Testing,*Equivalence Class*: set of values that produce the same output*Example: We are testing if an alert is sent*Valid ECs will produce a positive output according to specification (i.e. will send an alert)Invalid ECs will produce a negative output (i.e. no alert sent).Error or exception ECs are invalid data ranges or types not within specification (i.e. exception is thrown).
Black Box Testing.md,Boundary Value Testing,"For each EC, there are 3 BVs for the 2 ends of the range:1. On the value2. Below the value3. Above the valueDiscrete values have no BV."
Boyer-Moore Algorithm.md,Bad character rule (charJump),Case 1 (mismatched character does not exist in the rest of the pattern):Case 2 (mismatched character is found in the rest of the pattern):
Boyer-Moore Algorithm.md,Example Array,
Boyer-Moore Algorithm.md,Pseudocode,Right most occurrence such that we do not over slide.
Boyer-Moore Algorithm.md,Simple BM scan,"When using charJump only, taking the max of charJump and $m-k+1$ will ensure that we do not _left  shift_ the pattern and that we always at least move by 1 character.Example of where we could have made a _left shift_ of the pattern, which is not what we want:"
Boyer-Moore Algorithm.md,Good suffix rule (matchJump),"Derive maximum shift from the structure of the pattern.Run through each case below in order (1 -> 2 -> 3) so as to shift by the least amount such that a suffix is matched.> [!NOTE] General formula for slide> $(m-k)+(m-q$)> - k is the position of the mismatch> - q is the position of end of the next matching suffixCase 1: matching suffix occurs earlier in the pattern and __preceded by a different character.Case 2: matching suffix occurs at the __start of the pattern.Case 3: __no occurrence of the matching suffix in the rest of the pattern__.Case 4: mismatch on the first characterThe last character array entry is always 1. We have no information about matching suffixes from the text as the first comparison is already mismatched. Hence, we can only safely shift by 1 character."
Boyer-Moore Algorithm.md,Example Array,
Boyer-Moore Algorithm.md,Pseudocode,
Boyer-Moore Algorithm.md,Examples,
Bonds.md,Terminologies,"Callability: Issuer can redeem the bond before maturity _leads to higher risk to the investor, will mean that the bond will have **relatively higher YTM_.Putability: Buyer can redeem the bond before maturity _leads to higher risk to the __issuer__, will mean that the bond will have **relatively lower YTM_."
Bonds.md,Prices,"> [!NOTE] The relationship between bond prices and interest rates> When interest rate increases, people are able to obtain bonds with higher YTM, this makes the current bond which offers a lower YTM __worth less__: becomes a discount bond.> Vice versa for when interest rates decreases> [!NOTE] Price Relationships> - Higher coupon payments will have less sensitivity to changes in interest rates> - Longer maturities will have higher sensitivity to interest rate changes> - Riskier bonds ($\beta\ is\ higher\ hence\ r_e$ is higher) will have lower price"
Bonds.md,Yield To Maturity,"The expected return if one was to hold the bond till maturity> [!IMPORTANT] YTM is different from EAR> 8% semi-annual coupon bond selling at par will have $EAR=(1+0.04)^2-1=8.16\%$ but will have a YTM of 8%>> A bond that has same risk and term as the above, but pays _annual_ coupons instead will have a YTM of 8.16%Depends only on the maturity and risk. If these are the same across 2 bonds, they will have the same effective yield with differing coupon payments."
Bonds.md,Current Yield,$$ Current\ Yield=\frac{Annual\ Interest\ Payments}{Current\ Bond\ Price} $$Zero coupon bonds: Bonds which give out no coupons- Current yield = 0> [!NOTE]> For all par bonds:> $$YTM=Current\ Yield=Coupon\ Rate$$> For all discount bonds:> $$YTM>Current\ Yield> Coupon\ Rate$$> For all premium bonds:> $$YTM<Current\ Yield< Coupon\ Rate$$
Bonds.md,Term Structure (Yield Curve),"Inflation premium: reflects the health of the economy. _Investors expect inflation to rise in an economy that is doing well_.Real rate: does not affect the slope of the curve, only translates the curve."
BitTorrent.md,Requesting Chunks,1. Alice issue requests for the list of chunks neighbouring peers have.2. Alice will use **rarest first** to determine the chunks that are the rarest among the neighbours and then request those chunks first
BitTorrent.md,Sending Chunks (tit-for-tat),"An incentive based trading algorithm:1. For each neighbour, continually measure the rate which she receive bits and determine the peers which are sending at the highest rate.2. Send chunks to these **unchoked peers**.3. Every 10 seconds, recalculate the rates4. Every 30 seconds, pick one additional neighbour at random and send it chunks, optimistically unchoking this peer.> [! Consider Alice and an optimistically unchoked Bob]> If the rate Alice is sending data to Bob is high enough, she may become one of Bob's top uploaders. In which case, Bob will begin to send data to Alice. If the rate Bob sends data is high enough, he might become Alice's top uploaders.>> *The effect is, peers capable of uploading at compatible rates tend to find each other.*>> The random neighbour selection allows new peers to get chunks so that they can have something to trade.>> All other peers are choked and do not receive chunks"
Bitmap.md,Bit Array,An array that compactly stores bits.
Binary Tree.md,Properties of binary trees,"Full binary trees (all nodes have 0 or 2 children)- The number of nodes n in a full binary tree is at least $2h+1$ and at most $2^{h+1}-1$, where h is the height of the tree. A tree consisting of only a root node has a height of 0.- The number of leaf nodes l in a perfect binary tree, is $\frac{(n+1)}{2}$ because the number of non-leaf (a.k.a. internal) nodes $\sum _{k=0}^{\log _{2}(l)-1}2^{k}=2^{\log _{2}(l)}-1=l-1$.- This means that a full binary tree with l leaves has $2l-1$ nodes.Complete binary trees (like those use in [Heaps](Notes/Heaps.md))- A complete binary tree  has $\lfloor n/2 \rfloor$ internal nodes"
Binary Tree.md,Array representation,For a node at index i:- Left child: 2i + 1- Right child: 2i + 2- Parent: $\lfloor{\frac{(i-1)}{2}}\rfloor$
Binary Search Tree.md,Operations,
Binary Search Tree.md,Searching,We are able to search for a specific key in O(h) time where h is the height of the tree. The BST property allows us to perform binary search.
Binary Search Tree.md,Min and Max,The smallest node is rooted at the left most part of the the tree and is symmetric for the largest node.```while x.left != nullx = x.leftreturn x```
Binary Search Tree.md,Successors,"We are able to find the successor of a node without any key comparisons:1. If there is a right subtree to the node x, the successor is the leftmost element in the right subtree2. Else, the successor is the parent of the first element which is a left child when traversing upwards through the tree.```if x.right != nullreturn Tree-Min(x.right)y = x.pwhile y != null and x == y.rightx = yy = y.preturn y```"
Binary Search Tree.md,Insert,"The procedure maintains the trailing pointer y as the parent of x. After initialization, the while loop in lines 2-6 causes these two pointers to move down the tree, going left or right depending on the key comparison, until x becomes NIL.  We need the trailing pointer y, because by the time we find the NIL where z belongs, the search has proceeded one step beyond the node that needs to be changed. Lines 7–10 set the pointers that cause z to be inserted.```x = rootwhile xy = xif z.key < x.keyx = x.leftelse x = x.rightz.p = yif y == nullroot = yelse if z.key < y.keyy.left = zelse y.right = z```"
B-tree.md,Practice Problems,a.1. Interior node min keys: $\lfloor(n/2)\rfloor=5$Min pointers: $min\ key + 1 = 5+1=6$2. Leaf node min key: $\lfloor(n+1/2)\rfloor=5$Min pointers: $min\ key + 1 = 5+1=6$b.1. Interior node min key: 5Min pointers: 5+1 =62. Leaf node min key: 6Min pointers: $min\ key + 1 = 6+1=7$
B+ Tree Index.md,Validity,
B+ Tree Index.md,Searching,"1. If search key is greater than ith key, follow i+1 pointer, else follow ith pointer"
B+ Tree Index.md,Insertion,"1. Find which node to insert record2. If node is not full, insert the record (maintain sorted order)3. Else1. Split the node and distribute the keys2. If no parent, create root node3. Else, insert into parent recursively until no splits needed"
B+ Tree Index.md,Deletion,"Case 1: key can be deleted while maintaining constraintsCase 2: key can be borrowed from sibling nodes, e.g. take 16 from left:Case 3: cannot borrow from siblings1. Merge 2 nodes and delete 1 of them2. Delete key from parent (one child is removed, may need to remove key from parent)3. Recursively apply delete on this parent if it is not full enough"
B+ Tree Index.md,Construction,One way to do so is through a series of insert operationsNo sequential storage of leaf nodes but is more suitable for dynamic data.
B+ Tree Index.md,Bulk Loading,"Leaves will be stored sequentially, will work for static data (all data is known before hand)1. Sort all data entries based on search key2. Start by creating all leaf nodes by packing the keys3. Insert internal nodes bottom up<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/HJgXVxsO5YU?start=160"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen></iframe>"
B+ Tree Index.md,Practice Problems,"a.1. 52. 53. 44. 4b.1. Min key + 1 = 32. Min key + 1 = 33. Original keys: n. After split at least  $\lfloor(n/2)\rfloor=2$4. $\lfloor(n+1)/2\rfloor=2$Height of the B+tree will be $log_{150}1000000=2.75$Need at least 3 levels in the btreeAt most will take 3 I/O + 1 I/O to access data block- Each internal node can index 151 children- Last level indexes 150 recordsa.i. Data level: 1,000,000 records -> 100,000 blocksLeaf level will have 1,000,000 pointers = $1000000/70\approx14286$ blocks2nd level will have $14286/70\approx205$ blocks3rd level will have $205/70\approx3$ blocksRoot will have 1 blockTotal blocks: $100000+14286+205+3=114494$Height of the B-tree will at least be: $log_{70}1000000=3.25\approx4$Number of I/O = 4+1 = 5b. Same as a. Since dense index, order of the data record does not matterc. What if a) but sparse index?Leaf level will have 100,000 pointers to each data record block: $100000/70=1429$ blocks2nd level: $1429/70\approx21$Root: 1Total blocks: $100,000+1429+21+1=101451$"
ASP.NET Web API.md,Routing,Routing uses `APIController`Add an attribute to a controller class to tell the compiler that this is an `APIController````csharp[APIController]public class TicketsController: ControllerBase{}```
ASP.NET Web API.md,Route pattern using _attribute binding_,"- `IActionResult` is a generic interface to encapsulate all the return types such as XML, JSON.- Use the route method attribute to set the endpoint```csharp[HTTPGet][Route(""api/tickets"")]public IActionResult GetTickets(){return Ok(""Reading tickets"");}//with interpolation[HTTPGet][Route(""api/tickets/{id}"")]public IActionResult GetTicket(int id){return Ok($""Reading tickets #{id}"");}```Can also define the route based on the controller name at the class level```csharp[APIController][Route(""api/[controller]"")]public class TicketsController: ControllerBase{[HTTPGet]public IActionResult GetTickets(){return Ok(""Reading tickets"");}[HTTPGet(""{id}"")]public IActionResult GetTicket(int id){return Ok($""Reading tickets #{id}"");}}```"
ASP.NET Web API.md,Route pattern using _model binding_,"[Primitive type binding](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/model-binding?view=aspnetcore-6.0#sources)- `FromRoute`- `FromQuery`: specifies that this attribute must come from the query string```csharp[HTTPGet][Route(""/api/projects/{pid}/tickets"")] //slash at the start indicates from root rather than the controller route defined in the class-levelpublic IActionResult GetTicketFromProject(int pid, [FromQuery] int tid){if (tid == 0){return Ok($""Reading all tickets belonging to project #{pid}"");}else {return Ok($""Reading project {pid}, tickets #{id}"");}}```Using a complex type:```csharppublic class Ticket{[FromQuery(Name=""tid"")]public int TicketId {get; set;}[FromRoute(Name=""pid"")]public int ProjectId {get; set;}}[HTTPGet(""{id}"")][Route(""/api/projects/{pid}/tickets"")]public IActionResult GetTicketFromProject(Ticket ticket){if (ticket.TicketId == 0){return Ok($""Reading all tickets belonging to project #{ticket.ProjectId}"");}else {return Ok($""Reading project {ticket.ProjectId}, tickets #{ticket.TickedId}"");}}```"
ASP.NET Web API.md,Post Routes,```csharp[HttpPost]public IActionResult Post([FromBody] Ticket ticket){return Ok(ticket); //automatically serializes the body into JSON}```
ASP.NET Web API.md,Validation,"[Data Annotations](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/model-binding?view=aspnetcore-6.0#sources)Place validation on the mode attributes```csharppublic class Ticket{[Required]public int TicketId {get; set;}[Required]public int ProjectId {get; set;}}```Custom validation attributes```csharppublic class Ticket_EnsureDueDateForTicketOwner: ValidationAttribute{protected override ValidationResult IsValid(Object value, ValidationContextvalidationContext){var ticket = validationContext.ObjectInstance as Ticket;if (ticket != null && !string.IsNullOrWhiteSpace(ticket.Owner)){if (!ticket.DueDate.HasValue){return new ValidationResult(""Due date is required when ticket has owner"");}}return ValidationResult.Success;}}/**some code **/public string Owner {get; set;}[Ticket_EnsureDueDateForTicketOwner]public DateTime? DueDate {get; set;}```"
ASP.NET Web API.md,Filters,How the filter pipeline works:
ASP.NET Web API.md,Action Filters,"[Place validation on endpoint routes.](https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters?view=aspnetcore-6.0#implementation)```csharppublic class ValidateModelAttribute : ActionFilterAttribute{public override void OnActionExecuting(ActionExecutingContext context){//custom validation on the endpointif (!context.ModelState.IsValid){context.ModelState.AddModelError(""SomeKey"", ""Key is missing"");//short circuit the requestcontext.Result = new BadRequestObjectResult(context.ModelState);}}}[HttpPost][ValidateModelAttribute]public IActionResult Post([FromBody] Ticket ticket){return Ok(ticket); //automatically serializes the body into JSON}```"
ASP.NET Web API.md,[Resource Filters](https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters?view=aspnetcore-6.0resource-filters),"Useful to short-circuit the rest of the pipeline such as during versioning and caching```csharppublic class Version1DiscontinueResourceFilter : Attribute, IResourceFilter{public void OnResourceExecuting(ResourceExecutingContext context){if (path contains v1){contex.Result = new BadRequestObjectResut(new {Versioning = new[] {""This API Version is discontinued""}});}}}```"
Arrays and Slices.md,Arrays,
Arrays and Slices.md,Slices,
Arrays and Slices.md,Slice reallocation,
Arrays and Slices.md,Deletion,
Arrays and Slices.md,Pitfalls,"Since a slice is a pointer to an array, passing a slice into a function will allow the function to modify the original array.However, if slice reallocation occurs inside this new function, the function will no longer be modifying the original array:To avoid this, we can return the modified array from the func"
Arrays and Slices.md,Goroutine Unsafety,
Application Layer.md,Network Application Architectures,Examples:- WebmailExamples:- File sharing- Bittorrent
Application Layer.md,CS vs P2P File Distribution,
Application Layer.md,Client Server,"- The server must transmit one copy of the file to each of the N peers. Thus the server must transmit NF bits. Since the server’s upload rate is us, the time to distribute the file must be at least NF/us.- Let $d_{min}$ denote the download rate of the peer with the lowest download rate, that is, $d_{min} = min \{d1, dp, . . . , dN\}$. The peer with the lowest download rate cannot obtain all F bits of the file in less than $F/d_{min}$ seconds. Thus the minimum distribution time is at least $F/d_{min}$.$$D_{CS} \ge max\{\frac{NF}{u_s},\frac{F}{d_{min}}\}$$From this we can observe that distribution time increases linearly with the number of peers N."
Application Layer.md,P2P,"- To get this file into the community of peers, the server must send each bit of the file at least once into its access link. Thus, the minimum distribution time is at least F/us.- The peer with the lowest download rate cannot obtain all F bits of the file in less than $F/d_{min}$ seconds.- The total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers, that is, $u_{total} = u_s + u_1 + ... + u_N$. The system must upload F bits to each of the N peers, thus delivering a total of NF bits. The minimum distribution time is also at least $NF/(u_s + u_1 + ... + uN)$."
Application Layer.md,Process Communication,Network applications on different hosts need a way to communicate with each other (sometimes across different operating systems).Client: process that initiates the communicationServer: the other part of the pair
Application Layer.md,Addressing Processes,"To receive messages, a process must have an identifier. Each host has a unique IP address but this is not enough as there are many processes which can be running on the same host. A **port number** is needed to identify the receiving process/socket:HTTP server: 80Mail server: 25"
Application Layer.md,Transport Service Requirements,"1. Data integrity: the amount of fault tolerance an application needs2. Throughput: rate which sending process can deliver bits to receiver. Because communication lines are shared, some bandwidth-sensitive applications (such as multimedia) may need a set throughout value.3. Timing: the amount of latency. An example guarantee might be that every bit that the sender pumps into the socket arrives at the receiver’s socket no more than 100 msec later. Such a service would be appealing to interactive real-time applications, such as multiplayer games.4. Security: encryption"
Application Layer.md,Application Layer Protocols,"An application layer protocol defines:- The types of messages exchanged, for example, request messages and response messages- The syntax of the various message types, such as the fields in the message and how the fields are delineated- The semantics of the fields, that is, the meaning of the information in the fields- Rules for determining when and how a process sends messages and responds to messages"
Application Layer.md,HTTP,The Web's application layer protocol is [HTTP](Notes/HTTP.md).
Application Layer.md,Electronic Mail,"A typical message starts its journey in the sender’s user agent, travels to the sender’s mail server, and travels to the recipient’s mail server, where it is deposited in the recipient’s mailbox. When Bob wants to access the messages in his mailbox, the mail server containing his mailbox authenticates Bob (with usernames and passwords)Each user agent uses a separate mail server rather than directly connecting with each other such that there is some recourse (able to keep retrying to send a message) when the destination is currently unreachable."
Application Layer.md,SMTP,"The heart of Internet electronic mail is [[SMTP]], which allows for the transfer of messages."
Application Layer.md,Mail Access Protocol,SMTP is a push protocol. Mail access protocols are needed to retrieve mail from the mail server via a pull operation:- [[POP3]]
Application Layer.md,DNS,Many application protocols are built on top of [[DNS]].
Application Layer.md,BitTorrent,The most popular P2P protocol for file distribution is [[BitTorrent]]
Application Layer.md,Distributed Hash Table,Another application of P2P is a [[Distributed Hash Table]]
Application Layer.md,Socket Programming,How are network applications actually created? Processes running on different machines communicate with each other through sockets.
Application Layer.md,TCP,"Network applications may communicate through TCP, and hence the connection socket must support TCP. TCP provides a reliable **byte-stream** service:Basic byte I/O classes in Java"
Application Layer.md,Client,The client must perform the following operations:1. Open TCP connection to the server2. Send data3. Receive data on the connection4. Close the connection
Application Layer.md,Server,
Application Layer.md,Encoding/Decoding,"To transfer a string between two processes over the network, we must decide how to represent the string as a sequence of bytes."
Application Layer.md,ASCII,
Application Layer.md,UTF-8,- Unicode Transformation Format – 8-bit- Variable length encoding- Up to four bytes per symbol- The first 128 are the same as for ASCII- Backwards compatibility – ASCII text is also valid UTF-8- Dominating format on the Web
Application Layer.md,Helpful Java classes,
Angular.md,Creating components,```consoleng generate component <name>ng g c <name>```
Angular.md,Defining metadata,"A file in the form of `<name>.component.ts` will be generated.```typescript@Component({selector:    'app-hero-list',templateUrl: './hero-list.component.html',providers:  [ HeroService ]})export class HeroListComponent implements OnInit {/* . . . */}````selector`A CSS selector that tells Angular to create and insert an instance of this component wherever it finds the corresponding tag in template HTML. For example, if an application's HTML contains `<app-hero-list></app-hero-list>`, then Angular inserts an instance of the `HeroListComponent` view between those tags.`templateUrl`The module-relative address of this component's HTML template. Alternatively, you can provide the HTML template inline, as the value of the `template` property. This template defines the component's _host view_.`providers`An array of [providers](https://angular.io/guide/glossary#provider) for services that the component requires. In the example, this tells Angular how to provide the `HeroService` instance that the component's constructor uses to get the list of heroes to display."
Angular.md,Templating,In a file in the form of `<name>.component.html`.
Angular.md,Data Binding,
Angular.md,2 way binding,
Angular.md,Pipes,"We can use pipes to transform values into a specific display format in our view.Angular defines various pipes, such as the [date](https://angular.io/api/common/DatePipe) pipe and [currency](https://angular.io/api/common/CurrencyPipe) pipe; for a complete list, see the [Pipes API list](https://angular.io/api?type=pipe). You can also define new pipes.To specify a value transformation in an HTML template, use the [pipe operator (`|`)](https://angular.io/guide/pipes):```{{interpolated_value | pipe_name}}```"
Angular.md,Directives,"Angular templates are dynamic. When Angular renders them, it transforms the DOM according to the instructions given by directives. A directive is a class with a ``@Directive()`` decorator."
Angular.md,Structural directives,"They alter layout by adding, removing, and replacing elements in the DOM. [Guide]()| Directive                                                      | Details                                                                               || -------------------------------------------------------------- | ------------------------------------------------------------------------------------- || [`*ngFor`](https://angular.io/guide/built-in-directives#ngFor) | An iterative; it tells Angular to stamp out one `<li>` per hero in the `heroes` list. ||[`*ngIf`](https://angular.io/guide/built-in-directives#ngIf)                                                                |A conditional; it includes the `HeroDetail` component only if a selected hero exists.|                                                                                       |```typescript<li *ngFor=""let hero of heroes""></li><app-hero-detail *ngIf=""selectedHero""></app-hero-detail>```"
Angular.md,Attribute directives,"They alter the appearance or behavior of an existing element. In templates they look like regular HTML attributes, hence the name. [Guide](https://angular.io/guide/attribute-directives)| Directive | Details || --------- | ------- || [ngModel](https://angular.io/api/forms/NgModel) | `ngModel` modifies the behavior of an existing element (typically `<input>`) by setting its display value property and responding to change events.        |"
Angular.md,Services,```ng generate service <name>ng g s <name>```
Angular.md,Dependency Injection,"Angular uses [Dependency Injection](Notes/Dependency%20Injection.md) to increase modularity.Use [](Notes/Dependency%20Injection.md#Constructor%20injection%20%7CConstructor%20Injection) to utilize a service:```typescriptexport class ProductDetailsComponent implements OnInit {constructor(private route: ActivatedRoute,private cartService: CartService) { }}```"
Angular.md,Hitting APIs,Configure [HTTPModule](https://angular.io/start/start-data#configure-appmodule-to-use-httpclient)Data is passed from services to components via [Observables](https://angular.io/guide/observables)
Angular.md,[Get](https://angular.io/guide/httprequesting-data-from-a-server),```typescriptclass SomeService{constructor(private http: HttpClient){}get(): Observable<Task[]>{return this.http.get<Task[]>(this.apiUrl)}}```
Angular.md,[Post](https://angular.io/guide/httpmaking-a-post-request),
Angular.md,[Delete](https://angular.io/guide/httpmaking-a-delete-request),
Angular.md,[Put](https://angular.io/guide/httpmaking-a-put-request),
Angular.md,[Error handling](https://angular.io/guide/httphandling-request-errors),
Angular.md,RxJS Observables,"Makes use of the [Observer Pattern](Notes/Observer%20Pattern.md).<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/T9wOu11uU6U"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen></iframe>We use `Observables` when there is some stream of data that is changing and we have multiple subscribers that want to change when there is some new data.RxJS provides multiple functions to modify how often we want to call next(), in what ways to format the data etc."
Alignment Problem.md,Problem Formulation,"Let $n1$ and $n2$ represent the position of the character in the respective subsequence S1 and S2. The cost to align characters up till $n1$ and $n2$ can be found by finding the solutions to the cost of aligning characters $n1-1$ or $n2-1$.If the last 2 characters are equal, the cost to align is simply the cost to align the rest of the $n1-1$ and $n2-1$ characters.If they are not equal, we can ignore 1 character, either from n1 or n2 (_resulting in $n1-1$ or $n2-1$_) by replacing it with an underscore: _resulting in a +1 cost_. Take the minimum of this 2."
Alignment Problem.md,Strategy,
Alignment Problem.md,Pseudocode,
Activity Diagrams.md,Swimlanes,_Partition_ an activity diagram to show who is doing which action.
Activity Diagrams.md,Parallel Paths,"**_Fork_ nodes indicate the start of concurrent flows of control.****_Join_ nodes indicate the end of parallel paths.**In a set of parallel paths, execution along **all parallel paths should be complete before the execution can start on the outgoing control flow of the _join_.**> [!EXAMPLE]>  In this activity diagram (from an online shop website) the actions _User browses products_ and _System records browsing data_ happen in parallel. Both of them need to finish before the _log out_ action can take place.>>"
Activity Diagrams.md,Examples,
A-Star Search.md,Example Graphs,
2460 Software Safety and Security.md,Verification vs Validation,Verification: does the software do things right?- can be automated by tools to verify specific propertiesValidation: does the software do the right thing?- requires human judgement to think about which are the correct requirements/operations
2460 Software Safety and Security.md,Verification,Dynamic analysis: performs at run time analysing the real state of the systemStatic analysis: performs at compile time to analyse the simplified state of the system
Brag Doc.md,Shopee,"Project: API auto failure detection, triaging and reporting internal tool- Roll out for LATAM regions- Build new service-specific email reporting flow to increase workflow efficiency and standardization for *all* service teams- Helped to increase success rates from 60% to 90% in the UAT environment"
"Why Vim, in the land of Go.md",Vim Motions...Sickness,"Vim is not *just* vim. Vim comprises of its interface (the program running in a terminal window) and vim **motions**. These are the keyboard shortcuts that can be tied to cursor movements, file operations and even custom functions. I won't go into the weeds about the different commands and modes available as there are a ton of interactive (and even gamified, if that's your type of thing) tutorials out there which do a much better job of what I can hope to do here. Here are some:- vimtutor- [Learn-Vim](https://github.com/iggredible/Learn-Vim)- [vim-be-good](https://github.com/ThePrimeagen/vim-be-good)But why should you learn a bunch of new commands and keystrokes? It seems like a massive time sink. Everyday, some new technology comes out demanding your attention and this just ain't helping."
"Why Vim, in the land of Go.md",Need for speed,"This brings me to my first reason, it makes you really *really* fast. The basic commands keep your hands on the home row of the keyboard, allowing you to just churn out lines and lines of text without ever touching the mouse. And of course, as programmers, we are not always writing code. Time is spent thinking and designing what is the *best* way to solve a problem. But during this deliberation process, we like to try things, add a few lines here and refactor a function there. In general, we like to break stuff to understand how things work and what we should do next. Being able to break stuff quickly, reducing mouse distractions, speeds up the iterative process that is software engineering."
"Why Vim, in the land of Go.md",SSS combo,"[Speed is fun](https://www.scienceabc.com/pure-sciences/why-do-we-feel-so-thrilled-by-speed.html). But if you have ever played brawler or hack and slash type games, you know the thrill of hitting insane combos. This is the same way I feel about vim motions. Want to refactor a nested function? `:10<CR>V10jd<C-d>P`. Want to change all its arguments? `ci(`. Want to give up? `:qa!<CR>`. Keeping that flow state, moving fast, pushing out combos just adds up to quite a lot of fun."
"Why Vim, in the land of Go.md",Vim the program,"Now, let's talk about vim the program itself. In terms of applicability, vim is still *somewhat* immemorial. Vi is installed by default in various Linux distributions, allowing you to interface with server files proficiently. But who cares? You just want to be able to write and debug Go code, see pretty rainbow bracket colours and run a separate terminal program all in the same view. Vim *can't* do that, after all, its website looks like it was made in the 90s:"
"Why Vim, in the land of Go.md",Running naked makes you faster?,"Vim starts of relatively barebones. From its website you can see the key features listed being a multi level undo tree and powerful search and replace, which are all things you would already expect to have. This means that at the start, writing code in vim is extremely painful. It will feel like you are writing code on Notepad but with the added difficulty of hundreds of commands. However, this also means that it is fast. It launches instantaneously on the terminal window and even on low performance virtual machines, the experience is still decent.But what is the point of it all if you are just going to be worse off as a programmer? Vim has an extensive plugin system. Adding functionality which you want is simple, and usually involves finding existing plugins on GitHub, adding them to your initialisation file and configuring the options that you want. See the key here is what *you* want. You get to decide what features you wish to add to your editor, and what you consider bloat. **To quickly get all the necessary features you are used to in GoLand into Vim, there is [this plugin](https://github.com/fatih/vim-go).**"
"Why Vim, in the land of Go.md",Nah it just allows you to tinkle I mean tinker while you run,"All this control does not just limit you to the functionality of things. Looking at the screen for hours a day means aesthetics is just as important to the programmer. Vim allows you extensive options to make it look the way you want to. This type of persistent tinkering may put off some of you who simply wish for some sensible defaults out of the box and at the beginning, it will be constant tinkering to get something you like. However, I assure you that the satisfaction of having something completely personalised to your taste and style will make coding in it 10x more enjoyable.My setup for Go in this year's Advent Of Code:"
"Why Vim, in the land of Go.md",How about running in the open?,"Vim is open source. This means you can (if you want) scrutinise the code for any malicious intent. This does not automatically make Vim *better* per-se. Instead, this might mean that stability is not as guaranteed as compared to an editor where people are paying $69.99 per month for. In fact, Paul Lutus would request for the user to ""stop complaining for a while and make the world a better place.""[^2]Personally, I use [NeoVim](https://github.com/neovim/neovim), which is a fork off Vim that encourages community contributions among other things. For essential Go development features, checkout the wonderful [go.nvim](https://github.com/ray-x/go.nvim) plugin."
"Why Vim, in the land of Go.md",Concrete steps out of the tarpit,"Past all my blabbering, I wish to offer some actionable steps which you can take.1. I would argue that learning vim motions bring you 80% of the way towards using vim as your daily driver. Hence, don't start with Vim, start with vim motions. Look for  options or plugins that enable the use of vim key bindings. For GoLand users, look to [IdeaVim](https://plugins.jetbrains.com/plugin/164-ideavim). This means that you can get good with the essential vim commands without leaving the comfort of GoLand.2. When you feel ready to leave the nest, I recommend installing NeoVim and fiddling with the configurations. Here I also recommend looking at videos, which can offer step by step walkthroughs on the the general ideas behind configuration.- [Your first vimrc](https://www.youtube.com/watch?v=x2QJYq4IX6M)- [Neovim from scratch](https://www.youtube.com/watch?v=ctH-a-1eUME)As with anything that is configurable, one must accept the inevitable situation of things breaking. It helps to treat these situations like mini side projects, ones that will help solidify your understanding of the tools you use and make you a better developer out of it.3. Get inspired. There is a great community out there that use Vim to make incredible things. Many make plugins which I cannot live without, and many others create insanely ""riced"" personal development environments out of their editor. It's hard to get into something tough, if you can't see or want the end goal. [Reddit](https://www.reddit.com/r/neovim)is a good place to explore what you may be interested in. I can also highly recommend following some extremely knowledgeable and entertaining vim content creators like [ThePrimeagen](https://www.youtube.com/@ThePrimeagen) and [TJ](https://www.youtube.com/channel/UCd3dNckv1Za2coSaHGHl5aA)."
"Why Vim, in the land of Go.md",References,[^1]: https://survey.stackoverflow.co/2022/#top-paying-technologies-integrated-development-environment[^2]: https://arachnoid.com/careware/index.html
ByteDance - 2nd Round.md,The Question,"*""A poor business man wants to set up a new business, selling cloud storage to customers. However, he only has 4 old servers, each of 1 TB capacity.""*Questions in order:1. Design the IO flow of such a system, to support basic features of uploading and downloading files given a specific file path.2. How could we support multiple clients?3. What if a client wants to upload a file larger than the capacit of a single server ( > 1 TB)?4. One of the servers failed, causing data loss and large costs to compensate users for the lost/corrupted data. How could we improve the reliability of the 4 servers.5. The businessman wants to be able to *oversell* his service. With only 4TB of storage, we need to be able to sell 8TB worth to customers. This is on the basis that not all customers will use all the storage they purchased."
ByteDance - 2nd Round.md,Wow I am bad,"Pointers 4 and 5 are most interesting, and stumped me during the interview.Question 4:With only 4 servers, and the constraint of being poor, a RAID configuration was not feasible. I think the interviewer expected me to list some redudancy algorithms, but with no knowledge or experience, I couldn't provide any.Question 5:This question and the answers from the interviewer, brought into light the technical difficulties which the cloud storage services we use face everyday. One solution is to compress the files uploaded. If we can compress and decompress files on the fly while serving requests to clients, we can make the 4TB of storage go a long way. Pair this up with *virtual uploads*.Virtual uploads is based on the assumption that every individual does not *really* have many personal files or data. This means a majority of storage that most people ever really make use of, is for public files - files such as music, or the first season of The Office. Ever wondered what Google Drive is doing during the *Scanning File* portion when you upload a file? Apparently, the local file is being hashed in its entirety and sent to the server to check if Google already has the same file in its store. This means that multiple users will have pointers to the same file stored on the server, and the local file is only ""virtually"" uploaded.The final step, and last resort, is to provision more servers. It is hard to provision servers out of thin air, and once bought, they start to add to the cost of the business. Here the interviewer mentioned how one could make use of the constraint on network and bandwith speeds as the time the business will have to provision more servers."
ByteDance - 2nd Round.md,Life's tough,"If it is not yet evident, I definitely did not come up with any of the responses above. A lot of very smart people have had to come up with such solutions and implement them in the real world. I wonder what other problems lie in plain sight, but have solutions which most will remain oblivious to."
4031 Database Systems.md,Indexes,- [Conventional Indexes](Notes/Conventional%20Indexes.md)- [B+ Tree Index](Notes/B+%20Tree%20Index.md)- [Hash Index](Notes/Hash%20Index.md)- [Multi Key Index](Notes/Multi%20Key%20Index.md)
4031 Database Systems.md,Query Processing,- [Query Processing](Notes/Query%20Processing.md)- [One Pass Algorithms](Notes/One%20Pass%20Algorithms.md)- [Two Pass Algorithms](Notes/Two%20Pass%20Algorithms.md)- [Index Based Algorithms](Notes/Index%20Based%20Algorithms.md)- [Query Execution](Notes/Query%20Execution.md)- [Query Compiler](Notes/Query%20Compiler.md)
4031 Database Systems.md,Transactions,- [Transaction Management](Notes/Transaction%20Management.md)- [Concurrency Control](Notes/Concurrency%20Control.md)
2421 Machine Learning.md,Training and validation,"Training a machine learning model involves the use of data. However, we need to test the effectiveness of the model, this is called validation. Hence we need to split the data into training and testing sets."
2421 Machine Learning.md,Bias vs Variance,"- Bias: level of inability for the model to fit the true nature of the data. High bias model cannot fit the true nature- Variance: is the amount which our predictions will change due to a different training data set. This can lead to worse fit on the test set. Formally, its the expected divergence of the estimated prediction from its average value.<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/EuBBz3bI-aA"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe>Our intuition may tell:- The presence of bias indicates something basically wrong with the model and algorithm...- Variance is also bad, but a model with high variance could at least predict well on average...So the model should minimize bias even at the expense of variance?? Not really!Bias and variance are equally important as we are always dealing with a single realization of the data set."
2421 Machine Learning.md,Bias and variance decomposition,- True function: $f(x)$- Prediction function estimated with data D: $\hat{f_D}(x)$- Average of prediction models: $E_D[\hat{f_D}(x)]$$$\begin{align}Variance=E_D[(E_D[\hat{f_D}(x)]-E_D[f(x)])^2]\\Bias=E_D[\hat{f_D}(x)]-f(x)\end{align}$$
2421 Machine Learning.md,Overfitting,"When the learned models are overly specialized for the training samples, leading to low bias and high variance."
2421 Machine Learning.md,Cross Validation,"How do we know how much % to split between test and train-set data? Cross validation will attempt many different combinations to find the best split.<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/fSytzGwwBVw"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe>"
2421 Machine Learning.md,Interpretability,
2421 Machine Learning.md,Shrinking the number of variables,"Among a large number of variables the model there are generally many that have little (or no) effect on Y- Leaving these variables in the model makes it harder to see the big picture, i.e. the effect of the “important variables”- Would be easier to interpret the model by removing unimportant variables (setting the coefficients to zero)"
2421 Machine Learning.md,Occam's Razor,"A principle about choosing the simplest explanation for the observed data, which can involve the number of model parameters, data points and fit to data."
2203 Distributed Systems.md,What are distributed systems,"A set of nodes, connected by a network, which appear to its users as a single coherent system."
2203 Distributed Systems.md,Core problems,
2203 Distributed Systems.md,Agreement,
2203 Distributed Systems.md,Two generals problem,“Two generals need to coordinate an attack”- Must agree on time to attack- They’ll win only if they attack simultaneously- Communicate through messengers- Messengers may be killed on their wayGenerals are unable to come to an agreement within a specified time bound using unreliable communication channels.
2203 Distributed Systems.md,Consensus Problem,"All nodes/processes propose a valueSome nodes (non correct nodes) might crash & stop respondingThe algorithm must ensure a set of properties (specification):- All correct nodes eventually decide- Every node decides the same- Only decide on proposed valuesThis problem models the core issue in distributed databases known as **atomic commits**, where we choose to commit if every node agrees to commit and abort if at least one node aborts. It is a consensus with 2 values {commit, abort}."
2203 Distributed Systems.md,Broadcast Problem,"Atomic Broadcast- A node broadcasts a message- If sender correct, all correct nodes deliver message- All correct nodes deliver the same messages (consensus)- Messages delivered in the same order> [!Note]> Atomic broadcast can be used to solve consensus in the following way:> 1. Decide on the first received proposal> 2. Since all messages are in the same order, all nodes will decide the same>> Consensus can be solved by Atomic broadcast>> *Atomic broadcast is equivalent to Consensus*"
2203 Distributed Systems.md,Modelling Distributed Systems,
2203 Distributed Systems.md,Timing assumptions,- Processes: bounds on time to make a computation step- Network: bounds on time to transmit a message- Clocks: lower and upper bounds on clock drift rate
2203 Distributed Systems.md,Failure assumptions,"- Processes: what kind of failure?- Network: can network drop messages, temporarily disconnect?"
2203 Distributed Systems.md,Asynchronous System Model,- No bound on time to deliver a message- No bound on time to compute- Clocks are not synchronized
2203 Distributed Systems.md,Synchronous system,"*""My server always serves requests within 1 week""*- Known bound on time to deliver a message (latency)- Known bound on time to compute- Known lower and upper bounds in physical clock drift rateExamples:- Embedded systems (shared clock)- Multicore computers"
2203 Distributed Systems.md,Partial Synchrony,"*""My server processes requests within one week when it is running, and it will eventually be running for at least a week, I just don't know when that will be.""*- A system that is asynchronous but eventually exhibits some period of synchrony."
2203 Distributed Systems.md,Measuring Performance,
2203 Distributed Systems.md,Message complexity,The number of messages required to terminate an operation of an abstraction
2203 Distributed Systems.md,Time complexity (Rounds),"One time unit in an Execution E is the longest message delay in E. We assume all communication steps takes one time unit. We also call this a round or step.Time Complexity is Maximum time taken by any execution of the algorithm under the assumptions- A process can execute any finite number of actions (events) in zero time- The time between send(m)i,j and deliver(m)i,j is at most one time unit"
2005 Operating Systems.md,Types of OS,"1. Batch Systems: batch similar jobs which automatically transfers control from one job to another- Only 1 job in memory at any time- When job waits for IO, the CPU is idle2. Multiprogram / Time-sharing Systems: several jobs are kept in main memory at the same time- Goal: Improve CPU utilization by running more than one program concurrently even in a single-core CPU- Different from multiprocessing: increase computing power with parallel architectures- __Requires OS to be able to handle memory management, CPU and I/O scheduling for efficiency3. Embedded Systems: physical systems where operations are controlled by computing- Examples:- Real time systems: have jobs that must complete without well-defined fixed time constraints (e.g. car airbag deployment)- Handheld systemsThe OS is typically [Interrupts](Notes/Interrupts.md) driven."
2005 Operating Systems.md,Functions of the OS,1. [Direct Memory Access](Notes/Direct%20Memory%20Access.md)2. [Hardware Protection](Notes/Hardware%20Protection.md)3. Handle [Processes](Notes/Processes.md)4. [Process scheduling](Notes/Process%20scheduling.md)5. [Process Synchronization](Notes/Process%20Synchronization.md)6. [Deadlocks](Notes/Deadlocks.md)7. [Real Time Operating Systems](Notes/Real%20Time%20Operating%20Systems.md)8. [Virtualization](Notes/Virtualization.md)9. [Memory Organisation](Notes/Memory%20Organisation.md)10. [Virtual Memory](Notes/Virtual%20Memory.md)11. [File Systems](Notes/File%20Systems.md)12. [IO Subsystem](Notes/IO%20Subsystem.md)
2005 Operating Systems.md,References,
2005 Operating Systems.md,Operating Systems Concepts,Exercise solutions:- https://codex.cs.yale.edu/avi/os-book/OS10/practice-exercises/index-solu.htmlInstructor's Manual:- http://web.uettaxila.edu.pk/CMS/AUT2011/seOSbs/tutorial/Sol.%20-%20Silberschatz.Galvin%20-%20Operating.System.Concepts.7th.pdf
100 Reading List.md,Algos,1. [A Common-Sense Guide to Data Structures and Algorithms](https://www.amazon.sg/Common-Sense-Guide-Data-Structures-Algorithms/dp/1680507222/ref=sr_1_9?crid=UQ12IKPHMY7G&keywords=Elements+of+Programming+Interviews&qid=1656292315&sprefix=elements+of+programming+interviews%2Caps%2C300&sr=8-9)2. [101 Introduction To Algorithms](Notes/101%20Introduction%20To%20Algorithms.md)Link: [Introduction to Algorithms](https://www.amazon.com/Introduction-Algorithms-fourth-Thomas-Cormen/dp/026204630X/ref=pd_cart_crc_cko_cp_2_6/134-8052667-1718550?_encoding=UTF8&content-id=amzn1.sym.7c768d31-fcb6-4e60-bb16-7d8e97d21350&pd_rd_i=026204630X&pd_rd_r=c619e326-f826-46d4-9060-27d427d0abd9&pd_rd_w=pJEXS&pd_rd_wg=zmv6K&pf_rd_p=7c768d31-fcb6-4e60-bb16-7d8e97d21350&pf_rd_r=3FJ07YDA0YDJHBD65F0W&psc=1&refRID=3FJ07YDA0YDJHBD65F0W)3. [Elements of Programming Interviews](https://www.amazon.sg/Elements-Programming-Interviews-Python-Insiders/dp/1537713949/ref=sr_1_1?crid=UQ12IKPHMY7G&keywords=Elements+of+Programming+Interviews&qid=1656292315&sprefix=elements+of+programming+interviews%2Caps%2C300&sr=8-1)
100 Reading List.md,Distributed Systems,"1. [Understanding Distributed Systems](https://www.amazon.com/Understanding-Distributed-Systems-Second-applications/dp/1838430210?keywords=understanding+distributed+systems&qid=1656280535&sprefix=understanding+dis,aps,118&sr=8-1&linkCode=sl1&tag=utsavized0d-20&linkId=a920b5dfb493c084cd500eb954527f5c&language=en_US&ref_=nav_signin&)"
100 Reading List.md,Under the Hood,"1. [Writing An Interpreter In Go](https://interpreterbook.com/)2. [Writing A Compiler In Go](https://compilerbook.com/)3. Ian McLoughlin, Computer Architecture: An Embedded Approach, McGraw-Hill Education (Asia), 2011, ISBN: 978-0071311182."
100 Reading List.md,Databases,1. High Performance MySQL
100 Reading List.md,Software Engineering,1. [Clean Code](Notes/Clean%20Code.md)
008 Networking.md,[Building Blocks of the Internet](Notes/Building%20Blocks%20of%20the%20Internet.md),
008 Networking.md,[[Application Layer]],- [[HTTP]]- [[Transport Layer Security]]- [[DNS]]- [SMTP](Notes/SMTP.md)- [POP3](Notes/POP3.md)- [BitTorrent](Notes/BitTorrent.md)- [Distributed Hash Table](Notes/Distributed%20Hash%20Table.md)
008 Networking.md,[[Transport Layer]],- [Transmission Control Protocol](Notes/Transmission%20Control%20Protocol.md)- [User Datagram Protocol](Notes/User%20Datagram%20Protocol.md)
008 Networking.md,[[Network Layer]],- [Internet Protocol](Notes/Internet%20Protocol.md)- [Network Address Translation](Notes/Network%20Address%20Translation.md)- [[Browser Networking]][[Link Layer]]- [[Wireless Networks]]
008 Networking.md,References,- [High Performance Browser Networking](https://hpbn.co/)- [@kuroseComputerNetworkingTopdown2017](References/@kuroseComputerNetworkingTopdown2017.md)
006 Tools.md,Frontend,- [TypeScript](Notes/TypeScript.md)- [Angular](Notes/Angular.md)- [React](React)
006 Tools.md,Backend,- [NodeJS](NodeJS)- [ASP.NET Web API](Notes/ASP.NET%20Web%20API.md)- [Go](Notes/Go.md)- [C](Notes/C.md)
006 Tools.md,Databases,- [SQL](SQL)- [MongoDB](MongoDB)-
005 Sorting Algorithms.md,Important algorithms,- [Insertion Sort](Notes/Insertion%20Sort.md)- [Merge Sort](Notes/Merge%20Sort.md)- [Quick Sort](Notes/Quick%20Sort.md)- [Heap Sort](Notes/Heap%20Sort.md)
005 Sorting Algorithms.md,Properties,[Stability](https://en.wikipedia.org/wiki/Sorting_algorithm#Stability): an algorithm is stable if it preserves the original order of any 2 equal elements in its input.  ^85ee66Time complexity:
004 String Matching.md,Important Algorithms,- [Rabin-Karp Algorithm](Notes/Rabin-Karp%20Algorithm.md)- [Boyer-Moore Algorithm](Notes/Boyer-Moore%20Algorithm.md)
004 String Matching.md,Straightforward Solution,
003 Dynamic Programming.md,Well Known Problems,- [Fibonacci Sequence](Notes/Fibonacci%20Sequence.md)- [Longest Common Subsequence](Notes/Longest%20Common%20Subsequence.md)- [Longest Increasing Subsequence](Notes/Longest%20Increasing%20Subsequence.md)- [Alignment Problem](Notes/Alignment%20Problem.md)- [Chain Matrix Multiplication](Notes/Chain%20Matrix%20Multiplication.md)- [Knapsack Problem](Notes/Knapsack%20Problem.md)- [Making Change](Notes/Making%20Change.md)- [Travelling Salesman Problem](Notes/Travelling%20Salesman%20Problem.md)
003 Dynamic Programming.md,Strategies,Both strategies will achieve the same time complexity but bottom up is usually more CPU time efficient due to the simplicity of the code
003 Dynamic Programming.md,Top Down Approach,"1. Formulate the problem in terms of recursive smaller subproblems.2. Use a dictionary to store the solutions to subproblems3. Turn the formulation into a recursive function1. Before any recursive call, check the store to see if a solution has been previously computed2. Store the solution before returningExample with Fib DP:"
003 Dynamic Programming.md,Bottom Up Approach,1. Formulate the problem in terms of recursive smaller subproblems.2. Draw the subproblem graph to find dependencies3. Use a dictionary to store the solutions to subproblems4. Turn the formulation into a recursive function1. Compute the solutions to subproblems first2. Use the solutions to compute the solution for P and store itExample with Fib DP:
003 Dynamic Programming.md,Exercises,
003 Dynamic Programming.md,Binomial Coefficients,b.c. A top down approach:d. A bottom up approach
003 Dynamic Programming.md,References,- https://www2.seas.gwu.edu/~ayoussef/cs6212/dynamicprog.html
002 Search Strategies.md,Factors for search,- Completeness: Does it always find a solution if one exists?- Optimality: Does it always find the best solution? [Shortest Path Problem](Notes/Shortest%20Path%20Problem.md)- Average Branching Factor: average number of successors of any node $$ABF=No.of\ Nodes/No.of\ non\ leaf\ nodes$$- ## Uninformed Search- [Depth First Search](Notes/Depth%20First%20Search.md)- [Breadth First Search](Notes/Breadth%20First%20Search.md)- [Iterative Deepening Search](Notes/Iterative%20Deepening%20Search.md)- Path Cost- [Uniform Cost Search](Notes/Uniform%20Cost%20Search.md)- [Dijkstra's Algorithm](Notes/Dijkstra's%20Algorithm.md)- ## Informed Search (Heuristics)- [Greedy Best First Search](Notes/Greedy%20Best%20First%20Search.md)- [A-Star Search](Notes/A-Star%20Search.md)d: depth of the optimal solutionm: maximum depthl: cut off maximum depth
